{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983eb787",
   "metadata": {},
   "source": [
    "# Model Training & Testing\n",
    "This notebook is intended to help train and test various models, and expanded from our data_model_prep notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f1e0a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "random_state = 42\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb1f885",
   "metadata": {},
   "source": [
    "## Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5588aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For python pipeline that will be run from root folder of project\n",
    "# ROOT_DIR = os.path.abspath(os.curdir)\n",
    "\n",
    "# # Pickle save paths\n",
    "# training_df_path = os.path.join(ROOT_DIR,\"data/processed/training_df.pkl\")\n",
    "# testing_df_path = os.path.join(ROOT_DIR,\"data/processed/testing_df.pkl\")\n",
    "\n",
    "# Relative path for notebook\n",
    "# training_df_path = \"../data/processed/training_df.pkl\"\n",
    "testing_df_path = \"../data/processed/testing_df.pkl\"\n",
    "\n",
    "# likedislike processed only\n",
    "# training_df_path = \"../data/processed/likedislike_withcomments_processed.pkl\"\n",
    "\n",
    "# Small df for testing with comments\n",
    "training_df_path = \"../data/processed/training_df_small_withcomments.pkl\"\n",
    "\n",
    "# Small df for testing\n",
    "training_df_small_path = \"../data/processed/training_df_small.pkl\"\n",
    "# Big df for testing\n",
    "randompct10_pklpath=r\"/run/user/1000/gvfs/smb-share:server=metebox,share=data/JAMES/datasets/youtube-meta/youtube-02-2019-dump/randompct_df_10.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af712c5",
   "metadata": {},
   "source": [
    "## Load training and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88940982",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_pickle(training_df_path)\n",
    "testing_df = pd.read_pickle(testing_df_path)\n",
    "\n",
    "# big df\n",
    "# training_df_big = pd.read_pickle(randompct10_pklpath)\n",
    "\n",
    "# small df\n",
    "# training_df = pd.read_pickle(training_df_small_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5f4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat training df and training_df_big for testing if more data helps\n",
    "# training_df = pd.concat([training_df,training_df_big])\n",
    "# training_df = training_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "559a0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_if_0(row):\n",
    "    if row[\"like_count\"] == 0:\n",
    "        vl_ratio = (row[\"view_count\"]+1) / (row[\"like_count\"]+1)\n",
    "    else:\n",
    "        vl_ratio = row[\"view_count\"] / row[\"like_count\"]\n",
    "    return round(vl_ratio,2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f4a8300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth view_like ratio by adding 1 to test if that changes performance. Avoids division by 0.\n",
    "training_df[\"view_like_ratio_smoothed\"] = training_df.apply(lambda row: smooth_if_0(row),axis=1)\n",
    "testing_df[\"view_like_ratio_smoothed\"] = testing_df.apply(lambda row: smooth_if_0(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10605af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>view_like_ratio</th>\n",
       "      <th>view_like_ratio_smoothed</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.33</td>\n",
       "      <td>8.33</td>\n",
       "      <td>3380</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256.29</td>\n",
       "      <td>256.29</td>\n",
       "      <td>1794</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>187.14</td>\n",
       "      <td>187.14</td>\n",
       "      <td>53708</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.10</td>\n",
       "      <td>6.10</td>\n",
       "      <td>17665</td>\n",
       "      <td>2896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   view_like_ratio  view_like_ratio_smoothed  view_count  like_count\n",
       "0             8.33                      8.33        3380         406\n",
       "1           256.29                    256.29        1794           7\n",
       "2           187.14                    187.14       53708         287\n",
       "3             6.10                      6.10       17665        2896\n",
       "4             0.00                     12.00          11           0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df[[\"view_like_ratio\",\"view_like_ratio_smoothed\",\"view_count\",\"like_count\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3c7957d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>view_like_ratio</th>\n",
       "      <th>view_like_ratio_smoothed</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.33</td>\n",
       "      <td>8.30</td>\n",
       "      <td>3380</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256.29</td>\n",
       "      <td>224.40</td>\n",
       "      <td>1794</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   view_like_ratio  view_like_ratio_smoothed  view_count  like_count\n",
       "0             8.33                      8.30        3380         406\n",
       "1           256.29                    224.40        1794           7"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df[[\"view_like_ratio\",\"view_like_ratio_smoothed\",\"view_count\",\"like_count\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe9d64",
   "metadata": {},
   "source": [
    "## Columns for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "283c877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on what we can get at inference time from the Youtube API or scraping\n",
    "X_cols = [\n",
    "    \"duration\",\n",
    "    \"age_limit\",\n",
    "    \"view_count\",\n",
    "    \"like_count\",\n",
    "#     \"view_like_ratio\",\n",
    "    \"view_like_ratio_smoothed\",\n",
    "    \"is_comments_enabled\",\n",
    "    \"is_live_content\",\n",
    "    \"cat_codes\",\n",
    "#     \"desc_neu\",\n",
    "#     \"desc_neg\",\n",
    "#     \"desc_pos\",\n",
    "    \"desc_compound\",\n",
    "#     \"comment_neu\",\n",
    "#     \"comment_neg\",\n",
    "#     \"comment_pos\",\n",
    "    \"comment_compound\",\n",
    "    \"votes\"\n",
    "]\n",
    "\n",
    "y_col = \"ld_score_ohe\"\n",
    "\n",
    "# Get all related columns - useful for fastai models\n",
    "all_related_cols = X_cols.copy()\n",
    "all_related_cols.append(y_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8640fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing scaled versions\n",
    "scaler = StandardScaler()\n",
    "training_df_scaled_X = scaler.fit_transform(training_df[X_cols])\n",
    "testing_df_scaled_X = scaler.transform(testing_df[X_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2159ff32",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60cb281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_test_sets(df,X_cols,y_col,random_state=None):\n",
    "    \"\"\"\n",
    "    Takes in a processed dataframe and splits it into appropriate training and test splits.\n",
    "    \"\"\"\n",
    "    X = df[X_cols]\n",
    "    y = df[y_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state=random_state)\n",
    "    print(\"Splits created\")\n",
    "    return X, y, X_train, X_test, y_train,y_test\n",
    "\n",
    "def train_model(clf_object,model_name,X_train,y_train,X_test,y_test):\n",
    "    clf_object.fit(X_train,y_train)\n",
    "    acc, f1 = test_model_metrics(clf=clf_object,model_name=model_name)\n",
    "    return clf_object, acc, f1\n",
    "\n",
    "def test_model_metrics(clf, model_name,X_test,y_test):\n",
    "    testpreds = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test,testpreds)\n",
    "    \n",
    "    if len(y_test.unique()) > 2:\n",
    "        average = \"weighted\"\n",
    "    else:\n",
    "        average = \"binary\"\n",
    "        \n",
    "    f1 = f1_score(y_test,testpreds,average=average)\n",
    "    print(f\"{model_name} metrics:\")\n",
    "    print(f\"Accuracy Score: {acc}\")\n",
    "    print(f\"F1 score: {f1}\")\n",
    "    return acc,f1\n",
    "\n",
    "def cross_val_model(df,clf_object,X_cols,y_col,random_state,scoring,cv=5):\n",
    "    \"\"\"\n",
    "    Takes in a df, processes it, and then outputs a cross-validation f1 score.\n",
    "    Adapted from sklearn docs.\n",
    "    Scoring types available here: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    \"\"\"\n",
    "    X, y, X_train,X_test,y_train,y_test = create_training_test_sets(df,X_cols,y_col,random_state=random_state)\n",
    "    scores = cross_val_score(clf_object, X, y, cv=cv,scoring=scoring,verbose=1,n_jobs=-1)\n",
    "    print(f\"{scores.mean():0.2f} {scoring} with a standard deviation of {scores.std():0.2f}\")\n",
    "    return scores\n",
    "\n",
    "def confusion_matrix_model(df,clf_object,X_cols,y_col,random_state,model_name):\n",
    "    \"\"\"\n",
    "    Takes in a df, processes it, and then outputs a confusion matrix.\n",
    "    Adapted from sklearn docs: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    Scoring types available here: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    \"\"\"\n",
    "    X, y, X_train,X_test,y_train,y_test = create_training_test_sets(df,X_cols,y_col,random_state=random_state)\n",
    "    clf_object.fit(X_train,y_train)\n",
    "    \n",
    "    # Adapted from sklearn docs\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    titles_options = [\n",
    "        (f\"{model_name} Confusion matrix, without normalization\", None),\n",
    "        (f\"{model_name} Normalized confusion matrix\", \"true\"),\n",
    "    ]\n",
    "    \n",
    "    for title, normalize in titles_options:\n",
    "        disp = ConfusionMatrixDisplay.from_estimator(\n",
    "            clf_object,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            cmap=plt.cm.Blues,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "        disp.ax_.set_title(title)\n",
    "\n",
    "        print(title)\n",
    "        print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa59c5",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cb2059",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "038e2ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest metrics:\n",
      "Accuracy Score: 0.6423027966237843\n",
      "F1 score: 0.6592976312347466\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "rf_clf = RandomForestClassifier(n_jobs=-1,random_state=random_state)\n",
    "rf_clf.fit(training_df[X_cols],training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "rf_acc,rf_f1 = test_model_metrics(rf_clf,\"Random Forest\",testing_df[X_cols],testing_df[y_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9dedc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Confusion Matrix\n",
      "[[0.88903894 0.02745892 0.08350214]\n",
      " [0.47956743 0.11469273 0.40573984]\n",
      " [0.3118142  0.00711859 0.68106721]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    rf_clf,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"RF Confusion Matrix\")\n",
    "\n",
    "print(\"RF Confusion Matrix\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "978505c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest-Scaled metrics:\n",
      "Accuracy Score: 0.6422719015311608\n",
      "F1 score: 0.6592258035331474\n",
      "RF Confusion Matrix-Scaled\n",
      "[[0.88903894 0.02745892 0.08350214]\n",
      " [0.4797754  0.1142768  0.4059478 ]\n",
      " [0.31183912 0.00706875 0.68109213]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training model - scaled\n",
    "rf_clf = RandomForestClassifier(n_jobs=-1,random_state=random_state)\n",
    "rf_clf.fit(training_df_scaled_X,training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "rf_acc,rf_f1 = test_model_metrics(rf_clf,\"Random Forest-Scaled\",testing_df_scaled_X,testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    rf_clf,\n",
    "    testing_df_scaled_X,\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"RF Confusion Matrix-Scaled\")\n",
    "\n",
    "print(\"RF Confusion Matrix-Scaled\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "92092d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest metrics:\n",
      "Accuracy Score: 0.6399733066399733\n",
      "F1 score: 0.6569425053278282\n",
      "RF Confusion Matrix\n",
      "[[0.88917398 0.02610849 0.08471753]\n",
      " [0.48591037 0.10985754 0.40423209]\n",
      " [0.31408185 0.00723488 0.67868327]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_weight_dict = {-1:0.5,0:2,1:4}\n",
    "\n",
    "# Training model\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_jobs=-1,\n",
    "    random_state=random_state,\n",
    "    class_weight = class_weight_dict,\n",
    "#     max_depth=64,\n",
    "# The below are good without class weights\n",
    "#     min_samples_split=0.01,\n",
    "#     min_samples_leaf=0.05,\n",
    "#     max_depth=8,\n",
    "    )\n",
    "rf_clf.fit(training_df[X_cols],training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "rf_acc,rf_f1 = test_model_metrics(rf_clf,\"Random Forest\",testing_df[X_cols],testing_df[y_col])\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    rf_clf,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"RF Confusion Matrix\")\n",
    "\n",
    "print(\"RF Confusion Matrix\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cfb58b",
   "metadata": {},
   "source": [
    "### GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8731796e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM metrics:\n",
      "Accuracy Score: 0.646109072034998\n",
      "F1 score: 0.6593052590849785\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "# Params Based on previous gridsearch cvs\n",
    "gbm_model = lgb.LGBMClassifier(learning_rate=0.05,\n",
    "                               max_depth=20,\n",
    "                               min_child_samples=15,\n",
    "                               num_leaves=100,\n",
    "                               reg_alpha=0.03,\n",
    "                               random_state=random_state)\n",
    "gbm_model.fit(training_df[X_cols],training_df[y_col], verbose=20,eval_metric='logloss')\n",
    "\n",
    "# Testing model\n",
    "gbm_acc,gbm_f1 = test_model_metrics(gbm_model,\"GBM\",testing_df[X_cols],testing_df[y_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d47cd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM Confusion Matrix\n",
      "[[8.90704479e-01 2.15169930e-02 8.77785280e-02]\n",
      " [4.76136009e-01 9.44161381e-02 4.29447853e-01]\n",
      " [3.10161227e-01 7.22657386e-04 6.89116115e-01]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    gbm_model,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"GBM Confusion Matrix\")\n",
    "\n",
    "print(\"GBM Confusion Matrix\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3700552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM-Scaled metrics:\n",
      "Accuracy Score: 0.6459731336274546\n",
      "F1 score: 0.6589998832172455\n",
      "GBM Confusion Matrix-Scaled\n",
      "[[8.91604772e-01 2.12018906e-02 8.71933378e-02]\n",
      " [4.80035354e-01 9.21805137e-02 4.27784132e-01]\n",
      " [3.10128002e-01 7.47576606e-04 6.89124422e-01]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training model - scaled version\n",
    "# Params Based on previous gridsearch cvs\n",
    "gbm_model = lgb.LGBMClassifier(learning_rate=0.05,\n",
    "                               max_depth=20,\n",
    "                               min_child_samples=15,\n",
    "                               num_leaves=100,\n",
    "                               reg_alpha=0.03,\n",
    "                               random_state=random_state)\n",
    "gbm_model.fit(training_df_scaled_X,training_df[y_col], verbose=20,eval_metric='logloss')\n",
    "\n",
    "# Testing model\n",
    "gbm_acc,gbm_f1 = test_model_metrics(gbm_model,\"GBM-Scaled\",testing_df_scaled_X,testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    gbm_model,\n",
    "    testing_df_scaled_X,\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"GBM Confusion Matrix-Scaled\")\n",
    "\n",
    "print(\"GBM Confusion Matrix-Scaled\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b334408",
   "metadata": {},
   "source": [
    "### Neural Net\n",
    "We will use sklearn mlp and fastai to create a tabular learner neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "862c6177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.66438556\n",
      "Iteration 2, loss = 3.54408755\n",
      "Iteration 3, loss = 3.13286018\n",
      "Iteration 4, loss = 3.30536573\n",
      "Iteration 5, loss = 2.99156791\n",
      "Iteration 6, loss = 2.99865823\n",
      "Iteration 7, loss = 2.87969024\n",
      "Iteration 8, loss = 2.85567691\n",
      "Iteration 9, loss = 2.80016096\n",
      "Iteration 10, loss = 3.03667147\n",
      "Iteration 11, loss = 2.74064644\n",
      "Iteration 12, loss = 2.76742595\n",
      "Iteration 13, loss = 3.05556528\n",
      "Iteration 14, loss = 2.79942735\n",
      "Iteration 15, loss = 3.01989315\n",
      "Iteration 16, loss = 3.02477163\n",
      "Iteration 17, loss = 3.07936507\n",
      "Iteration 18, loss = 2.94051255\n",
      "Iteration 19, loss = 2.90007679\n",
      "Iteration 20, loss = 3.12330087\n",
      "Iteration 21, loss = 3.14555594\n",
      "Iteration 22, loss = 2.99393566\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP metrics:\n",
      "Accuracy Score: 0.6375882054894401\n",
      "F1 score: 0.6614146370902487\n",
      "MLP Confusion Matrix\n",
      "[[0.86203016 0.04938105 0.08858879]\n",
      " [0.6446397  0.10216284 0.25319746]\n",
      " [0.24913406 0.06915084 0.68171511]]\n",
      "CPU times: user 5min 56s, sys: 15.5 s, total: 6min 11s\n",
      "Wall time: 1min 2s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# Starting with Sklearn MLP\n",
    "# Training model\n",
    "mlp_clf = MLPClassifier(random_state=random_state, max_iter=300, verbose=2)\n",
    "mlp_clf.fit(training_df[X_cols],training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "mlp_acc,mlp_f1 = test_model_metrics(mlp_clf,\"MLP\",testing_df[X_cols],testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    mlp_clf,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"MLP Confusion Matrix\")\n",
    "\n",
    "print(\"MLP Confusion Matrix\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "723e15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_target(value):\n",
    "    if value == -1:\n",
    "        return \"Negative\"\n",
    "    elif value == 0:\n",
    "        return \"Neutral\"\n",
    "    elif value == 1:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93e69a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'hidden_layer_sizes': (10, 10, 10),\n",
       " 'learning_rate': 'constant'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_scaled_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3882d",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb57bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65909687\n",
      "Iteration 2, loss = 0.39678566\n",
      "Iteration 3, loss = 0.38150078\n",
      "Iteration 4, loss = 0.38007179\n",
      "Iteration 5, loss = 0.38013141\n",
      "Iteration 6, loss = 0.36632018\n",
      "Iteration 7, loss = 0.34526975\n",
      "Iteration 8, loss = 0.31913522\n",
      "Iteration 9, loss = 0.28969150\n",
      "Iteration 10, loss = 0.36281543\n",
      "Iteration 11, loss = 0.30733309\n",
      "Iteration 12, loss = 0.33821705\n",
      "Iteration 13, loss = 0.29039501\n",
      "Iteration 14, loss = 0.29838711\n",
      "Iteration 15, loss = 0.26005249\n",
      "Iteration 16, loss = 0.28385491\n",
      "Iteration 17, loss = 0.26207056\n",
      "Iteration 18, loss = 0.26049553\n",
      "Iteration 19, loss = 0.29287013\n",
      "Iteration 20, loss = 0.28642488\n",
      "Iteration 21, loss = 0.25569598\n",
      "Iteration 22, loss = 0.26283006\n",
      "Iteration 23, loss = 0.26713178\n",
      "Iteration 24, loss = 0.26558052\n",
      "Iteration 25, loss = 0.28093716\n",
      "Iteration 26, loss = 0.25200414\n",
      "Iteration 27, loss = 0.27285981\n",
      "Iteration 28, loss = 0.25286581\n",
      "Iteration 29, loss = 0.25029781\n",
      "Iteration 30, loss = 0.25511140\n",
      "Iteration 31, loss = 0.26415123\n",
      "Iteration 32, loss = 0.24561133\n",
      "Iteration 33, loss = 0.24750386\n",
      "Iteration 34, loss = 0.24728160\n",
      "Iteration 35, loss = 0.25350571\n",
      "Iteration 36, loss = 0.25526849\n",
      "Iteration 37, loss = 0.24872099\n",
      "Iteration 38, loss = 0.26151374\n",
      "Iteration 39, loss = 0.25679483\n",
      "Iteration 40, loss = 0.25084339\n",
      "Iteration 41, loss = 0.24099319\n",
      "Iteration 42, loss = 0.24085949\n",
      "Iteration 43, loss = 0.24685562\n",
      "Iteration 44, loss = 0.24310452\n",
      "Iteration 45, loss = 0.25150142\n",
      "Iteration 46, loss = 0.24001576\n",
      "Iteration 47, loss = 0.23902914\n",
      "Iteration 48, loss = 0.25473203\n",
      "Iteration 49, loss = 0.26616040\n",
      "Iteration 50, loss = 0.24337455\n",
      "Iteration 51, loss = 0.23584826\n",
      "Iteration 52, loss = 0.27096657\n",
      "Iteration 53, loss = 0.24508576\n",
      "Iteration 54, loss = 0.24508869\n",
      "Iteration 55, loss = 0.24835534\n",
      "Iteration 56, loss = 0.30679455\n",
      "Iteration 57, loss = 0.30552955\n",
      "Iteration 58, loss = 0.28967370\n",
      "Iteration 59, loss = 0.26837577\n",
      "Iteration 60, loss = 0.24399377\n",
      "Iteration 61, loss = 0.25036426\n",
      "Iteration 62, loss = 0.23146802\n",
      "Iteration 63, loss = 0.23619183\n",
      "Iteration 64, loss = 0.23348820\n",
      "Iteration 65, loss = 0.25514821\n",
      "Iteration 66, loss = 0.26410617\n",
      "Iteration 67, loss = 0.24020664\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Starting with Sklearn MLP\n",
    "# Training model\n",
    "mlp_clf = MLPClassifier(random_state=random_state, \n",
    "                        max_iter=300, \n",
    "                        verbose=2,\n",
    "                        activation=\"relu\",\n",
    "                        hidden_layer_sizes=(5,3,),\n",
    "                        learning_rate=\"adaptive\",\n",
    "                        alpha=0.0001)\n",
    "\n",
    "mlp_clf.fit(training_df[X_cols],training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "mlp_acc,mlp_f1 = test_model_metrics(mlp_clf,\"MLP-BestParams\",testing_df[X_cols],testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    mlp_clf,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"MLP Confusion Matrix-BestParams\")\n",
    "\n",
    "print(\"MLP Confusion Matrix-BestParams\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261547f",
   "metadata": {},
   "source": [
    "#### MLP with Scaled X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Starting with Sklearn MLP\n",
    "# Training model\n",
    "mlp_clf = MLPClassifier(random_state=random_state, \n",
    "                        max_iter=300, \n",
    "                        verbose=2,\n",
    "                        activation=\"relu\",\n",
    "                        hidden_layer_sizes=(5,3,),\n",
    "                        learning_rate=\"adaptive\",\n",
    "                        alpha=0.0001)\n",
    "\n",
    "mlp_clf.fit(training_df_scaled_X,training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "mlp_acc,mlp_f1 = test_model_metrics(mlp_clf,\"MLP-BestParams-Scaled\",testing_df_scaled_X,testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    mlp_clf,\n",
    "    testing_df_scaled_X,\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"MLP Confusion Matrix-BestParams-Scaled\")\n",
    "\n",
    "print(\"MLP Confusion Matrix-BestParams-Scaled\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0d8ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.52888149\n",
      "Iteration 2, loss = 0.45452839\n",
      "Iteration 3, loss = 0.43503824\n",
      "Iteration 4, loss = 0.42217512\n",
      "Iteration 5, loss = 0.41280090\n",
      "Iteration 6, loss = 0.40462974\n",
      "Iteration 7, loss = 0.39753161\n",
      "Iteration 8, loss = 0.38915297\n",
      "Iteration 9, loss = 0.38123902\n",
      "Iteration 10, loss = 0.37458984\n",
      "Iteration 11, loss = 0.36895252\n",
      "Iteration 12, loss = 0.36397631\n",
      "Iteration 13, loss = 0.35963411\n",
      "Iteration 14, loss = 0.35531498\n",
      "Iteration 15, loss = 0.35205756\n",
      "Iteration 16, loss = 0.34842535\n",
      "Iteration 17, loss = 0.34576676\n",
      "Iteration 18, loss = 0.34285251\n",
      "Iteration 19, loss = 0.34089546\n",
      "Iteration 20, loss = 0.33869156\n",
      "Iteration 21, loss = 0.33695216\n",
      "Iteration 22, loss = 0.33499959\n",
      "Iteration 23, loss = 0.33353915\n",
      "Iteration 24, loss = 0.33181305\n",
      "Iteration 25, loss = 0.32977335\n",
      "Iteration 26, loss = 0.32894643\n",
      "Iteration 27, loss = 0.32776113\n",
      "Iteration 28, loss = 0.32671537\n",
      "Iteration 29, loss = 0.32560771\n",
      "Iteration 30, loss = 0.32408620\n",
      "Iteration 31, loss = 0.32290429\n",
      "Iteration 32, loss = 0.32237581\n",
      "Iteration 33, loss = 0.32139370\n",
      "Iteration 34, loss = 0.32002606\n",
      "Iteration 35, loss = 0.31975339\n",
      "Iteration 36, loss = 0.31849274\n",
      "Iteration 37, loss = 0.31799886\n",
      "Iteration 38, loss = 0.31742785\n",
      "Iteration 39, loss = 0.31651666\n",
      "Iteration 40, loss = 0.31590620\n",
      "Iteration 41, loss = 0.31566462\n",
      "Iteration 42, loss = 0.31489282\n",
      "Iteration 43, loss = 0.31461642\n",
      "Iteration 44, loss = 0.31426660\n",
      "Iteration 45, loss = 0.31381502\n",
      "Iteration 46, loss = 0.31357520\n",
      "Iteration 47, loss = 0.31307796\n",
      "Iteration 48, loss = 0.31247214\n",
      "Iteration 49, loss = 0.31161227\n",
      "Iteration 50, loss = 0.31174415\n",
      "Iteration 51, loss = 0.31091615\n",
      "Iteration 52, loss = 0.31094916\n",
      "Iteration 53, loss = 0.31149827\n",
      "Iteration 54, loss = 0.31033933\n",
      "Iteration 55, loss = 0.31045071\n",
      "Iteration 56, loss = 0.31008013\n",
      "Iteration 57, loss = 0.30991602\n",
      "Iteration 58, loss = 0.30934253\n",
      "Iteration 59, loss = 0.30880318\n",
      "Iteration 60, loss = 0.30868098\n",
      "Iteration 61, loss = 0.30869055\n",
      "Iteration 62, loss = 0.30836068\n",
      "Iteration 63, loss = 0.30800431\n",
      "Iteration 64, loss = 0.30690231\n",
      "Iteration 65, loss = 0.30814661\n",
      "Iteration 66, loss = 0.30705528\n",
      "Iteration 67, loss = 0.30722602\n",
      "Iteration 68, loss = 0.30670784\n",
      "Iteration 69, loss = 0.30649609\n",
      "Iteration 70, loss = 0.30644363\n",
      "Iteration 71, loss = 0.30606570\n",
      "Iteration 72, loss = 0.30640294\n",
      "Iteration 73, loss = 0.30611454\n",
      "Iteration 74, loss = 0.30554937\n",
      "Iteration 75, loss = 0.30536798\n",
      "Iteration 76, loss = 0.30475647\n",
      "Iteration 77, loss = 0.30507828\n",
      "Iteration 78, loss = 0.30507615\n",
      "Iteration 79, loss = 0.30546442\n",
      "Iteration 80, loss = 0.30551854\n",
      "Iteration 81, loss = 0.30464310\n",
      "Iteration 82, loss = 0.30407571\n",
      "Iteration 83, loss = 0.30420274\n",
      "Iteration 84, loss = 0.30487480\n",
      "Iteration 85, loss = 0.30414891\n",
      "Iteration 86, loss = 0.30413849\n",
      "Iteration 87, loss = 0.30384598\n",
      "Iteration 88, loss = 0.30384827\n",
      "Iteration 89, loss = 0.30363723\n",
      "Iteration 90, loss = 0.30303959\n",
      "Iteration 91, loss = 0.30295677\n",
      "Iteration 92, loss = 0.30321859\n",
      "Iteration 93, loss = 0.30302828\n",
      "Iteration 94, loss = 0.30289775\n",
      "Iteration 95, loss = 0.30236895\n",
      "Iteration 96, loss = 0.30288141\n",
      "Iteration 97, loss = 0.30202122\n",
      "Iteration 98, loss = 0.30245049\n",
      "Iteration 99, loss = 0.30183711\n",
      "Iteration 100, loss = 0.30183750\n",
      "Iteration 101, loss = 0.30182555\n",
      "Iteration 102, loss = 0.30176326\n",
      "Iteration 103, loss = 0.30231682\n",
      "Iteration 104, loss = 0.30103099\n",
      "Iteration 105, loss = 0.30175281\n",
      "Iteration 106, loss = 0.30130672\n",
      "Iteration 107, loss = 0.30136934\n",
      "Iteration 108, loss = 0.30125336\n",
      "Iteration 109, loss = 0.30150064\n",
      "Iteration 110, loss = 0.30088800\n",
      "Iteration 111, loss = 0.30104147\n",
      "Iteration 112, loss = 0.30096487\n",
      "Iteration 113, loss = 0.30051331\n",
      "Iteration 114, loss = 0.30091037\n",
      "Iteration 115, loss = 0.30041917\n",
      "Iteration 116, loss = 0.30034547\n",
      "Iteration 117, loss = 0.30059148\n",
      "Iteration 118, loss = 0.30083908\n",
      "Iteration 119, loss = 0.29979175\n",
      "Iteration 120, loss = 0.29982565\n",
      "Iteration 121, loss = 0.29975100\n",
      "Iteration 122, loss = 0.29924385\n",
      "Iteration 123, loss = 0.29947310\n",
      "Iteration 124, loss = 0.29966863\n",
      "Iteration 125, loss = 0.29933657\n",
      "Iteration 126, loss = 0.29974274\n",
      "Iteration 127, loss = 0.29856663\n",
      "Iteration 128, loss = 0.29959955\n",
      "Iteration 129, loss = 0.29887989\n",
      "Iteration 130, loss = 0.29861118\n",
      "Iteration 131, loss = 0.29920393\n",
      "Iteration 132, loss = 0.29825620\n",
      "Iteration 133, loss = 0.29919310\n",
      "Iteration 134, loss = 0.29821654\n",
      "Iteration 135, loss = 0.29818867\n",
      "Iteration 136, loss = 0.29826318\n",
      "Iteration 137, loss = 0.29867544\n",
      "Iteration 138, loss = 0.29779693\n",
      "Iteration 139, loss = 0.29847433\n",
      "Iteration 140, loss = 0.29818635\n",
      "Iteration 141, loss = 0.29766970\n",
      "Iteration 142, loss = 0.29778160\n",
      "Iteration 143, loss = 0.29789199\n",
      "Iteration 144, loss = 0.29690438\n",
      "Iteration 145, loss = 0.29728909\n",
      "Iteration 146, loss = 0.29774769\n",
      "Iteration 147, loss = 0.29736654\n",
      "Iteration 148, loss = 0.29717979\n",
      "Iteration 149, loss = 0.29761501\n",
      "Iteration 150, loss = 0.29728422\n",
      "Iteration 151, loss = 0.29687847\n",
      "Iteration 152, loss = 0.29655063\n",
      "Iteration 153, loss = 0.29685347\n",
      "Iteration 154, loss = 0.29711520\n",
      "Iteration 155, loss = 0.29701968\n",
      "Iteration 156, loss = 0.29612678\n",
      "Iteration 157, loss = 0.29711503\n",
      "Iteration 158, loss = 0.29630919\n",
      "Iteration 159, loss = 0.29660917\n",
      "Iteration 160, loss = 0.29694571\n",
      "Iteration 161, loss = 0.29600743\n",
      "Iteration 162, loss = 0.29653635\n",
      "Iteration 163, loss = 0.29612042\n",
      "Iteration 164, loss = 0.29577699\n",
      "Iteration 165, loss = 0.29552632\n",
      "Iteration 166, loss = 0.29567548\n",
      "Iteration 167, loss = 0.29589733\n",
      "Iteration 168, loss = 0.29546622\n",
      "Iteration 169, loss = 0.29599204\n",
      "Iteration 170, loss = 0.29541652\n",
      "Iteration 171, loss = 0.29553605\n",
      "Iteration 172, loss = 0.29498046\n",
      "Iteration 173, loss = 0.29524613\n",
      "Iteration 174, loss = 0.29589048\n",
      "Iteration 175, loss = 0.29537446\n",
      "Iteration 176, loss = 0.29534253\n",
      "Iteration 177, loss = 0.29505773\n",
      "Iteration 178, loss = 0.29474055\n",
      "Iteration 179, loss = 0.29547905\n",
      "Iteration 180, loss = 0.29504412\n",
      "Iteration 181, loss = 0.29529162\n",
      "Iteration 182, loss = 0.29493744\n",
      "Iteration 183, loss = 0.29457853\n",
      "Iteration 184, loss = 0.29504029\n",
      "Iteration 185, loss = 0.29490183\n",
      "Iteration 186, loss = 0.29459652\n",
      "Iteration 187, loss = 0.29388614\n",
      "Iteration 188, loss = 0.29516332\n",
      "Iteration 189, loss = 0.29467582\n",
      "Iteration 190, loss = 0.29469713\n",
      "Iteration 191, loss = 0.29387004\n",
      "Iteration 192, loss = 0.29492151\n",
      "Iteration 193, loss = 0.29410039\n",
      "Iteration 194, loss = 0.29419426\n",
      "Iteration 195, loss = 0.29438839\n",
      "Iteration 196, loss = 0.29359345\n",
      "Iteration 197, loss = 0.29369781\n",
      "Iteration 198, loss = 0.29393363\n",
      "Iteration 199, loss = 0.29452686\n",
      "Iteration 200, loss = 0.29387441\n",
      "Iteration 201, loss = 0.29374552\n",
      "Iteration 202, loss = 0.29384298\n",
      "Iteration 203, loss = 0.29335357\n",
      "Iteration 204, loss = 0.29423296\n",
      "Iteration 205, loss = 0.29371302\n",
      "Iteration 206, loss = 0.29332048\n",
      "Iteration 207, loss = 0.29371154\n",
      "Iteration 208, loss = 0.29338339\n",
      "Iteration 209, loss = 0.29325125\n",
      "Iteration 210, loss = 0.29326695\n",
      "Iteration 211, loss = 0.29279153\n",
      "Iteration 212, loss = 0.29330649\n",
      "Iteration 213, loss = 0.29281097\n",
      "Iteration 214, loss = 0.29361046\n",
      "Iteration 215, loss = 0.29343059\n",
      "Iteration 216, loss = 0.29292150\n",
      "Iteration 217, loss = 0.29281753\n",
      "Iteration 218, loss = 0.29323715\n",
      "Iteration 219, loss = 0.29317442\n",
      "Iteration 220, loss = 0.29248131\n",
      "Iteration 221, loss = 0.29296128\n",
      "Iteration 222, loss = 0.29313455\n",
      "Iteration 223, loss = 0.29251139\n",
      "Iteration 224, loss = 0.29234400\n",
      "Iteration 225, loss = 0.29212789\n",
      "Iteration 226, loss = 0.29222372\n",
      "Iteration 227, loss = 0.29212244\n",
      "Iteration 228, loss = 0.29234730\n",
      "Iteration 229, loss = 0.29219140\n",
      "Iteration 230, loss = 0.29147185\n",
      "Iteration 231, loss = 0.29243487\n",
      "Iteration 232, loss = 0.29200217\n",
      "Iteration 233, loss = 0.29196741\n",
      "Iteration 234, loss = 0.29178357\n",
      "Iteration 235, loss = 0.29197687\n",
      "Iteration 236, loss = 0.29143964\n",
      "Iteration 237, loss = 0.29111337\n",
      "Iteration 238, loss = 0.29199148\n",
      "Iteration 239, loss = 0.29163764\n",
      "Iteration 240, loss = 0.29158829\n",
      "Iteration 241, loss = 0.29132336\n",
      "Iteration 242, loss = 0.29109332\n",
      "Iteration 243, loss = 0.29122760\n",
      "Iteration 244, loss = 0.29118173\n",
      "Iteration 245, loss = 0.29134089\n",
      "Iteration 246, loss = 0.29155140\n",
      "Iteration 247, loss = 0.29111938\n",
      "Iteration 248, loss = 0.29068767\n",
      "Iteration 249, loss = 0.29164970\n",
      "Iteration 250, loss = 0.29125775\n",
      "Iteration 251, loss = 0.29105743\n",
      "Iteration 252, loss = 0.29058608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.29106422\n",
      "Iteration 254, loss = 0.29083931\n",
      "Iteration 255, loss = 0.29093415\n",
      "Iteration 256, loss = 0.29077275\n",
      "Iteration 257, loss = 0.29095828\n",
      "Iteration 258, loss = 0.29068945\n",
      "Iteration 259, loss = 0.29043570\n",
      "Iteration 260, loss = 0.29064498\n",
      "Iteration 261, loss = 0.29039395\n",
      "Iteration 262, loss = 0.29096491\n",
      "Iteration 263, loss = 0.29039398\n",
      "Iteration 264, loss = 0.29034399\n",
      "Iteration 265, loss = 0.29036935\n",
      "Iteration 266, loss = 0.28994328\n",
      "Iteration 267, loss = 0.28975751\n",
      "Iteration 268, loss = 0.29016122\n",
      "Iteration 269, loss = 0.29005193\n",
      "Iteration 270, loss = 0.28985348\n",
      "Iteration 271, loss = 0.29003092\n",
      "Iteration 272, loss = 0.28978113\n",
      "Iteration 273, loss = 0.29058611\n",
      "Iteration 274, loss = 0.28987994\n",
      "Iteration 275, loss = 0.29013931\n",
      "Iteration 276, loss = 0.29024714\n",
      "Iteration 277, loss = 0.28943902\n",
      "Iteration 278, loss = 0.28980867\n",
      "Iteration 279, loss = 0.28947305\n",
      "Iteration 280, loss = 0.28994736\n",
      "Iteration 281, loss = 0.28951315\n",
      "Iteration 282, loss = 0.29001049\n",
      "Iteration 283, loss = 0.28951405\n",
      "Iteration 284, loss = 0.28970222\n",
      "Iteration 285, loss = 0.28965472\n",
      "Iteration 286, loss = 0.28975397\n",
      "Iteration 287, loss = 0.28955687\n",
      "Iteration 288, loss = 0.28923531\n",
      "Iteration 289, loss = 0.28925289\n",
      "Iteration 290, loss = 0.28919380\n",
      "Iteration 291, loss = 0.28940797\n",
      "Iteration 292, loss = 0.28895637\n",
      "Iteration 293, loss = 0.28910091\n",
      "Iteration 294, loss = 0.28969830\n",
      "Iteration 295, loss = 0.28882778\n",
      "Iteration 296, loss = 0.28928294\n",
      "Iteration 297, loss = 0.28933241\n",
      "Iteration 298, loss = 0.28921114\n",
      "Iteration 299, loss = 0.28922337\n",
      "Iteration 300, loss = 0.28946019\n",
      "MLP-scaled metrics:\n",
      "Accuracy Score: 0.7711476909007773\n",
      "F1 score: 0.7368985108906481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Confusion Matrix-scaled\n",
      "[[8.08327707e-01 1.12536574e-03 1.90546928e-01]\n",
      " [4.47436831e-01 5.45908287e-03 5.47104087e-01]\n",
      " [1.12585037e-01 7.97415046e-04 8.86617548e-01]]\n",
      "CPU times: user 1h 51min 17s, sys: 4min 38s, total: 1h 55min 56s\n",
      "Wall time: 19min 20s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time \n",
    "# Starting with Sklearn MLP\n",
    "# Training model\n",
    "mlp_clf = MLPClassifier(random_state=random_state, max_iter=300, verbose=10)\n",
    "mlp_clf.fit(training_df_scaled_X,training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "mlp_acc,mlp_f1 = test_model_metrics(mlp_clf,\"MLP-scaled\",testing_df_scaled_X,testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    mlp_clf,\n",
    "    testing_df_scaled_X,\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"MLP Confusion Matrix-scaled\")\n",
    "\n",
    "print(\"MLP Confusion Matrix-scaled\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6784cb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.40701400\n",
      "Iteration 2, loss = 0.31252940\n",
      "Iteration 3, loss = 0.30277185\n",
      "Iteration 4, loss = 0.30906611\n",
      "Iteration 5, loss = 0.29275641\n",
      "Iteration 6, loss = 0.28824150\n",
      "Iteration 7, loss = 0.28439195\n",
      "Iteration 8, loss = 0.29689165\n",
      "Iteration 9, loss = 0.27265937\n",
      "Iteration 10, loss = 0.28276546\n",
      "Iteration 11, loss = 0.28344491\n",
      "Iteration 12, loss = 0.30125225\n",
      "Iteration 13, loss = 0.28142215\n",
      "Iteration 14, loss = 0.27446613\n",
      "Iteration 15, loss = 0.27726245\n",
      "Iteration 16, loss = 0.27750526\n",
      "Iteration 17, loss = 0.27472576\n",
      "Iteration 18, loss = 0.29798478\n",
      "Iteration 19, loss = 0.27344115\n",
      "Iteration 20, loss = 0.27723733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33151311\n",
      "Iteration 2, loss = 0.28947453\n",
      "Iteration 3, loss = 0.27905612\n",
      "Iteration 4, loss = 0.27487679\n",
      "Iteration 5, loss = 0.27349553\n",
      "Iteration 6, loss = 0.27349734\n",
      "Iteration 7, loss = 0.26545096\n",
      "Iteration 8, loss = 0.26780056\n",
      "Iteration 9, loss = 0.27019535\n",
      "Iteration 10, loss = 0.26352828\n",
      "Iteration 11, loss = 0.26251522\n",
      "Iteration 12, loss = 0.26131800\n",
      "Iteration 13, loss = 0.26319210\n",
      "Iteration 14, loss = 0.26076037\n",
      "Iteration 15, loss = 0.26463225\n",
      "Iteration 16, loss = 0.26420030\n",
      "Iteration 17, loss = 0.26245610\n",
      "Iteration 18, loss = 0.25809431\n",
      "Iteration 19, loss = 0.25912869\n",
      "Iteration 20, loss = 0.25764032\n",
      "Iteration 21, loss = 0.25622206\n",
      "Iteration 22, loss = 0.26324904\n",
      "Iteration 23, loss = 0.25670276\n",
      "Iteration 24, loss = 0.25505707\n",
      "Iteration 25, loss = 0.25654553\n",
      "Iteration 26, loss = 0.25630235\n",
      "Iteration 27, loss = 0.25776975\n",
      "Iteration 28, loss = 0.25645046\n",
      "Iteration 29, loss = 0.25730915\n",
      "Iteration 30, loss = 0.25621657\n",
      "Iteration 31, loss = 0.25539488\n",
      "Iteration 32, loss = 0.25624228\n",
      "Iteration 33, loss = 0.25360930\n",
      "Iteration 34, loss = 0.25557961\n",
      "Iteration 35, loss = 0.25553889\n",
      "Iteration 36, loss = 0.25778810\n",
      "Iteration 37, loss = 0.25508887\n",
      "Iteration 38, loss = 0.25414387\n",
      "Iteration 39, loss = 0.25342958\n",
      "Iteration 40, loss = 0.26147934\n",
      "Iteration 41, loss = 0.25905275\n",
      "Iteration 42, loss = 0.25461603\n",
      "Iteration 43, loss = 0.25411535\n",
      "Iteration 44, loss = 0.25944967\n",
      "Iteration 45, loss = 0.25393417\n",
      "Iteration 46, loss = 0.25428147\n",
      "Iteration 47, loss = 0.25308785\n",
      "Iteration 48, loss = 0.25401726\n",
      "Iteration 49, loss = 0.25564002\n",
      "Iteration 50, loss = 0.25591395\n",
      "Iteration 51, loss = 0.25577974\n",
      "Iteration 52, loss = 0.25217546\n",
      "Iteration 53, loss = 0.25247073\n",
      "Iteration 54, loss = 0.25170026\n",
      "Iteration 55, loss = 0.25297521\n",
      "Iteration 56, loss = 0.25436681\n",
      "Iteration 57, loss = 0.25361548\n",
      "Iteration 58, loss = 0.25426128\n",
      "Iteration 59, loss = 0.25122683\n",
      "Iteration 60, loss = 0.25344580\n",
      "Iteration 61, loss = 0.25222897\n",
      "Iteration 62, loss = 0.25551096\n",
      "Iteration 63, loss = 0.25327813\n",
      "Iteration 64, loss = 0.25419450\n",
      "Iteration 65, loss = 0.25240072\n",
      "Iteration 66, loss = 0.25018247\n",
      "Iteration 67, loss = 0.25251785\n",
      "Iteration 68, loss = 0.25390283\n",
      "Iteration 69, loss = 0.25459611\n",
      "Iteration 70, loss = 0.25112403\n",
      "Iteration 71, loss = 0.25181009\n",
      "Iteration 72, loss = 0.25375163\n",
      "Iteration 73, loss = 0.25159532\n",
      "Iteration 74, loss = 0.25060620\n",
      "Iteration 75, loss = 0.25239002\n",
      "Iteration 76, loss = 0.25120615\n",
      "Iteration 77, loss = 0.25238745\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33997125\n",
      "Iteration 2, loss = 0.29539368\n",
      "Iteration 3, loss = 0.29507243\n",
      "Iteration 4, loss = 0.28532762\n",
      "Iteration 5, loss = 0.28135136\n",
      "Iteration 6, loss = 0.28502956\n",
      "Iteration 7, loss = 0.27266012\n",
      "Iteration 8, loss = 0.27748809\n",
      "Iteration 9, loss = 0.27056093\n",
      "Iteration 10, loss = 0.27045629\n",
      "Iteration 11, loss = 0.27345408\n",
      "Iteration 12, loss = 0.27578223\n",
      "Iteration 13, loss = 0.26338679\n",
      "Iteration 14, loss = 0.27297334\n",
      "Iteration 15, loss = 0.26640932\n",
      "Iteration 16, loss = 0.26764288\n",
      "Iteration 17, loss = 0.26415566\n",
      "Iteration 18, loss = 0.26213627\n",
      "Iteration 19, loss = 0.26431007\n",
      "Iteration 20, loss = 0.26140680\n",
      "Iteration 21, loss = 0.26196546\n",
      "Iteration 22, loss = 0.26047105\n",
      "Iteration 23, loss = 0.25917369\n",
      "Iteration 24, loss = 0.26111586\n",
      "Iteration 25, loss = 0.25987146\n",
      "Iteration 26, loss = 0.25732424\n",
      "Iteration 27, loss = 0.25997891\n",
      "Iteration 28, loss = 0.25839253\n",
      "Iteration 29, loss = 0.25611018\n",
      "Iteration 30, loss = 0.25857453\n",
      "Iteration 31, loss = 0.26175838\n",
      "Iteration 32, loss = 0.25661687\n",
      "Iteration 33, loss = 0.26037248\n",
      "Iteration 34, loss = 0.26065735\n",
      "Iteration 35, loss = 0.26076017\n",
      "Iteration 36, loss = 0.25959812\n",
      "Iteration 37, loss = 0.25866199\n",
      "Iteration 38, loss = 0.25746707\n",
      "Iteration 39, loss = 0.26004436\n",
      "Iteration 40, loss = 0.26084979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45275526\n",
      "Iteration 2, loss = 0.32560499\n",
      "Iteration 3, loss = 0.31452355\n",
      "Iteration 4, loss = 0.32580662\n",
      "Iteration 5, loss = 0.29341889\n",
      "Iteration 6, loss = 0.28483689\n",
      "Iteration 7, loss = 0.29165098\n",
      "Iteration 8, loss = 0.28636074\n",
      "Iteration 9, loss = 0.28689821\n",
      "Iteration 10, loss = 0.29008393\n",
      "Iteration 11, loss = 0.27521144\n",
      "Iteration 12, loss = 0.27387334\n",
      "Iteration 13, loss = 0.27391479\n",
      "Iteration 14, loss = 0.28197902\n",
      "Iteration 15, loss = 0.27639178\n",
      "Iteration 16, loss = 0.28908648\n",
      "Iteration 17, loss = 0.29012385\n",
      "Iteration 18, loss = 0.28440596\n",
      "Iteration 19, loss = 0.27585607\n",
      "Iteration 20, loss = 0.28933934\n",
      "Iteration 21, loss = 0.27958911\n",
      "Iteration 22, loss = 0.28063368\n",
      "Iteration 23, loss = 0.27714014\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42228354\n",
      "Iteration 2, loss = 0.32215967\n",
      "Iteration 3, loss = 0.33908476\n",
      "Iteration 4, loss = 0.32521411\n",
      "Iteration 5, loss = 0.32044450\n",
      "Iteration 6, loss = 0.31179280\n",
      "Iteration 7, loss = 0.29381041\n",
      "Iteration 8, loss = 0.28349443\n",
      "Iteration 9, loss = 0.31964936\n",
      "Iteration 10, loss = 0.29322928\n",
      "Iteration 11, loss = 0.30093576\n",
      "Iteration 12, loss = 0.28619338\n",
      "Iteration 13, loss = 0.29023536\n",
      "Iteration 14, loss = 0.28329078\n",
      "Iteration 15, loss = 0.28776916\n",
      "Iteration 16, loss = 0.30342951\n",
      "Iteration 17, loss = 0.29841244\n",
      "Iteration 18, loss = 0.28476726\n",
      "Iteration 19, loss = 0.29218428\n",
      "Iteration 20, loss = 0.30346762\n",
      "Iteration 21, loss = 0.27419065\n",
      "Iteration 22, loss = 0.28124028\n",
      "Iteration 23, loss = 0.28219874\n",
      "Iteration 24, loss = 0.28055917\n",
      "Iteration 25, loss = 0.29387752\n",
      "Iteration 26, loss = 0.27792768\n",
      "Iteration 27, loss = 0.28441130\n",
      "Iteration 28, loss = 0.28188538\n",
      "Iteration 29, loss = 0.27375106\n",
      "Iteration 30, loss = 0.28203961\n",
      "Iteration 31, loss = 0.27606503\n",
      "Iteration 32, loss = 0.28179319\n",
      "Iteration 33, loss = 0.28937709\n",
      "Iteration 34, loss = 0.28695145\n",
      "Iteration 35, loss = 0.28219006\n",
      "Iteration 36, loss = 0.28241085\n",
      "Iteration 37, loss = 0.29089766\n",
      "Iteration 38, loss = 0.28637184\n",
      "Iteration 39, loss = 0.27293665\n",
      "Iteration 40, loss = 0.28057402\n",
      "Iteration 41, loss = 0.28730272\n",
      "Iteration 42, loss = 0.27935682\n",
      "Iteration 43, loss = 0.27516534\n",
      "Iteration 44, loss = 0.28736978\n",
      "Iteration 45, loss = 0.27591741\n",
      "Iteration 46, loss = 0.28472459\n",
      "Iteration 47, loss = 0.27635595\n",
      "Iteration 48, loss = 0.27355454\n",
      "Iteration 49, loss = 0.28148498\n",
      "Iteration 50, loss = 0.27211471\n",
      "Iteration 51, loss = 0.28507277\n",
      "Iteration 52, loss = 0.27622686\n",
      "Iteration 53, loss = 0.27450023\n",
      "Iteration 54, loss = 0.28213408\n",
      "Iteration 55, loss = 0.28091597\n",
      "Iteration 56, loss = 0.28580932\n",
      "Iteration 57, loss = 0.27667131\n",
      "Iteration 58, loss = 0.28859642\n",
      "Iteration 59, loss = 0.28628586\n",
      "Iteration 60, loss = 0.28737237\n",
      "Iteration 61, loss = 0.27819459\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33886880\n",
      "Iteration 2, loss = 0.30227519\n",
      "Iteration 3, loss = 0.30093644\n",
      "Iteration 4, loss = 0.29721878\n",
      "Iteration 5, loss = 0.29810112\n",
      "Iteration 6, loss = 0.28372478\n",
      "Iteration 7, loss = 0.29411973\n",
      "Iteration 8, loss = 0.28725877\n",
      "Iteration 9, loss = 0.29052292\n",
      "Iteration 10, loss = 0.28122062\n",
      "Iteration 11, loss = 0.28404076\n",
      "Iteration 12, loss = 0.28360170\n",
      "Iteration 13, loss = 0.28201041\n",
      "Iteration 14, loss = 0.28509947\n",
      "Iteration 15, loss = 0.28321803\n",
      "Iteration 16, loss = 0.28851742\n",
      "Iteration 17, loss = 0.28646807\n",
      "Iteration 18, loss = 0.29269542\n",
      "Iteration 19, loss = 0.29207284\n",
      "Iteration 20, loss = 0.27558027\n",
      "Iteration 21, loss = 0.27571184\n",
      "Iteration 22, loss = 0.28152000\n",
      "Iteration 1, loss = 0.40701400\n",
      "Iteration 2, loss = 0.31252940\n",
      "Iteration 3, loss = 0.30277185\n",
      "Iteration 4, loss = 0.30906611\n",
      "Iteration 5, loss = 0.29275641\n",
      "Iteration 6, loss = 0.28824150\n",
      "Iteration 7, loss = 0.28439195\n",
      "Iteration 8, loss = 0.29689165\n",
      "Iteration 9, loss = 0.27265937\n",
      "Iteration 10, loss = 0.28276546\n",
      "Iteration 11, loss = 0.28344491\n",
      "Iteration 12, loss = 0.30125225\n",
      "Iteration 13, loss = 0.28142215\n",
      "Iteration 14, loss = 0.27446613\n",
      "Iteration 15, loss = 0.27726245\n",
      "Iteration 16, loss = 0.27750526\n",
      "Iteration 17, loss = 0.27472576\n",
      "Iteration 18, loss = 0.29798478\n",
      "Iteration 19, loss = 0.27344115\n",
      "Iteration 20, loss = 0.27723733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35310690\n",
      "Iteration 2, loss = 0.29126031\n",
      "Iteration 3, loss = 0.27900377\n",
      "Iteration 4, loss = 0.28017537\n",
      "Iteration 5, loss = 0.27582387\n",
      "Iteration 6, loss = 0.27587155\n",
      "Iteration 7, loss = 0.27904152\n",
      "Iteration 8, loss = 0.27379281\n",
      "Iteration 9, loss = 0.26701148\n",
      "Iteration 10, loss = 0.26656521\n",
      "Iteration 11, loss = 0.27045159\n",
      "Iteration 12, loss = 0.26603306\n",
      "Iteration 13, loss = 0.26635417\n",
      "Iteration 14, loss = 0.26360474\n",
      "Iteration 15, loss = 0.25977120\n",
      "Iteration 16, loss = 0.26397038\n",
      "Iteration 17, loss = 0.26226392\n",
      "Iteration 18, loss = 0.26406126\n",
      "Iteration 19, loss = 0.26059179\n",
      "Iteration 20, loss = 0.25974975\n",
      "Iteration 21, loss = 0.26264193\n",
      "Iteration 22, loss = 0.26257541\n",
      "Iteration 23, loss = 0.26154436\n",
      "Iteration 24, loss = 0.25674708\n",
      "Iteration 25, loss = 0.26095626\n",
      "Iteration 26, loss = 0.25592188\n",
      "Iteration 27, loss = 0.25928222\n",
      "Iteration 28, loss = 0.26028780\n",
      "Iteration 29, loss = 0.25755722\n",
      "Iteration 30, loss = 0.25395901\n",
      "Iteration 31, loss = 0.25503988\n",
      "Iteration 32, loss = 0.25724433\n",
      "Iteration 33, loss = 0.25611049\n",
      "Iteration 34, loss = 0.25579115\n",
      "Iteration 35, loss = 0.25507405\n",
      "Iteration 36, loss = 0.25810615\n",
      "Iteration 37, loss = 0.26202333\n",
      "Iteration 38, loss = 0.25692064\n",
      "Iteration 39, loss = 0.25561773\n",
      "Iteration 40, loss = 0.25325582\n",
      "Iteration 41, loss = 0.25732719\n",
      "Iteration 42, loss = 0.26141346\n",
      "Iteration 43, loss = 0.25365774\n",
      "Iteration 44, loss = 0.25505255\n",
      "Iteration 45, loss = 0.25330053\n",
      "Iteration 46, loss = 0.25549687\n",
      "Iteration 47, loss = 0.25372307\n",
      "Iteration 48, loss = 0.25542309\n",
      "Iteration 49, loss = 0.25167797\n",
      "Iteration 50, loss = 0.25172891\n",
      "Iteration 51, loss = 0.25211969\n",
      "Iteration 52, loss = 0.25466409\n",
      "Iteration 53, loss = 0.25272482\n",
      "Iteration 54, loss = 0.25550707\n",
      "Iteration 55, loss = 0.25083508\n",
      "Iteration 56, loss = 0.25303383\n",
      "Iteration 57, loss = 0.25156925\n",
      "Iteration 58, loss = 0.25434770\n",
      "Iteration 59, loss = 0.25390362\n",
      "Iteration 60, loss = 0.25116677\n",
      "Iteration 61, loss = 0.25445935\n",
      "Iteration 62, loss = 0.25365540\n",
      "Iteration 63, loss = 0.25375427\n",
      "Iteration 64, loss = 0.25168098\n",
      "Iteration 65, loss = 0.25327376\n",
      "Iteration 66, loss = 0.25462668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34412152\n",
      "Iteration 2, loss = 0.30507362\n",
      "Iteration 3, loss = 0.28688225\n",
      "Iteration 4, loss = 0.28465214\n",
      "Iteration 5, loss = 0.28346516\n",
      "Iteration 6, loss = 0.27826548\n",
      "Iteration 7, loss = 0.27111969\n",
      "Iteration 8, loss = 0.26859874\n",
      "Iteration 9, loss = 0.27663955\n",
      "Iteration 10, loss = 0.26935929\n",
      "Iteration 11, loss = 0.27348801\n",
      "Iteration 12, loss = 0.26735122\n",
      "Iteration 13, loss = 0.26954567\n",
      "Iteration 14, loss = 0.26080474\n",
      "Iteration 15, loss = 0.26695176\n",
      "Iteration 16, loss = 0.26470107\n",
      "Iteration 17, loss = 0.26914987\n",
      "Iteration 18, loss = 0.26520454\n",
      "Iteration 19, loss = 0.26286107\n",
      "Iteration 20, loss = 0.25930528\n",
      "Iteration 21, loss = 0.26202986\n",
      "Iteration 22, loss = 0.25915062\n",
      "Iteration 23, loss = 0.26093488\n",
      "Iteration 24, loss = 0.25880682\n",
      "Iteration 25, loss = 0.25974888\n",
      "Iteration 26, loss = 0.25753064\n",
      "Iteration 27, loss = 0.25910204\n",
      "Iteration 28, loss = 0.25803839\n",
      "Iteration 29, loss = 0.26096888\n",
      "Iteration 30, loss = 0.25744794\n",
      "Iteration 31, loss = 0.26151137\n",
      "Iteration 32, loss = 0.25990849\n",
      "Iteration 33, loss = 0.25746378\n",
      "Iteration 34, loss = 0.25697236\n",
      "Iteration 35, loss = 0.25544654\n",
      "Iteration 36, loss = 0.25817216\n",
      "Iteration 37, loss = 0.25586631\n",
      "Iteration 38, loss = 0.25636061\n",
      "Iteration 39, loss = 0.25574711\n",
      "Iteration 40, loss = 0.25627766\n",
      "Iteration 41, loss = 0.25819090\n",
      "Iteration 42, loss = 0.25808895\n",
      "Iteration 43, loss = 0.25391420\n",
      "Iteration 44, loss = 0.25472954\n",
      "Iteration 45, loss = 0.25111495\n",
      "Iteration 46, loss = 0.25806814\n",
      "Iteration 47, loss = 0.26367074\n",
      "Iteration 48, loss = 0.26384067\n",
      "Iteration 49, loss = 0.25948340\n",
      "Iteration 50, loss = 0.25567969\n",
      "Iteration 51, loss = 0.25502550\n",
      "Iteration 52, loss = 0.25164974\n",
      "Iteration 53, loss = 0.25167926\n",
      "Iteration 54, loss = 0.25414922\n",
      "Iteration 55, loss = 0.25283118\n",
      "Iteration 56, loss = 0.25541345\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43983587\n",
      "Iteration 2, loss = 0.31729028\n",
      "Iteration 3, loss = 0.31520438\n",
      "Iteration 4, loss = 0.32229455\n",
      "Iteration 5, loss = 0.31791594\n",
      "Iteration 6, loss = 0.31137275\n",
      "Iteration 7, loss = 0.30226321\n",
      "Iteration 8, loss = 0.29729596\n",
      "Iteration 9, loss = 0.28987493\n",
      "Iteration 10, loss = 0.29233973\n",
      "Iteration 11, loss = 0.30081873\n",
      "Iteration 12, loss = 0.28684776\n",
      "Iteration 13, loss = 0.29986260\n",
      "Iteration 14, loss = 0.29266166\n",
      "Iteration 15, loss = 0.28097592\n",
      "Iteration 16, loss = 0.28441976\n",
      "Iteration 17, loss = 0.28720398\n",
      "Iteration 18, loss = 0.28669895\n",
      "Iteration 19, loss = 0.27988257\n",
      "Iteration 20, loss = 0.28899673\n",
      "Iteration 21, loss = 0.27408737\n",
      "Iteration 22, loss = 0.29983929\n",
      "Iteration 23, loss = 0.28418638\n",
      "Iteration 24, loss = 0.29932057\n",
      "Iteration 25, loss = 0.27737012\n",
      "Iteration 26, loss = 0.30741849\n",
      "Iteration 27, loss = 0.29578433\n",
      "Iteration 28, loss = 0.28405771\n",
      "Iteration 29, loss = 0.28218164\n",
      "Iteration 30, loss = 0.28136491\n",
      "Iteration 31, loss = 0.28889972\n",
      "Iteration 32, loss = 0.28979961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40000571\n",
      "Iteration 2, loss = 0.37220302\n",
      "Iteration 3, loss = 0.32016539\n",
      "Iteration 4, loss = 0.30737767\n",
      "Iteration 5, loss = 0.30925138\n",
      "Iteration 6, loss = 0.29328554\n",
      "Iteration 7, loss = 0.30448788\n",
      "Iteration 8, loss = 0.29331085\n",
      "Iteration 9, loss = 0.29151593\n",
      "Iteration 10, loss = 0.29175163\n",
      "Iteration 11, loss = 0.29018645\n",
      "Iteration 12, loss = 0.27903685\n",
      "Iteration 13, loss = 0.28337694\n",
      "Iteration 14, loss = 0.28741029\n",
      "Iteration 15, loss = 0.28446943\n",
      "Iteration 16, loss = 0.28255836\n",
      "Iteration 17, loss = 0.28286942\n",
      "Iteration 18, loss = 0.28012104\n",
      "Iteration 19, loss = 0.28975265\n",
      "Iteration 20, loss = 0.28423648\n",
      "Iteration 21, loss = 0.28825179\n",
      "Iteration 22, loss = 0.28541930\n",
      "Iteration 23, loss = 0.28465797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33969944\n",
      "Iteration 2, loss = 0.31447001\n",
      "Iteration 3, loss = 0.30077515\n",
      "Iteration 4, loss = 0.29296112\n",
      "Iteration 5, loss = 0.29624165\n",
      "Iteration 6, loss = 0.28895337\n",
      "Iteration 7, loss = 0.28212130\n",
      "Iteration 8, loss = 0.29246379\n",
      "Iteration 9, loss = 0.28455260\n",
      "Iteration 10, loss = 0.28570243\n",
      "Iteration 11, loss = 0.27872090\n",
      "Iteration 12, loss = 0.27897943\n",
      "Iteration 13, loss = 0.27810370\n",
      "Iteration 14, loss = 0.28219594\n",
      "Iteration 15, loss = 0.27571478\n",
      "Iteration 16, loss = 0.27969054\n",
      "Iteration 17, loss = 0.28249519\n",
      "Iteration 18, loss = 0.28444932\n",
      "Iteration 19, loss = 0.28018518\n",
      "Iteration 20, loss = 0.27543873\n",
      "Iteration 21, loss = 0.28336504\n",
      "Iteration 22, loss = 0.28011209\n",
      "Iteration 23, loss = 0.28260725\n",
      "Iteration 24, loss = 0.28103323\n",
      "Iteration 25, loss = 0.27666036\n",
      "Iteration 26, loss = 0.27555071\n",
      "Iteration 27, loss = 0.27562886\n",
      "Iteration 28, loss = 0.27443534\n",
      "Iteration 29, loss = 0.27585572\n",
      "Iteration 30, loss = 0.28362881\n",
      "Iteration 31, loss = 0.28319067\n",
      "Iteration 32, loss = 0.27869514\n",
      "Iteration 33, loss = 0.27837626\n",
      "Iteration 34, loss = 0.27506372\n",
      "Iteration 35, loss = 0.28144086\n",
      "Iteration 36, loss = 0.28676845\n",
      "Iteration 37, loss = 0.27660267\n",
      "Iteration 38, loss = 0.28035506\n",
      "Iteration 39, loss = 0.27865599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34238184\n",
      "Iteration 2, loss = 0.31384285\n",
      "Iteration 3, loss = 0.30361478\n",
      "Iteration 4, loss = 0.30548195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.29190243Iteration 1, loss = 0.45841544\n",
      "Iteration 2, loss = 0.34654656\n",
      "Iteration 3, loss = 0.31394718\n",
      "Iteration 4, loss = 0.28977850\n",
      "Iteration 5, loss = 0.29568180\n",
      "Iteration 6, loss = 0.31243977\n",
      "Iteration 7, loss = 0.30642095\n",
      "Iteration 8, loss = 0.29913575\n",
      "Iteration 9, loss = 0.29182255\n",
      "Iteration 10, loss = 0.29886972\n",
      "Iteration 11, loss = 0.29874969\n",
      "Iteration 12, loss = 0.28401646\n",
      "Iteration 13, loss = 0.28735091\n",
      "Iteration 14, loss = 0.29277096\n",
      "Iteration 15, loss = 0.28090091\n",
      "Iteration 16, loss = 0.28376037\n",
      "Iteration 17, loss = 0.29561624\n",
      "Iteration 18, loss = 0.27745587\n",
      "Iteration 19, loss = 0.27463096\n",
      "Iteration 20, loss = 0.27954703\n",
      "Iteration 21, loss = 0.28071557\n",
      "Iteration 22, loss = 0.27581267\n",
      "Iteration 23, loss = 0.28211803\n",
      "Iteration 24, loss = 0.27460174\n",
      "Iteration 25, loss = 0.27419507\n",
      "Iteration 26, loss = 0.28064788\n",
      "Iteration 27, loss = 0.27651367\n",
      "Iteration 28, loss = 0.27514478\n",
      "Iteration 29, loss = 0.27130439\n",
      "Iteration 30, loss = 0.26952141\n",
      "Iteration 31, loss = 0.27813779\n",
      "Iteration 32, loss = 0.26822979\n",
      "Iteration 33, loss = 0.26409080\n",
      "Iteration 34, loss = 0.26880538\n",
      "Iteration 35, loss = 0.27146014\n",
      "Iteration 36, loss = 0.26879669\n",
      "Iteration 37, loss = 0.27306933\n",
      "Iteration 38, loss = 0.27849066\n",
      "Iteration 39, loss = 0.27951046\n",
      "Iteration 40, loss = 0.27310617\n",
      "Iteration 41, loss = 0.27022008\n",
      "Iteration 42, loss = 0.26748070\n",
      "Iteration 43, loss = 0.26589211\n",
      "Iteration 44, loss = 0.27344177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35310690\n",
      "Iteration 2, loss = 0.29126031\n",
      "Iteration 3, loss = 0.27900377\n",
      "Iteration 4, loss = 0.28017537\n",
      "Iteration 5, loss = 0.27582387\n",
      "Iteration 6, loss = 0.27587155\n",
      "Iteration 7, loss = 0.27904152\n",
      "Iteration 8, loss = 0.27379281\n",
      "Iteration 9, loss = 0.26701148\n",
      "Iteration 10, loss = 0.26656521\n",
      "Iteration 11, loss = 0.27045159\n",
      "Iteration 12, loss = 0.26603306\n",
      "Iteration 13, loss = 0.26635417\n",
      "Iteration 14, loss = 0.26360474\n",
      "Iteration 15, loss = 0.25977120\n",
      "Iteration 16, loss = 0.26397038\n",
      "Iteration 17, loss = 0.26226392\n",
      "Iteration 18, loss = 0.26406126\n",
      "Iteration 19, loss = 0.26059179\n",
      "Iteration 20, loss = 0.25974975\n",
      "Iteration 21, loss = 0.26264193\n",
      "Iteration 22, loss = 0.26257541\n",
      "Iteration 23, loss = 0.26154436\n",
      "Iteration 24, loss = 0.25674708\n",
      "Iteration 25, loss = 0.26095626\n",
      "Iteration 26, loss = 0.25592188\n",
      "Iteration 27, loss = 0.25928222\n",
      "Iteration 28, loss = 0.26028780\n",
      "Iteration 29, loss = 0.25755722\n",
      "Iteration 30, loss = 0.25395901\n",
      "Iteration 31, loss = 0.25503988\n",
      "Iteration 32, loss = 0.25724433\n",
      "Iteration 33, loss = 0.25611049\n",
      "Iteration 34, loss = 0.25579115\n",
      "Iteration 35, loss = 0.25507405\n",
      "Iteration 36, loss = 0.25810615\n",
      "Iteration 37, loss = 0.26202333\n",
      "Iteration 38, loss = 0.25692064\n",
      "Iteration 39, loss = 0.25561773\n",
      "Iteration 40, loss = 0.25325582\n",
      "Iteration 41, loss = 0.25732719\n",
      "Iteration 42, loss = 0.26141346\n",
      "Iteration 43, loss = 0.25365774\n",
      "Iteration 44, loss = 0.25505255\n",
      "Iteration 45, loss = 0.25330053\n",
      "Iteration 46, loss = 0.25549687\n",
      "Iteration 47, loss = 0.25372307\n",
      "Iteration 48, loss = 0.25542309\n",
      "Iteration 49, loss = 0.25167797\n",
      "Iteration 50, loss = 0.25172891\n",
      "Iteration 51, loss = 0.25211969\n",
      "Iteration 52, loss = 0.25466409\n",
      "Iteration 53, loss = 0.25272482\n",
      "Iteration 54, loss = 0.25550707\n",
      "Iteration 55, loss = 0.25083508\n",
      "Iteration 56, loss = 0.25303383\n",
      "Iteration 57, loss = 0.25156925\n",
      "Iteration 58, loss = 0.25434770\n",
      "Iteration 59, loss = 0.25390362\n",
      "Iteration 60, loss = 0.25116677\n",
      "Iteration 61, loss = 0.25445935\n",
      "Iteration 62, loss = 0.25365540\n",
      "Iteration 63, loss = 0.25375427\n",
      "Iteration 64, loss = 0.25168098\n",
      "Iteration 65, loss = 0.25327376\n",
      "Iteration 66, loss = 0.25462668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34412152\n",
      "Iteration 2, loss = 0.30507362\n",
      "Iteration 3, loss = 0.28688225\n",
      "Iteration 4, loss = 0.28465214\n",
      "Iteration 5, loss = 0.28346516\n",
      "Iteration 6, loss = 0.27826548\n",
      "Iteration 7, loss = 0.27111969\n",
      "Iteration 8, loss = 0.26859874\n",
      "Iteration 9, loss = 0.27663955\n",
      "Iteration 10, loss = 0.26935929\n",
      "Iteration 11, loss = 0.27348801\n",
      "Iteration 12, loss = 0.26735122\n",
      "Iteration 13, loss = 0.26954567\n",
      "Iteration 14, loss = 0.26080474\n",
      "Iteration 15, loss = 0.26695176\n",
      "Iteration 16, loss = 0.26470107\n",
      "Iteration 17, loss = 0.26914987\n",
      "Iteration 18, loss = 0.26520454\n",
      "Iteration 19, loss = 0.26286107\n",
      "Iteration 20, loss = 0.25930528\n",
      "Iteration 21, loss = 0.26202986\n",
      "Iteration 22, loss = 0.25915062\n",
      "Iteration 23, loss = 0.26093488\n",
      "Iteration 24, loss = 0.25880682\n",
      "Iteration 25, loss = 0.25974888\n",
      "Iteration 26, loss = 0.25753064\n",
      "Iteration 27, loss = 0.25910204\n",
      "Iteration 28, loss = 0.25803839\n",
      "Iteration 29, loss = 0.26096888\n",
      "Iteration 30, loss = 0.25744794\n",
      "Iteration 31, loss = 0.26151137\n",
      "Iteration 32, loss = 0.25990849\n",
      "Iteration 33, loss = 0.25746378\n",
      "Iteration 34, loss = 0.25697236\n",
      "Iteration 35, loss = 0.25544654\n",
      "Iteration 36, loss = 0.25817216\n",
      "Iteration 37, loss = 0.25586631\n",
      "Iteration 38, loss = 0.25636061\n",
      "Iteration 39, loss = 0.25574711\n",
      "Iteration 40, loss = 0.25627766\n",
      "Iteration 41, loss = 0.25819090\n",
      "Iteration 42, loss = 0.25808895\n",
      "Iteration 43, loss = 0.25391420\n",
      "Iteration 44, loss = 0.25472954\n",
      "Iteration 45, loss = 0.25111495\n",
      "Iteration 46, loss = 0.25806814\n",
      "Iteration 47, loss = 0.26367074\n",
      "Iteration 48, loss = 0.26384067\n",
      "Iteration 49, loss = 0.25948340\n",
      "Iteration 50, loss = 0.25567969\n",
      "Iteration 51, loss = 0.25502550\n",
      "Iteration 52, loss = 0.25164974\n",
      "Iteration 53, loss = 0.25167926\n",
      "Iteration 54, loss = 0.25414922\n",
      "Iteration 55, loss = 0.25283118\n",
      "Iteration 56, loss = 0.25541345\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42228354\n",
      "Iteration 2, loss = 0.32215967\n",
      "Iteration 3, loss = 0.33908476\n",
      "Iteration 4, loss = 0.32521411\n",
      "Iteration 5, loss = 0.32044450\n",
      "Iteration 6, loss = 0.31179280\n",
      "Iteration 7, loss = 0.29381041\n",
      "Iteration 8, loss = 0.28349443\n",
      "Iteration 9, loss = 0.31964936\n",
      "Iteration 10, loss = 0.29322928\n",
      "Iteration 11, loss = 0.30093576\n",
      "Iteration 12, loss = 0.28619338\n",
      "Iteration 13, loss = 0.29023536\n",
      "Iteration 14, loss = 0.28329078\n",
      "Iteration 15, loss = 0.28776916\n",
      "Iteration 16, loss = 0.30342951\n",
      "Iteration 17, loss = 0.29841244\n",
      "Iteration 18, loss = 0.28476726\n",
      "Iteration 19, loss = 0.29218428\n",
      "Iteration 20, loss = 0.30346762\n",
      "Iteration 21, loss = 0.27419065\n",
      "Iteration 22, loss = 0.28124028\n",
      "Iteration 23, loss = 0.28219874\n",
      "Iteration 24, loss = 0.28055917\n",
      "Iteration 25, loss = 0.29387752\n",
      "Iteration 26, loss = 0.27792768\n",
      "Iteration 27, loss = 0.28441130\n",
      "Iteration 28, loss = 0.28188538\n",
      "Iteration 29, loss = 0.27375106\n",
      "Iteration 30, loss = 0.28203961\n",
      "Iteration 31, loss = 0.27606503\n",
      "Iteration 32, loss = 0.28179319\n",
      "Iteration 33, loss = 0.28937709\n",
      "Iteration 34, loss = 0.28695145\n",
      "Iteration 35, loss = 0.28219006\n",
      "Iteration 36, loss = 0.28241085\n",
      "Iteration 37, loss = 0.29089766\n",
      "Iteration 38, loss = 0.28637184\n",
      "Iteration 39, loss = 0.27293665\n",
      "Iteration 40, loss = 0.28057402\n",
      "Iteration 41, loss = 0.28730272\n",
      "Iteration 42, loss = 0.27935682\n",
      "Iteration 43, loss = 0.27516534\n",
      "Iteration 44, loss = 0.28736978\n",
      "Iteration 45, loss = 0.27591741\n",
      "Iteration 46, loss = 0.28472459\n",
      "Iteration 47, loss = 0.27635595\n",
      "Iteration 48, loss = 0.27355454\n",
      "Iteration 49, loss = 0.28148498\n",
      "Iteration 50, loss = 0.27211471\n",
      "Iteration 51, loss = 0.28507277\n",
      "Iteration 52, loss = 0.27622686\n",
      "Iteration 53, loss = 0.27450023\n",
      "Iteration 54, loss = 0.28213408\n",
      "Iteration 55, loss = 0.28091597\n",
      "Iteration 56, loss = 0.28580932\n",
      "Iteration 57, loss = 0.27667131\n",
      "Iteration 58, loss = 0.28859642\n",
      "Iteration 59, loss = 0.28628586\n",
      "Iteration 60, loss = 0.28737237\n",
      "Iteration 61, loss = 0.27819459\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35012753\n",
      "Iteration 2, loss = 0.31187009\n",
      "Iteration 3, loss = 0.30081213\n",
      "Iteration 4, loss = 0.30203804\n",
      "Iteration 5, loss = 0.29962846\n",
      "Iteration 6, loss = 0.29144054\n",
      "Iteration 7, loss = 0.30352538\n",
      "Iteration 8, loss = 0.28212710\n",
      "Iteration 9, loss = 0.27611418\n",
      "Iteration 10, loss = 0.28331971\n",
      "Iteration 11, loss = 0.27936639\n",
      "Iteration 12, loss = 0.28649040\n",
      "Iteration 13, loss = 0.28354700\n",
      "Iteration 14, loss = 0.28269315\n",
      "Iteration 15, loss = 0.27968561\n",
      "Iteration 16, loss = 0.27436379\n",
      "Iteration 17, loss = 0.27809853\n",
      "Iteration 18, loss = 0.27708354\n",
      "Iteration 19, loss = 0.27994934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.45841544\n",
      "Iteration 2, loss = 0.34654656\n",
      "Iteration 3, loss = 0.31394718\n",
      "Iteration 4, loss = 0.28977850\n",
      "Iteration 5, loss = 0.29568180\n",
      "Iteration 6, loss = 0.31243977\n",
      "Iteration 7, loss = 0.30642095\n",
      "Iteration 8, loss = 0.29913575\n",
      "Iteration 9, loss = 0.29182255\n",
      "Iteration 10, loss = 0.29886972\n",
      "Iteration 11, loss = 0.29874969\n",
      "Iteration 12, loss = 0.28401646\n",
      "Iteration 13, loss = 0.28735091\n",
      "Iteration 14, loss = 0.29277096\n",
      "Iteration 15, loss = 0.28090091\n",
      "Iteration 16, loss = 0.28376037\n",
      "Iteration 17, loss = 0.29561624\n",
      "Iteration 18, loss = 0.27745587\n",
      "Iteration 19, loss = 0.27463096\n",
      "Iteration 20, loss = 0.27954703\n",
      "Iteration 21, loss = 0.28071557\n",
      "Iteration 22, loss = 0.27581267\n",
      "Iteration 23, loss = 0.28211803\n",
      "Iteration 24, loss = 0.27460174\n",
      "Iteration 25, loss = 0.27419507\n",
      "Iteration 26, loss = 0.28064788\n",
      "Iteration 27, loss = 0.27651367\n",
      "Iteration 28, loss = 0.27514478\n",
      "Iteration 29, loss = 0.27130439\n",
      "Iteration 30, loss = 0.26952141\n",
      "Iteration 31, loss = 0.27813779\n",
      "Iteration 32, loss = 0.26822979\n",
      "Iteration 33, loss = 0.26409080\n",
      "Iteration 34, loss = 0.26880538\n",
      "Iteration 35, loss = 0.27146014\n",
      "Iteration 36, loss = 0.26879669\n",
      "Iteration 37, loss = 0.27306933\n",
      "Iteration 38, loss = 0.27849066\n",
      "Iteration 39, loss = 0.27951046\n",
      "Iteration 40, loss = 0.27310617\n",
      "Iteration 41, loss = 0.27022008\n",
      "Iteration 42, loss = 0.26748070\n",
      "Iteration 43, loss = 0.26589211\n",
      "Iteration 44, loss = 0.27344177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34291501\n",
      "Iteration 2, loss = 0.29500607\n",
      "Iteration 3, loss = 0.28058830\n",
      "Iteration 4, loss = 0.27178575\n",
      "Iteration 5, loss = 0.27609873\n",
      "Iteration 6, loss = 0.27801016\n",
      "Iteration 7, loss = 0.27528812\n",
      "Iteration 8, loss = 0.27252364\n",
      "Iteration 9, loss = 0.27038149\n",
      "Iteration 10, loss = 0.26240489\n",
      "Iteration 11, loss = 0.26627241\n",
      "Iteration 12, loss = 0.26774125\n",
      "Iteration 13, loss = 0.26427118\n",
      "Iteration 14, loss = 0.26522747\n",
      "Iteration 15, loss = 0.26211975\n",
      "Iteration 16, loss = 0.26155282\n",
      "Iteration 17, loss = 0.26259581\n",
      "Iteration 18, loss = 0.26852240\n",
      "Iteration 19, loss = 0.26405049\n",
      "Iteration 20, loss = 0.25933372\n",
      "Iteration 21, loss = 0.26023630\n",
      "Iteration 22, loss = 0.26001604\n",
      "Iteration 23, loss = 0.26179863\n",
      "Iteration 24, loss = 0.25826912\n",
      "Iteration 25, loss = 0.26377833\n",
      "Iteration 26, loss = 0.25846869\n",
      "Iteration 27, loss = 0.26156723\n",
      "Iteration 28, loss = 0.25755943\n",
      "Iteration 29, loss = 0.25935396\n",
      "Iteration 30, loss = 0.26148714\n",
      "Iteration 31, loss = 0.26253635\n",
      "Iteration 32, loss = 0.26294067\n",
      "Iteration 33, loss = 0.26538209\n",
      "Iteration 34, loss = 0.25929151\n",
      "Iteration 35, loss = 0.25784545\n",
      "Iteration 36, loss = 0.26695703\n",
      "Iteration 37, loss = 0.25982008\n",
      "Iteration 38, loss = 0.26353839\n",
      "Iteration 39, loss = 0.25585629\n",
      "Iteration 40, loss = 0.25655704\n",
      "Iteration 41, loss = 0.26105007\n",
      "Iteration 42, loss = 0.25925358\n",
      "Iteration 43, loss = 0.25577151\n",
      "Iteration 44, loss = 0.25501973\n",
      "Iteration 45, loss = 0.25483249\n",
      "Iteration 46, loss = 0.25843106\n",
      "Iteration 47, loss = 0.26708715\n",
      "Iteration 48, loss = 0.26090175\n",
      "Iteration 49, loss = 0.25412638\n",
      "Iteration 50, loss = 0.25729877\n",
      "Iteration 51, loss = 0.25798209\n",
      "Iteration 52, loss = 0.25305552\n",
      "Iteration 53, loss = 0.25570322\n",
      "Iteration 54, loss = 0.25517822\n",
      "Iteration 55, loss = 0.25446731\n",
      "Iteration 56, loss = 0.25493160\n",
      "Iteration 57, loss = 0.25423001\n",
      "Iteration 58, loss = 0.25343570\n",
      "Iteration 59, loss = 0.25394544\n",
      "Iteration 60, loss = 0.25531175\n",
      "Iteration 61, loss = 0.25594552\n",
      "Iteration 62, loss = 0.25496612\n",
      "Iteration 63, loss = 0.25478571\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34046450\n",
      "Iteration 2, loss = 0.29284170\n",
      "Iteration 3, loss = 0.29636928\n",
      "Iteration 4, loss = 0.29038097\n",
      "Iteration 5, loss = 0.28046988\n",
      "Iteration 6, loss = 0.28027077\n",
      "Iteration 7, loss = 0.27241974\n",
      "Iteration 8, loss = 0.26979722\n",
      "Iteration 9, loss = 0.27498592\n",
      "Iteration 10, loss = 0.27103554\n",
      "Iteration 11, loss = 0.27342860\n",
      "Iteration 12, loss = 0.26846742\n",
      "Iteration 13, loss = 0.27085779\n",
      "Iteration 14, loss = 0.27121645\n",
      "Iteration 15, loss = 0.26490563\n",
      "Iteration 16, loss = 0.26872473\n",
      "Iteration 17, loss = 0.26643400\n",
      "Iteration 18, loss = 0.26899433\n",
      "Iteration 19, loss = 0.26454798\n",
      "Iteration 20, loss = 0.27697933\n",
      "Iteration 21, loss = 0.26856130\n",
      "Iteration 22, loss = 0.26171339\n",
      "Iteration 23, loss = 0.25994784\n",
      "Iteration 24, loss = 0.25915033\n",
      "Iteration 25, loss = 0.26295082\n",
      "Iteration 26, loss = 0.26389108\n",
      "Iteration 27, loss = 0.26106324\n",
      "Iteration 28, loss = 0.26101430\n",
      "Iteration 29, loss = 0.26235098\n",
      "Iteration 30, loss = 0.26197620\n",
      "Iteration 31, loss = 0.26112588\n",
      "Iteration 32, loss = 0.26748913\n",
      "Iteration 33, loss = 0.26332203\n",
      "Iteration 34, loss = 0.25861028\n",
      "Iteration 35, loss = 0.25973752\n",
      "Iteration 36, loss = 0.25778477\n",
      "Iteration 37, loss = 0.25819609\n",
      "Iteration 38, loss = 0.25727670\n",
      "Iteration 39, loss = 0.25645869\n",
      "Iteration 40, loss = 0.25629221\n",
      "Iteration 41, loss = 0.26045788\n",
      "Iteration 42, loss = 0.25970154\n",
      "Iteration 43, loss = 0.25997620\n",
      "Iteration 44, loss = 0.25779954\n",
      "Iteration 45, loss = 0.25869989\n",
      "Iteration 46, loss = 0.25951138\n",
      "Iteration 47, loss = 0.25679167\n",
      "Iteration 48, loss = 0.25538106\n",
      "Iteration 49, loss = 0.25529408\n",
      "Iteration 50, loss = 0.25724414\n",
      "Iteration 51, loss = 0.25846311\n",
      "Iteration 52, loss = 0.25817688\n",
      "Iteration 53, loss = 0.25264486\n",
      "Iteration 54, loss = 0.25580812\n",
      "Iteration 55, loss = 0.26435318\n",
      "Iteration 56, loss = 0.25749681\n",
      "Iteration 57, loss = 0.25702260\n",
      "Iteration 58, loss = 0.25832108\n",
      "Iteration 59, loss = 0.25497284\n",
      "Iteration 60, loss = 0.25501213\n",
      "Iteration 61, loss = 0.25485345\n",
      "Iteration 62, loss = 0.25547245\n",
      "Iteration 63, loss = 0.25398175\n",
      "Iteration 64, loss = 0.25532931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.39443838\n",
      "Iteration 2, loss = 0.33292099\n",
      "Iteration 3, loss = 0.32900879\n",
      "Iteration 4, loss = 0.29889072\n",
      "Iteration 5, loss = 0.29166310\n",
      "Iteration 6, loss = 0.29106813\n",
      "Iteration 7, loss = 0.28847920\n",
      "Iteration 8, loss = 0.30751759\n",
      "Iteration 9, loss = 0.30405878\n",
      "Iteration 10, loss = 0.30945946\n",
      "Iteration 11, loss = 0.29037193\n",
      "Iteration 12, loss = 0.28887894\n",
      "Iteration 13, loss = 0.27965071\n",
      "Iteration 14, loss = 0.30863038\n",
      "Iteration 15, loss = 0.28092835\n",
      "Iteration 16, loss = 0.28911840\n",
      "Iteration 17, loss = 0.27822269\n",
      "Iteration 18, loss = 0.27948408\n",
      "Iteration 19, loss = 0.28948319\n",
      "Iteration 20, loss = 0.29806050\n",
      "Iteration 21, loss = 0.28314987\n",
      "Iteration 22, loss = 0.27505109\n",
      "Iteration 23, loss = 0.27916823\n",
      "Iteration 24, loss = 0.27733262\n",
      "Iteration 25, loss = 0.27643289\n",
      "Iteration 26, loss = 0.29130423\n",
      "Iteration 27, loss = 0.28522523\n",
      "Iteration 28, loss = 0.28498711\n",
      "Iteration 29, loss = 0.29282798\n",
      "Iteration 30, loss = 0.28597295\n",
      "Iteration 31, loss = 0.27376143\n",
      "Iteration 32, loss = 0.27422798\n",
      "Iteration 33, loss = 0.27979881\n",
      "Iteration 34, loss = 0.28598303\n",
      "Iteration 35, loss = 0.28640313\n",
      "Iteration 36, loss = 0.28398905\n",
      "Iteration 37, loss = 0.28586615\n",
      "Iteration 38, loss = 0.28168766\n",
      "Iteration 39, loss = 0.28092529\n",
      "Iteration 40, loss = 0.28327101\n",
      "Iteration 41, loss = 0.28525217\n",
      "Iteration 42, loss = 0.28102315\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35012753\n",
      "Iteration 2, loss = 0.31187009\n",
      "Iteration 3, loss = 0.30081213\n",
      "Iteration 4, loss = 0.30203804\n",
      "Iteration 5, loss = 0.29962846\n",
      "Iteration 6, loss = 0.29144054\n",
      "Iteration 7, loss = 0.30352538\n",
      "Iteration 8, loss = 0.28212710\n",
      "Iteration 9, loss = 0.27611418\n",
      "Iteration 10, loss = 0.28331971\n",
      "Iteration 11, loss = 0.27936639\n",
      "Iteration 12, loss = 0.28649040\n",
      "Iteration 13, loss = 0.28354700\n",
      "Iteration 14, loss = 0.28269315\n",
      "Iteration 15, loss = 0.27968561\n",
      "Iteration 16, loss = 0.27436379\n",
      "Iteration 17, loss = 0.27809853\n",
      "Iteration 18, loss = 0.27708354\n",
      "Iteration 19, loss = 0.27994934\n",
      "Iteration 20, loss = 0.28042078\n",
      "Iteration 21, loss = 0.27569442\n",
      "Iteration 22, loss = 0.27933167\n",
      "Iteration 23, loss = 0.27792424\n",
      "Iteration 24, loss = 0.27968770\n",
      "Iteration 25, loss = 0.27708390\n",
      "Iteration 26, loss = 0.28308456\n",
      "Iteration 27, loss = 0.28120179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03730475\n",
      "Iteration 2, loss = 0.52652208\n",
      "Iteration 3, loss = 0.53688611\n",
      "Iteration 1, loss = 0.44875388\n",
      "Iteration 2, loss = 0.32803265\n",
      "Iteration 3, loss = 0.30295411\n",
      "Iteration 4, loss = 0.30525981\n",
      "Iteration 5, loss = 0.28385153\n",
      "Iteration 6, loss = 0.28093854\n",
      "Iteration 7, loss = 0.28211341\n",
      "Iteration 8, loss = 0.27435152\n",
      "Iteration 9, loss = 0.28466128\n",
      "Iteration 10, loss = 0.27983947\n",
      "Iteration 11, loss = 0.27222857\n",
      "Iteration 12, loss = 0.27685946\n",
      "Iteration 13, loss = 0.27565362\n",
      "Iteration 14, loss = 0.27264528\n",
      "Iteration 15, loss = 0.27710601\n",
      "Iteration 16, loss = 0.27124760\n",
      "Iteration 17, loss = 0.26814518\n",
      "Iteration 18, loss = 0.27494164\n",
      "Iteration 19, loss = 0.27865161\n",
      "Iteration 20, loss = 0.27068859\n",
      "Iteration 21, loss = 0.26518025\n",
      "Iteration 22, loss = 0.26931815\n",
      "Iteration 23, loss = 0.26274321\n",
      "Iteration 24, loss = 0.26806973\n",
      "Iteration 25, loss = 0.26914704\n",
      "Iteration 26, loss = 0.26481809\n",
      "Iteration 27, loss = 0.26363173\n",
      "Iteration 28, loss = 0.25824601\n",
      "Iteration 29, loss = 0.26876245\n",
      "Iteration 30, loss = 0.26608196\n",
      "Iteration 31, loss = 0.26632558\n",
      "Iteration 32, loss = 0.26291151\n",
      "Iteration 33, loss = 0.26366030\n",
      "Iteration 34, loss = 0.26617174\n",
      "Iteration 35, loss = 0.26174809\n",
      "Iteration 36, loss = 0.26166652\n",
      "Iteration 37, loss = 0.25816570\n",
      "Iteration 38, loss = 0.25820141\n",
      "Iteration 39, loss = 0.26088375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34291501\n",
      "Iteration 2, loss = 0.29500607\n",
      "Iteration 3, loss = 0.28058830\n",
      "Iteration 4, loss = 0.27178575\n",
      "Iteration 5, loss = 0.27609873\n",
      "Iteration 6, loss = 0.27801016\n",
      "Iteration 7, loss = 0.27528812\n",
      "Iteration 8, loss = 0.27252364\n",
      "Iteration 9, loss = 0.27038149\n",
      "Iteration 10, loss = 0.26240489\n",
      "Iteration 11, loss = 0.26627241\n",
      "Iteration 12, loss = 0.26774125\n",
      "Iteration 13, loss = 0.26427118\n",
      "Iteration 14, loss = 0.26522747\n",
      "Iteration 15, loss = 0.26211975\n",
      "Iteration 16, loss = 0.26155282\n",
      "Iteration 17, loss = 0.26259581\n",
      "Iteration 18, loss = 0.26852240\n",
      "Iteration 19, loss = 0.26405049\n",
      "Iteration 20, loss = 0.25933372\n",
      "Iteration 21, loss = 0.26023630\n",
      "Iteration 22, loss = 0.26001604\n",
      "Iteration 23, loss = 0.26179863\n",
      "Iteration 24, loss = 0.25826912\n",
      "Iteration 25, loss = 0.26377833\n",
      "Iteration 26, loss = 0.25846869\n",
      "Iteration 27, loss = 0.26156723\n",
      "Iteration 28, loss = 0.25755943\n",
      "Iteration 29, loss = 0.25935396\n",
      "Iteration 30, loss = 0.26148714\n",
      "Iteration 31, loss = 0.26253635\n",
      "Iteration 32, loss = 0.26294067\n",
      "Iteration 33, loss = 0.26538209\n",
      "Iteration 34, loss = 0.25929151\n",
      "Iteration 35, loss = 0.25784545\n",
      "Iteration 36, loss = 0.26695703\n",
      "Iteration 37, loss = 0.25982008\n",
      "Iteration 38, loss = 0.26353839\n",
      "Iteration 39, loss = 0.25585629\n",
      "Iteration 40, loss = 0.25655704\n",
      "Iteration 41, loss = 0.26105007\n",
      "Iteration 42, loss = 0.25925358\n",
      "Iteration 43, loss = 0.25577151\n",
      "Iteration 44, loss = 0.25501973\n",
      "Iteration 45, loss = 0.25483249\n",
      "Iteration 46, loss = 0.25843106\n",
      "Iteration 47, loss = 0.26708715\n",
      "Iteration 48, loss = 0.26090175\n",
      "Iteration 49, loss = 0.25412638\n",
      "Iteration 50, loss = 0.25729877\n",
      "Iteration 51, loss = 0.25798209\n",
      "Iteration 52, loss = 0.25305552\n",
      "Iteration 53, loss = 0.25570322\n",
      "Iteration 54, loss = 0.25517822\n",
      "Iteration 55, loss = 0.25446731\n",
      "Iteration 56, loss = 0.25493160\n",
      "Iteration 57, loss = 0.25423001\n",
      "Iteration 58, loss = 0.25343570\n",
      "Iteration 59, loss = 0.25394544\n",
      "Iteration 60, loss = 0.25531175\n",
      "Iteration 61, loss = 0.25594552\n",
      "Iteration 62, loss = 0.25496612\n",
      "Iteration 63, loss = 0.25478571\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33997125\n",
      "Iteration 2, loss = 0.29539368\n",
      "Iteration 3, loss = 0.29507243\n",
      "Iteration 4, loss = 0.28532762\n",
      "Iteration 5, loss = 0.28135136\n",
      "Iteration 6, loss = 0.28502956\n",
      "Iteration 7, loss = 0.27266012\n",
      "Iteration 8, loss = 0.27748809\n",
      "Iteration 9, loss = 0.27056093\n",
      "Iteration 10, loss = 0.27045629\n",
      "Iteration 11, loss = 0.27345408\n",
      "Iteration 12, loss = 0.27578223\n",
      "Iteration 13, loss = 0.26338679\n",
      "Iteration 14, loss = 0.27297334\n",
      "Iteration 15, loss = 0.26640932\n",
      "Iteration 16, loss = 0.26764288\n",
      "Iteration 17, loss = 0.26415566\n",
      "Iteration 18, loss = 0.26213627\n",
      "Iteration 19, loss = 0.26431007\n",
      "Iteration 20, loss = 0.26140680\n",
      "Iteration 21, loss = 0.26196546\n",
      "Iteration 22, loss = 0.26047105\n",
      "Iteration 23, loss = 0.25917369\n",
      "Iteration 24, loss = 0.26111586\n",
      "Iteration 25, loss = 0.25987146\n",
      "Iteration 26, loss = 0.25732424\n",
      "Iteration 27, loss = 0.25997891\n",
      "Iteration 28, loss = 0.25839253\n",
      "Iteration 29, loss = 0.25611018\n",
      "Iteration 30, loss = 0.25857453\n",
      "Iteration 31, loss = 0.26175838\n",
      "Iteration 32, loss = 0.25661687\n",
      "Iteration 33, loss = 0.26037248\n",
      "Iteration 34, loss = 0.26065735\n",
      "Iteration 35, loss = 0.26076017\n",
      "Iteration 36, loss = 0.25959812\n",
      "Iteration 37, loss = 0.25866199\n",
      "Iteration 38, loss = 0.25746707\n",
      "Iteration 39, loss = 0.26004436\n",
      "Iteration 40, loss = 0.26084979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45275526\n",
      "Iteration 2, loss = 0.32560499\n",
      "Iteration 3, loss = 0.31452355\n",
      "Iteration 4, loss = 0.32580662\n",
      "Iteration 5, loss = 0.29341889\n",
      "Iteration 6, loss = 0.28483689\n",
      "Iteration 7, loss = 0.29165098\n",
      "Iteration 8, loss = 0.28636074\n",
      "Iteration 9, loss = 0.28689821\n",
      "Iteration 10, loss = 0.29008393\n",
      "Iteration 11, loss = 0.27521144\n",
      "Iteration 12, loss = 0.27387334\n",
      "Iteration 13, loss = 0.27391479\n",
      "Iteration 14, loss = 0.28197902\n",
      "Iteration 15, loss = 0.27639178\n",
      "Iteration 16, loss = 0.28908648\n",
      "Iteration 17, loss = 0.29012385\n",
      "Iteration 18, loss = 0.28440596\n",
      "Iteration 19, loss = 0.27585607\n",
      "Iteration 20, loss = 0.28933934\n",
      "Iteration 21, loss = 0.27958911\n",
      "Iteration 22, loss = 0.28063368\n",
      "Iteration 23, loss = 0.27714014\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43983587\n",
      "Iteration 2, loss = 0.31729028\n",
      "Iteration 3, loss = 0.31520438\n",
      "Iteration 4, loss = 0.32229455\n",
      "Iteration 5, loss = 0.31791594\n",
      "Iteration 6, loss = 0.31137275\n",
      "Iteration 7, loss = 0.30226321\n",
      "Iteration 8, loss = 0.29729596\n",
      "Iteration 9, loss = 0.28987493\n",
      "Iteration 10, loss = 0.29233973\n",
      "Iteration 11, loss = 0.30081873\n",
      "Iteration 12, loss = 0.28684776\n",
      "Iteration 13, loss = 0.29986260\n",
      "Iteration 14, loss = 0.29266166\n",
      "Iteration 15, loss = 0.28097592\n",
      "Iteration 16, loss = 0.28441976\n",
      "Iteration 17, loss = 0.28720398\n",
      "Iteration 18, loss = 0.28669895\n",
      "Iteration 19, loss = 0.27988257\n",
      "Iteration 20, loss = 0.28899673\n",
      "Iteration 21, loss = 0.27408737\n",
      "Iteration 22, loss = 0.29983929\n",
      "Iteration 23, loss = 0.28418638\n",
      "Iteration 24, loss = 0.29932057\n",
      "Iteration 25, loss = 0.27737012\n",
      "Iteration 26, loss = 0.30741849\n",
      "Iteration 27, loss = 0.29578433\n",
      "Iteration 28, loss = 0.28405771\n",
      "Iteration 29, loss = 0.28218164\n",
      "Iteration 30, loss = 0.28136491\n",
      "Iteration 31, loss = 0.28889972\n",
      "Iteration 32, loss = 0.28979961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34249469\n",
      "Iteration 2, loss = 0.30939975\n",
      "Iteration 3, loss = 0.30104668\n",
      "Iteration 4, loss = 0.30917268\n",
      "Iteration 5, loss = 0.29554753\n",
      "Iteration 6, loss = 0.28846199\n",
      "Iteration 7, loss = 0.28578333\n",
      "Iteration 8, loss = 0.28190714\n",
      "Iteration 9, loss = 0.29203196\n",
      "Iteration 10, loss = 0.29031537\n",
      "Iteration 11, loss = 0.27794009\n",
      "Iteration 12, loss = 0.28747130\n",
      "Iteration 13, loss = 0.29081796\n",
      "Iteration 14, loss = 0.28625215\n",
      "Iteration 15, loss = 0.28232886\n",
      "Iteration 16, loss = 0.28493678\n",
      "Iteration 17, loss = 0.28377445\n",
      "Iteration 18, loss = 0.28456187\n",
      "Iteration 19, loss = 0.28147283\n",
      "Iteration 20, loss = 0.29084189\n",
      "Iteration 21, loss = 0.27825771\n",
      "Iteration 22, loss = 0.27814402\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33858635\n",
      "Iteration 2, loss = 0.30778193\n",
      "Iteration 3, loss = 0.30471595\n",
      "Iteration 4, loss = 0.29040910\n",
      "Iteration 5, loss = 0.30071873\n",
      "Iteration 6, loss = 0.28754639\n",
      "Iteration 7, loss = 0.29352935\n",
      "Iteration 8, loss = 0.28722879\n",
      "Iteration 9, loss = 0.29000302\n",
      "Iteration 10, loss = 0.28573283\n",
      "Iteration 11, loss = 0.29185717\n",
      "Iteration 12, loss = 0.29160123\n",
      "Iteration 13, loss = 0.28589554\n",
      "Iteration 14, loss = 0.28194657\n",
      "Iteration 15, loss = 0.28180095\n",
      "Iteration 16, loss = 0.27576904\n",
      "Iteration 17, loss = 0.28626491\n",
      "Iteration 18, loss = 0.28025919\n",
      "Iteration 19, loss = 0.27900945\n",
      "Iteration 20, loss = 0.27767558\n",
      "Iteration 21, loss = 0.28208661\n",
      "Iteration 22, loss = 0.28280583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39242055\n",
      "Iteration 2, loss = 0.31204405\n",
      "Iteration 3, loss = 0.28957022\n",
      "Iteration 4, loss = 0.29374337\n",
      "Iteration 5, loss = 0.29989757\n",
      "Iteration 6, loss = 0.29258826\n",
      "Iteration 7, loss = 0.28457235\n",
      "Iteration 8, loss = 0.29022549\n",
      "Iteration 9, loss = 0.28188179\n",
      "Iteration 10, loss = 0.26999265\n",
      "Iteration 11, loss = 0.27272760\n",
      "Iteration 12, loss = 0.27325180\n",
      "Iteration 13, loss = 0.27196838\n",
      "Iteration 14, loss = 0.28348588\n",
      "Iteration 15, loss = 0.27541411\n",
      "Iteration 16, loss = 0.27938415\n",
      "Iteration 17, loss = 0.26783011\n",
      "Iteration 18, loss = 0.26672062\n",
      "Iteration 19, loss = 0.27240025\n",
      "Iteration 20, loss = 0.27135273\n",
      "Iteration 21, loss = 0.27228484\n",
      "Iteration 22, loss = 0.27035860\n",
      "Iteration 23, loss = 0.27653132\n",
      "Iteration 24, loss = 0.26120957\n",
      "Iteration 25, loss = 0.27231464\n",
      "Iteration 26, loss = 0.26824774\n",
      "Iteration 27, loss = 0.26252530\n",
      "Iteration 28, loss = 0.27605703\n",
      "Iteration 29, loss = 0.27084002\n",
      "Iteration 30, loss = 0.26141037\n",
      "Iteration 31, loss = 0.27804010\n",
      "Iteration 32, loss = 0.26967168\n",
      "Iteration 33, loss = 0.26427760\n",
      "Iteration 34, loss = 0.26486691\n",
      "Iteration 35, loss = 0.25939272\n",
      "Iteration 36, loss = 0.26296934\n",
      "Iteration 37, loss = 0.26197350\n",
      "Iteration 38, loss = 0.26352083\n",
      "Iteration 39, loss = 0.26417965\n",
      "Iteration 40, loss = 0.25778622\n",
      "Iteration 41, loss = 0.27372531\n",
      "Iteration 42, loss = 0.27316681\n",
      "Iteration 43, loss = 0.26286818\n",
      "Iteration 44, loss = 0.26230359\n",
      "Iteration 45, loss = 0.26592481\n",
      "Iteration 46, loss = 0.26076650\n",
      "Iteration 47, loss = 0.27010382\n",
      "Iteration 48, loss = 0.26818141\n",
      "Iteration 49, loss = 0.26268712\n",
      "Iteration 50, loss = 0.26497388\n",
      "Iteration 51, loss = 0.25999259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32436504\n",
      "Iteration 2, loss = 0.29267955\n",
      "Iteration 3, loss = 0.27552604\n",
      "Iteration 4, loss = 0.28160806\n",
      "Iteration 5, loss = 0.28362334\n",
      "Iteration 6, loss = 0.26966019\n",
      "Iteration 7, loss = 0.27732815\n",
      "Iteration 8, loss = 0.27254651\n",
      "Iteration 9, loss = 0.27190064\n",
      "Iteration 10, loss = 0.26929618\n",
      "Iteration 11, loss = 0.27055575\n",
      "Iteration 12, loss = 0.26526046\n",
      "Iteration 13, loss = 0.27091367\n",
      "Iteration 14, loss = 0.27082958\n",
      "Iteration 15, loss = 0.26368384\n",
      "Iteration 16, loss = 0.26567026\n",
      "Iteration 17, loss = 0.26492746\n",
      "Iteration 18, loss = 0.25943577\n",
      "Iteration 19, loss = 0.26267235\n",
      "Iteration 20, loss = 0.26173274\n",
      "Iteration 21, loss = 0.25806392\n",
      "Iteration 22, loss = 0.25815437\n",
      "Iteration 23, loss = 0.25797938\n",
      "Iteration 24, loss = 0.25945586\n",
      "Iteration 25, loss = 0.26078264\n",
      "Iteration 26, loss = 0.25992750\n",
      "Iteration 27, loss = 0.25892138\n",
      "Iteration 28, loss = 0.26008763\n",
      "Iteration 29, loss = 0.25817315\n",
      "Iteration 30, loss = 0.25928301\n",
      "Iteration 31, loss = 0.25902017\n",
      "Iteration 32, loss = 0.25716729\n",
      "Iteration 33, loss = 0.26002691\n",
      "Iteration 34, loss = 0.25748724\n",
      "Iteration 35, loss = 0.25575005\n",
      "Iteration 36, loss = 0.26041141\n",
      "Iteration 37, loss = 0.25982077\n",
      "Iteration 38, loss = 0.25577404\n",
      "Iteration 39, loss = 0.26273307\n",
      "Iteration 40, loss = 0.25585238\n",
      "Iteration 41, loss = 0.25743137\n",
      "Iteration 42, loss = 0.25781635\n",
      "Iteration 43, loss = 0.25887336\n",
      "Iteration 44, loss = 0.25434334\n",
      "Iteration 45, loss = 0.25761018\n",
      "Iteration 46, loss = 0.25685808\n",
      "Iteration 47, loss = 0.25616351\n",
      "Iteration 48, loss = 0.25479267\n",
      "Iteration 49, loss = 0.25459775\n",
      "Iteration 50, loss = 0.25516402\n",
      "Iteration 51, loss = 0.25597100\n",
      "Iteration 52, loss = 0.25666565\n",
      "Iteration 53, loss = 0.25745901\n",
      "Iteration 54, loss = 0.25780891\n",
      "Iteration 55, loss = 0.25857528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45854977\n",
      "Iteration 2, loss = 0.32705334\n",
      "Iteration 3, loss = 0.30647440\n",
      "Iteration 4, loss = 0.31381893\n",
      "Iteration 5, loss = 0.32287646\n",
      "Iteration 6, loss = 0.30008669\n",
      "Iteration 7, loss = 0.30398073\n",
      "Iteration 8, loss = 0.30161651\n",
      "Iteration 9, loss = 0.29553628\n",
      "Iteration 10, loss = 0.29668539\n",
      "Iteration 11, loss = 0.30049975\n",
      "Iteration 12, loss = 0.29837256\n",
      "Iteration 13, loss = 0.29187693\n",
      "Iteration 14, loss = 0.29996969\n",
      "Iteration 15, loss = 0.29214676\n",
      "Iteration 16, loss = 0.28715203\n",
      "Iteration 17, loss = 0.28777686\n",
      "Iteration 18, loss = 0.29096674\n",
      "Iteration 19, loss = 0.28196483\n",
      "Iteration 20, loss = 0.29316554\n",
      "Iteration 21, loss = 0.29719384\n",
      "Iteration 22, loss = 0.28797157\n",
      "Iteration 23, loss = 0.28802062\n",
      "Iteration 24, loss = 0.30485876\n",
      "Iteration 25, loss = 0.29089130\n",
      "Iteration 26, loss = 0.28562553\n",
      "Iteration 27, loss = 0.27803426\n",
      "Iteration 28, loss = 0.28427327\n",
      "Iteration 29, loss = 0.27953030\n",
      "Iteration 30, loss = 0.28595144\n",
      "Iteration 31, loss = 0.28354627\n",
      "Iteration 32, loss = 0.28929023\n",
      "Iteration 33, loss = 0.28718427\n",
      "Iteration 34, loss = 0.28500774\n",
      "Iteration 35, loss = 0.27457554\n",
      "Iteration 36, loss = 0.28960681\n",
      "Iteration 37, loss = 0.28523049\n",
      "Iteration 38, loss = 0.27958499\n",
      "Iteration 39, loss = 0.27601912\n",
      "Iteration 40, loss = 0.28530363\n",
      "Iteration 41, loss = 0.27911645\n",
      "Iteration 42, loss = 0.27421995\n",
      "Iteration 43, loss = 0.27580120\n",
      "Iteration 44, loss = 0.28816286\n",
      "Iteration 45, loss = 0.27642962\n",
      "Iteration 46, loss = 0.27929834\n",
      "Iteration 47, loss = 0.27471052\n",
      "Iteration 48, loss = 0.28028755\n",
      "Iteration 49, loss = 0.28300192\n",
      "Iteration 50, loss = 0.28902122\n",
      "Iteration 51, loss = 0.27593822\n",
      "Iteration 52, loss = 0.28232583\n",
      "Iteration 53, loss = 0.28158612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34871220\n",
      "Iteration 2, loss = 0.29981910\n",
      "Iteration 3, loss = 0.29185756\n",
      "Iteration 4, loss = 0.30017587\n",
      "Iteration 5, loss = 0.29834249\n",
      "Iteration 6, loss = 0.29422057\n",
      "Iteration 7, loss = 0.30070252\n",
      "Iteration 8, loss = 0.28407037\n",
      "Iteration 9, loss = 0.27963147\n",
      "Iteration 10, loss = 0.28080668\n",
      "Iteration 11, loss = 0.28355050\n",
      "Iteration 12, loss = 0.28023564\n",
      "Iteration 13, loss = 0.28010103\n",
      "Iteration 14, loss = 0.27455026\n",
      "Iteration 15, loss = 0.28526906\n",
      "Iteration 16, loss = 0.27852308\n",
      "Iteration 17, loss = 0.27462149\n",
      "Iteration 18, loss = 0.27743606\n",
      "Iteration 19, loss = 0.28895725\n",
      "Iteration 20, loss = 0.28475098\n",
      "Iteration 21, loss = 0.28036930\n",
      "Iteration 22, loss = 0.27169199\n",
      "Iteration 23, loss = 0.28344635\n",
      "Iteration 24, loss = 0.28039466\n",
      "Iteration 25, loss = 0.28113810\n",
      "Iteration 26, loss = 0.27874194\n",
      "Iteration 27, loss = 0.28485145\n",
      "Iteration 28, loss = 0.27753205\n",
      "Iteration 29, loss = 0.28464029\n",
      "Iteration 30, loss = 0.27865688\n",
      "Iteration 31, loss = 0.27479750\n",
      "Iteration 32, loss = 0.28201473\n",
      "Iteration 33, loss = 0.27963003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33886880\n",
      "Iteration 2, loss = 0.30227519\n",
      "Iteration 3, loss = 0.30093644\n",
      "Iteration 4, loss = 0.29721878\n",
      "Iteration 5, loss = 0.29810112\n",
      "Iteration 6, loss = 0.28372478\n",
      "Iteration 7, loss = 0.29411973\n",
      "Iteration 8, loss = 0.28725877\n",
      "Iteration 9, loss = 0.29052292\n",
      "Iteration 10, loss = 0.28122062\n",
      "Iteration 11, loss = 0.28404076\n",
      "Iteration 12, loss = 0.28360170\n",
      "Iteration 13, loss = 0.28201041\n",
      "Iteration 14, loss = 0.28509947\n",
      "Iteration 15, loss = 0.28321803\n",
      "Iteration 16, loss = 0.28851742\n",
      "Iteration 17, loss = 0.28646807\n",
      "Iteration 18, loss = 0.29269542\n",
      "Iteration 19, loss = 0.29207284\n",
      "Iteration 20, loss = 0.27558027\n",
      "Iteration 21, loss = 0.27571184\n",
      "Iteration 22, loss = 0.28152000\n",
      "Iteration 23, loss = 0.27988077\n",
      "Iteration 24, loss = 0.28031619\n",
      "Iteration 25, loss = 0.29849990\n",
      "Iteration 26, loss = 0.28053530\n",
      "Iteration 27, loss = 0.28798878\n",
      "Iteration 28, loss = 0.28110858\n",
      "Iteration 29, loss = 0.27508688\n",
      "Iteration 30, loss = 0.28056612\n",
      "Iteration 31, loss = 0.28385828\n",
      "Iteration 32, loss = 0.28053998\n",
      "Iteration 33, loss = 0.28835195\n",
      "Iteration 34, loss = 0.27869765\n",
      "Iteration 35, loss = 0.28081795\n",
      "Iteration 36, loss = 0.28706433\n",
      "Iteration 37, loss = 0.28518791\n",
      "Iteration 38, loss = 0.28930397\n",
      "Iteration 39, loss = 0.28047430\n",
      "Iteration 40, loss = 0.27632530\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.01521708\n",
      "Iteration 2, loss = 1.64897489\n",
      "Iteration 3, loss = 1.81415397\n",
      "Iteration 4, loss = 1.47177078\n",
      "Iteration 5, loss = 0.54865839\n",
      "Iteration 6, loss = 0.43726069\n",
      "Iteration 7, loss = 0.38119775\n",
      "Iteration 8, loss = 0.34830063\n",
      "Iteration 9, loss = 0.36633994\n",
      "Iteration 10, loss = 0.34233760\n",
      "Iteration 11, loss = 0.35932068\n",
      "Iteration 1, loss = 0.43228643\n",
      "Iteration 2, loss = 0.32308270\n",
      "Iteration 3, loss = 0.31103518\n",
      "Iteration 4, loss = 0.31969680\n",
      "Iteration 5, loss = 0.31817753\n",
      "Iteration 6, loss = 0.29652956\n",
      "Iteration 7, loss = 0.28340677\n",
      "Iteration 8, loss = 0.28506687\n",
      "Iteration 9, loss = 0.29999942\n",
      "Iteration 10, loss = 0.29030464\n",
      "Iteration 11, loss = 0.28989360\n",
      "Iteration 12, loss = 0.28486162\n",
      "Iteration 13, loss = 0.28209297\n",
      "Iteration 14, loss = 0.28262956\n",
      "Iteration 15, loss = 0.27798160\n",
      "Iteration 16, loss = 0.27096726\n",
      "Iteration 17, loss = 0.26611379\n",
      "Iteration 18, loss = 0.28328057\n",
      "Iteration 19, loss = 0.26881195\n",
      "Iteration 20, loss = 0.27464903\n",
      "Iteration 21, loss = 0.27106633\n",
      "Iteration 22, loss = 0.26892023\n",
      "Iteration 23, loss = 0.27653037\n",
      "Iteration 24, loss = 0.27241722\n",
      "Iteration 25, loss = 0.26666767\n",
      "Iteration 26, loss = 0.26336568\n",
      "Iteration 27, loss = 0.27374789\n",
      "Iteration 28, loss = 0.27803383\n",
      "Iteration 29, loss = 0.27297842\n",
      "Iteration 30, loss = 0.27108207\n",
      "Iteration 31, loss = 0.26726562\n",
      "Iteration 32, loss = 0.26461053\n",
      "Iteration 33, loss = 0.28226236\n",
      "Iteration 34, loss = 0.27006554\n",
      "Iteration 35, loss = 0.26455900\n",
      "Iteration 36, loss = 0.26480816\n",
      "Iteration 37, loss = 0.26141438\n",
      "Iteration 38, loss = 0.26329052\n",
      "Iteration 39, loss = 0.26592469\n",
      "Iteration 40, loss = 0.26645773\n",
      "Iteration 41, loss = 0.28111673\n",
      "Iteration 42, loss = 0.26063417\n",
      "Iteration 43, loss = 0.26738898\n",
      "Iteration 44, loss = 0.25807088\n",
      "Iteration 45, loss = 0.26315012\n",
      "Iteration 46, loss = 0.26131449\n",
      "Iteration 47, loss = 0.26146796\n",
      "Iteration 48, loss = 0.26159466\n",
      "Iteration 49, loss = 0.26666708\n",
      "Iteration 50, loss = 0.26552470\n",
      "Iteration 51, loss = 0.26132345\n",
      "Iteration 52, loss = 0.26157995\n",
      "Iteration 53, loss = 0.25665452\n",
      "Iteration 54, loss = 0.25968546\n",
      "Iteration 55, loss = 0.26635515\n",
      "Iteration 56, loss = 0.26715518\n",
      "Iteration 57, loss = 0.26078040\n",
      "Iteration 58, loss = 0.26302141\n",
      "Iteration 59, loss = 0.25644241\n",
      "Iteration 60, loss = 0.25855108\n",
      "Iteration 61, loss = 0.26141055\n",
      "Iteration 62, loss = 0.26257111\n",
      "Iteration 63, loss = 0.26101854\n",
      "Iteration 64, loss = 0.25992800\n",
      "Iteration 65, loss = 0.26119649\n",
      "Iteration 66, loss = 0.25860744\n",
      "Iteration 67, loss = 0.25541273\n",
      "Iteration 68, loss = 0.25680503\n",
      "Iteration 69, loss = 0.25493068\n",
      "Iteration 70, loss = 0.25879034\n",
      "Iteration 71, loss = 0.25776234\n",
      "Iteration 72, loss = 0.25824165\n",
      "Iteration 73, loss = 0.25984357\n",
      "Iteration 74, loss = 0.26115468\n",
      "Iteration 75, loss = 0.26865868\n",
      "Iteration 76, loss = 0.26604475\n",
      "Iteration 77, loss = 0.26567838\n",
      "Iteration 78, loss = 0.26436652\n",
      "Iteration 79, loss = 0.26626573\n",
      "Iteration 80, loss = 0.26363499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32436504\n",
      "Iteration 2, loss = 0.29267955\n",
      "Iteration 3, loss = 0.27552604\n",
      "Iteration 4, loss = 0.28160806\n",
      "Iteration 5, loss = 0.28362334\n",
      "Iteration 6, loss = 0.26966019\n",
      "Iteration 7, loss = 0.27732815\n",
      "Iteration 8, loss = 0.27254651\n",
      "Iteration 9, loss = 0.27190064\n",
      "Iteration 10, loss = 0.26929618\n",
      "Iteration 11, loss = 0.27055575\n",
      "Iteration 12, loss = 0.26526046\n",
      "Iteration 13, loss = 0.27091367\n",
      "Iteration 14, loss = 0.27082958\n",
      "Iteration 15, loss = 0.26368384\n",
      "Iteration 16, loss = 0.26567026\n",
      "Iteration 17, loss = 0.26492746\n",
      "Iteration 18, loss = 0.25943577\n",
      "Iteration 19, loss = 0.26267235\n",
      "Iteration 20, loss = 0.26173274\n",
      "Iteration 21, loss = 0.25806392\n",
      "Iteration 22, loss = 0.25815437\n",
      "Iteration 23, loss = 0.25797938\n",
      "Iteration 24, loss = 0.25945586\n",
      "Iteration 25, loss = 0.26078264\n",
      "Iteration 26, loss = 0.25992750\n",
      "Iteration 27, loss = 0.25892138\n",
      "Iteration 28, loss = 0.26008763\n",
      "Iteration 29, loss = 0.25817315\n",
      "Iteration 30, loss = 0.25928301\n",
      "Iteration 31, loss = 0.25902017\n",
      "Iteration 32, loss = 0.25716729\n",
      "Iteration 33, loss = 0.26002691\n",
      "Iteration 34, loss = 0.25748724\n",
      "Iteration 35, loss = 0.25575005\n",
      "Iteration 36, loss = 0.26041141\n",
      "Iteration 37, loss = 0.25982077\n",
      "Iteration 38, loss = 0.25577404\n",
      "Iteration 39, loss = 0.26273307\n",
      "Iteration 40, loss = 0.25585238\n",
      "Iteration 41, loss = 0.25743137\n",
      "Iteration 42, loss = 0.25781635\n",
      "Iteration 43, loss = 0.25887336\n",
      "Iteration 44, loss = 0.25434334\n",
      "Iteration 45, loss = 0.25761018\n",
      "Iteration 46, loss = 0.25685808\n",
      "Iteration 47, loss = 0.25616351\n",
      "Iteration 48, loss = 0.25479267\n",
      "Iteration 49, loss = 0.25459775\n",
      "Iteration 50, loss = 0.25516402\n",
      "Iteration 51, loss = 0.25597100\n",
      "Iteration 52, loss = 0.25666565\n",
      "Iteration 53, loss = 0.25745901\n",
      "Iteration 54, loss = 0.25780891\n",
      "Iteration 55, loss = 0.25857528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45854977\n",
      "Iteration 2, loss = 0.32705334\n",
      "Iteration 3, loss = 0.30647440\n",
      "Iteration 4, loss = 0.31381893\n",
      "Iteration 5, loss = 0.32287646\n",
      "Iteration 6, loss = 0.30008669\n",
      "Iteration 7, loss = 0.30398073\n",
      "Iteration 8, loss = 0.30161651\n",
      "Iteration 9, loss = 0.29553628\n",
      "Iteration 10, loss = 0.29668539\n",
      "Iteration 11, loss = 0.30049975\n",
      "Iteration 12, loss = 0.29837256\n",
      "Iteration 13, loss = 0.29187693\n",
      "Iteration 14, loss = 0.29996969\n",
      "Iteration 15, loss = 0.29214676\n",
      "Iteration 16, loss = 0.28715203\n",
      "Iteration 17, loss = 0.28777686\n",
      "Iteration 18, loss = 0.29096674\n",
      "Iteration 19, loss = 0.28196483\n",
      "Iteration 20, loss = 0.29316554\n",
      "Iteration 21, loss = 0.29719384\n",
      "Iteration 22, loss = 0.28797157\n",
      "Iteration 23, loss = 0.28802062\n",
      "Iteration 24, loss = 0.30485876\n",
      "Iteration 25, loss = 0.29089130\n",
      "Iteration 26, loss = 0.28562553\n",
      "Iteration 27, loss = 0.27803426\n",
      "Iteration 28, loss = 0.28427327\n",
      "Iteration 29, loss = 0.27953030\n",
      "Iteration 30, loss = 0.28595144\n",
      "Iteration 31, loss = 0.28354627\n",
      "Iteration 32, loss = 0.28929023\n",
      "Iteration 33, loss = 0.28718427\n",
      "Iteration 34, loss = 0.28500774\n",
      "Iteration 35, loss = 0.27457554\n",
      "Iteration 36, loss = 0.28960681\n",
      "Iteration 37, loss = 0.28523049\n",
      "Iteration 38, loss = 0.27958499\n",
      "Iteration 39, loss = 0.27601912\n",
      "Iteration 40, loss = 0.28530363\n",
      "Iteration 41, loss = 0.27911645\n",
      "Iteration 42, loss = 0.27421995\n",
      "Iteration 43, loss = 0.27580120\n",
      "Iteration 44, loss = 0.28816286\n",
      "Iteration 45, loss = 0.27642962\n",
      "Iteration 46, loss = 0.27929834\n",
      "Iteration 47, loss = 0.27471052\n",
      "Iteration 48, loss = 0.28028755\n",
      "Iteration 49, loss = 0.28300192\n",
      "Iteration 50, loss = 0.28902122\n",
      "Iteration 51, loss = 0.27593822\n",
      "Iteration 52, loss = 0.28232583\n",
      "Iteration 53, loss = 0.28158612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33858635\n",
      "Iteration 2, loss = 0.30778193\n",
      "Iteration 3, loss = 0.30471595\n",
      "Iteration 4, loss = 0.29040910\n",
      "Iteration 5, loss = 0.30071873\n",
      "Iteration 6, loss = 0.28754639\n",
      "Iteration 7, loss = 0.29352935\n",
      "Iteration 8, loss = 0.28722879\n",
      "Iteration 9, loss = 0.29000302\n",
      "Iteration 10, loss = 0.28573283\n",
      "Iteration 11, loss = 0.29185717\n",
      "Iteration 12, loss = 0.29160123\n",
      "Iteration 13, loss = 0.28589554\n",
      "Iteration 14, loss = 0.28194657\n",
      "Iteration 15, loss = 0.28180095\n",
      "Iteration 16, loss = 0.27576904\n",
      "Iteration 17, loss = 0.28626491\n",
      "Iteration 18, loss = 0.28025919\n",
      "Iteration 19, loss = 0.27900945\n",
      "Iteration 20, loss = 0.27767558\n",
      "Iteration 21, loss = 0.28208661\n",
      "Iteration 22, loss = 0.28280583\n",
      "Iteration 23, loss = 0.28182723\n",
      "Iteration 24, loss = 0.27821578\n",
      "Iteration 25, loss = 0.28814125\n",
      "Iteration 26, loss = 0.28696574\n",
      "Iteration 27, loss = 0.27912478\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.62706357\n",
      "Iteration 2, loss = 0.55629896\n",
      "Iteration 3, loss = 0.41722030\n",
      "Iteration 4, loss = 0.40907008\n",
      "Iteration 5, loss = 0.41138787\n",
      "Iteration 6, loss = 0.39820108\n",
      "Iteration 7, loss = 0.36213394\n",
      "Iteration 8, loss = 0.36245695\n",
      "Iteration 9, loss = 0.37257134\n",
      "Iteration 10, loss = 0.38364261\n",
      "Iteration 11, loss = 0.33257169\n",
      "Iteration 12, loss = 0.34460911\n",
      "Iteration 13, loss = 0.34227834\n",
      "Iteration 14, loss = 0.33524577\n",
      "Iteration 15, loss = 0.33397612\n",
      "Iteration 16, loss = 0.33797543\n",
      "Iteration 17, loss = 0.32768054\n",
      "Iteration 18, loss = 0.31329356\n",
      "Iteration 19, loss = 0.30191182\n",
      "Iteration 20, loss = 0.30947770\n",
      "Iteration 21, loss = 0.30191768\n",
      "Iteration 22, loss = 0.29710415\n",
      "Iteration 23, loss = 0.28373661\n",
      "Iteration 24, loss = 0.28913701\n",
      "Iteration 25, loss = 0.27867094\n",
      "Iteration 26, loss = 0.28857820\n",
      "Iteration 27, loss = 0.26955110\n",
      "Iteration 28, loss = 0.27159739\n",
      "Iteration 29, loss = 0.26655253\n",
      "Iteration 30, loss = 0.29026483\n",
      "Iteration 31, loss = 0.28287326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.43228643\n",
      "Iteration 2, loss = 0.32308270\n",
      "Iteration 3, loss = 0.31103518\n",
      "Iteration 4, loss = 0.31969680\n",
      "Iteration 5, loss = 0.31817753\n",
      "Iteration 6, loss = 0.29652956\n",
      "Iteration 7, loss = 0.28340677\n",
      "Iteration 8, loss = 0.28506687\n",
      "Iteration 9, loss = 0.29999942\n",
      "Iteration 10, loss = 0.29030464\n",
      "Iteration 11, loss = 0.28989360\n",
      "Iteration 12, loss = 0.28486162\n",
      "Iteration 13, loss = 0.28209297\n",
      "Iteration 14, loss = 0.28262956\n",
      "Iteration 15, loss = 0.27798160\n",
      "Iteration 16, loss = 0.27096726\n",
      "Iteration 17, loss = 0.26611379\n",
      "Iteration 18, loss = 0.28328057\n",
      "Iteration 19, loss = 0.26881195\n",
      "Iteration 20, loss = 0.27464903\n",
      "Iteration 21, loss = 0.27106633\n",
      "Iteration 22, loss = 0.26892023\n",
      "Iteration 23, loss = 0.27653037\n",
      "Iteration 24, loss = 0.27241722\n",
      "Iteration 25, loss = 0.26666767\n",
      "Iteration 26, loss = 0.26336568\n",
      "Iteration 27, loss = 0.27374789\n",
      "Iteration 28, loss = 0.27803383\n",
      "Iteration 29, loss = 0.27297842\n",
      "Iteration 30, loss = 0.27108207\n",
      "Iteration 31, loss = 0.26726562\n",
      "Iteration 32, loss = 0.26461053\n",
      "Iteration 33, loss = 0.28226236\n",
      "Iteration 34, loss = 0.27006554\n",
      "Iteration 35, loss = 0.26455900\n",
      "Iteration 36, loss = 0.26480816\n",
      "Iteration 37, loss = 0.26141438\n",
      "Iteration 38, loss = 0.26329052\n",
      "Iteration 39, loss = 0.26592469\n",
      "Iteration 40, loss = 0.26645773\n",
      "Iteration 41, loss = 0.28111673\n",
      "Iteration 42, loss = 0.26063417\n",
      "Iteration 43, loss = 0.26738898\n",
      "Iteration 44, loss = 0.25807088\n",
      "Iteration 45, loss = 0.26315012\n",
      "Iteration 46, loss = 0.26131449\n",
      "Iteration 47, loss = 0.26146796\n",
      "Iteration 48, loss = 0.26159466\n",
      "Iteration 49, loss = 0.26666708\n",
      "Iteration 50, loss = 0.26552470\n",
      "Iteration 51, loss = 0.26132345\n",
      "Iteration 52, loss = 0.26157995\n",
      "Iteration 53, loss = 0.25665452\n",
      "Iteration 54, loss = 0.25968546\n",
      "Iteration 55, loss = 0.26635515\n",
      "Iteration 56, loss = 0.26715518\n",
      "Iteration 57, loss = 0.26078040\n",
      "Iteration 58, loss = 0.26302141\n",
      "Iteration 59, loss = 0.25644241\n",
      "Iteration 60, loss = 0.25855108\n",
      "Iteration 61, loss = 0.26141055\n",
      "Iteration 62, loss = 0.26257111\n",
      "Iteration 63, loss = 0.26101854\n",
      "Iteration 64, loss = 0.25992800\n",
      "Iteration 65, loss = 0.26119649\n",
      "Iteration 66, loss = 0.25860744\n",
      "Iteration 67, loss = 0.25541273\n",
      "Iteration 68, loss = 0.25680503\n",
      "Iteration 69, loss = 0.25493068\n",
      "Iteration 70, loss = 0.25879034\n",
      "Iteration 71, loss = 0.25776234\n",
      "Iteration 72, loss = 0.25824165\n",
      "Iteration 73, loss = 0.25984357\n",
      "Iteration 74, loss = 0.26115468\n",
      "Iteration 75, loss = 0.26865868\n",
      "Iteration 76, loss = 0.26604475\n",
      "Iteration 77, loss = 0.26567838\n",
      "Iteration 78, loss = 0.26436652\n",
      "Iteration 79, loss = 0.26626573\n",
      "Iteration 80, loss = 0.26363499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32434942\n",
      "Iteration 2, loss = 0.29985038\n",
      "Iteration 3, loss = 0.29090230\n",
      "Iteration 4, loss = 0.28850457\n",
      "Iteration 5, loss = 0.27272719\n",
      "Iteration 6, loss = 0.27055070\n",
      "Iteration 7, loss = 0.27346628\n",
      "Iteration 8, loss = 0.27117008\n",
      "Iteration 9, loss = 0.26830509\n",
      "Iteration 10, loss = 0.26956159\n",
      "Iteration 11, loss = 0.26846116\n",
      "Iteration 12, loss = 0.27014146\n",
      "Iteration 13, loss = 0.27058057\n",
      "Iteration 14, loss = 0.27079866\n",
      "Iteration 15, loss = 0.26668562\n",
      "Iteration 16, loss = 0.26012376\n",
      "Iteration 17, loss = 0.25996659\n",
      "Iteration 18, loss = 0.26242659\n",
      "Iteration 19, loss = 0.26204022\n",
      "Iteration 20, loss = 0.26099314\n",
      "Iteration 21, loss = 0.26173423\n",
      "Iteration 22, loss = 0.25898780\n",
      "Iteration 23, loss = 0.25884459\n",
      "Iteration 24, loss = 0.25740253\n",
      "Iteration 25, loss = 0.25961267\n",
      "Iteration 26, loss = 0.25906162\n",
      "Iteration 27, loss = 0.25823804\n",
      "Iteration 28, loss = 0.25670945\n",
      "Iteration 29, loss = 0.25745858\n",
      "Iteration 30, loss = 0.25844053\n",
      "Iteration 31, loss = 0.25661233\n",
      "Iteration 32, loss = 0.25714210\n",
      "Iteration 33, loss = 0.25988589\n",
      "Iteration 34, loss = 0.26075137\n",
      "Iteration 35, loss = 0.26219823\n",
      "Iteration 36, loss = 0.25861344\n",
      "Iteration 37, loss = 0.25743115\n",
      "Iteration 38, loss = 0.26019598\n",
      "Iteration 39, loss = 0.25649596\n",
      "Iteration 40, loss = 0.25749612\n",
      "Iteration 41, loss = 0.26454367\n",
      "Iteration 42, loss = 0.26070327\n",
      "Iteration 43, loss = 0.25566664\n",
      "Iteration 44, loss = 0.25738582\n",
      "Iteration 45, loss = 0.25571767\n",
      "Iteration 46, loss = 0.25669669\n",
      "Iteration 47, loss = 0.25949312\n",
      "Iteration 48, loss = 0.25554268\n",
      "Iteration 49, loss = 0.25494504\n",
      "Iteration 50, loss = 0.25725210\n",
      "Iteration 51, loss = 0.25634936\n",
      "Iteration 52, loss = 0.25539986\n",
      "Iteration 53, loss = 0.25883489\n",
      "Iteration 54, loss = 0.25959323\n",
      "Iteration 55, loss = 0.26029152\n",
      "Iteration 56, loss = 0.25601634\n",
      "Iteration 57, loss = 0.25934361\n",
      "Iteration 58, loss = 0.25375830\n",
      "Iteration 59, loss = 0.25648503\n",
      "Iteration 60, loss = 0.25246916\n",
      "Iteration 61, loss = 0.25528138\n",
      "Iteration 62, loss = 0.26105891\n",
      "Iteration 63, loss = 0.25807591\n",
      "Iteration 64, loss = 0.25602675\n",
      "Iteration 65, loss = 0.25378010\n",
      "Iteration 66, loss = 0.25463779\n",
      "Iteration 67, loss = 0.25567661\n",
      "Iteration 68, loss = 0.25425159\n",
      "Iteration 69, loss = 0.25357130\n",
      "Iteration 70, loss = 0.25369497\n",
      "Iteration 71, loss = 0.25404609\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34249469\n",
      "Iteration 2, loss = 0.30939975\n",
      "Iteration 3, loss = 0.30104668\n",
      "Iteration 4, loss = 0.30917268\n",
      "Iteration 5, loss = 0.29554753\n",
      "Iteration 6, loss = 0.28846199\n",
      "Iteration 7, loss = 0.28578333\n",
      "Iteration 8, loss = 0.28190714\n",
      "Iteration 9, loss = 0.29203196\n",
      "Iteration 10, loss = 0.29031537\n",
      "Iteration 11, loss = 0.27794009\n",
      "Iteration 12, loss = 0.28747130\n",
      "Iteration 13, loss = 0.29081796\n",
      "Iteration 14, loss = 0.28625215\n",
      "Iteration 15, loss = 0.28232886\n",
      "Iteration 16, loss = 0.28493678\n",
      "Iteration 17, loss = 0.28377445\n",
      "Iteration 18, loss = 0.28456187\n",
      "Iteration 19, loss = 0.28147283\n",
      "Iteration 20, loss = 0.29084189\n",
      "Iteration 21, loss = 0.27825771\n",
      "Iteration 22, loss = 0.27814402\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34772470\n",
      "Iteration 2, loss = 0.30868981\n",
      "Iteration 3, loss = 0.30100217\n",
      "Iteration 4, loss = 0.29633502\n",
      "Iteration 5, loss = 0.28880930\n",
      "Iteration 6, loss = 0.29520778\n",
      "Iteration 7, loss = 0.29105342\n",
      "Iteration 8, loss = 0.28922895\n",
      "Iteration 9, loss = 0.28774358\n",
      "Iteration 10, loss = 0.29083045\n",
      "Iteration 11, loss = 0.29504800\n",
      "Iteration 12, loss = 0.28148057\n",
      "Iteration 13, loss = 0.28895930\n",
      "Iteration 14, loss = 0.28373150\n",
      "Iteration 15, loss = 0.28125956\n",
      "Iteration 16, loss = 0.28586745\n",
      "Iteration 17, loss = 0.28441730\n",
      "Iteration 18, loss = 0.28368598\n",
      "Iteration 19, loss = 0.29072603\n",
      "Iteration 20, loss = 0.28072587\n",
      "Iteration 21, loss = 0.28169707\n",
      "Iteration 22, loss = 0.28171458\n",
      "Iteration 23, loss = 0.27588566\n",
      "Iteration 24, loss = 0.28317490\n",
      "Iteration 25, loss = 0.28260859\n",
      "Iteration 26, loss = 0.27978162\n",
      "Iteration 27, loss = 0.28348853\n",
      "Iteration 28, loss = 0.27940686\n",
      "Iteration 29, loss = 0.30550173\n",
      "Iteration 30, loss = 0.28348657\n",
      "Iteration 31, loss = 0.28131422\n",
      "Iteration 32, loss = 0.29738083\n",
      "Iteration 33, loss = 0.28740456\n",
      "Iteration 34, loss = 0.28238663\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.01521708\n",
      "Iteration 2, loss = 1.64897489\n",
      "Iteration 3, loss = 1.81415397\n",
      "Iteration 4, loss = 1.47177078\n",
      "Iteration 5, loss = 0.54865839\n",
      "Iteration 6, loss = 0.43726069\n",
      "Iteration 7, loss = 0.38119775\n",
      "Iteration 8, loss = 0.34830063\n",
      "Iteration 9, loss = 0.36633994\n",
      "Iteration 10, loss = 0.34233760\n",
      "Iteration 11, loss = 0.35932068\n",
      "Iteration 12, loss = 0.33843686\n",
      "Iteration 13, loss = 0.33073238\n",
      "Iteration 14, loss = 0.34857813\n",
      "Iteration 15, loss = 0.32466532\n",
      "Iteration 16, loss = 0.31929802\n",
      "Iteration 17, loss = 0.30489573\n",
      "Iteration 18, loss = 0.30278815\n",
      "Iteration 19, loss = 0.30386948\n",
      "Iteration 20, loss = 0.31009412\n",
      "Iteration 21, loss = 0.29126980\n",
      "Iteration 22, loss = 0.29962275\n",
      "Iteration 23, loss = 0.29676968\n",
      "Iteration 24, loss = 0.28835326\n",
      "Iteration 25, loss = 0.29539973\n",
      "Iteration 26, loss = 0.30785738\n",
      "Iteration 27, loss = 0.29848506\n",
      "Iteration 28, loss = 0.28663958\n",
      "Iteration 29, loss = 0.29018834\n",
      "Iteration 30, loss = 0.28354246\n",
      "Iteration 31, loss = 0.28385747\n",
      "Iteration 32, loss = 0.28169338\n",
      "Iteration 33, loss = 0.27131886\n",
      "Iteration 34, loss = 0.27636691\n",
      "Iteration 35, loss = 0.29458031\n",
      "Iteration 36, loss = 0.28171488\n",
      "Iteration 37, loss = 0.27392482\n",
      "Iteration 38, loss = 0.27618959\n",
      "Iteration 39, loss = 0.26012561\n",
      "Iteration 1, loss = 0.44875388\n",
      "Iteration 2, loss = 0.32803265\n",
      "Iteration 3, loss = 0.30295411\n",
      "Iteration 4, loss = 0.30525981\n",
      "Iteration 5, loss = 0.28385153\n",
      "Iteration 6, loss = 0.28093854\n",
      "Iteration 7, loss = 0.28211341\n",
      "Iteration 8, loss = 0.27435152\n",
      "Iteration 9, loss = 0.28466128\n",
      "Iteration 10, loss = 0.27983947\n",
      "Iteration 11, loss = 0.27222857\n",
      "Iteration 12, loss = 0.27685946\n",
      "Iteration 13, loss = 0.27565362\n",
      "Iteration 14, loss = 0.27264528\n",
      "Iteration 15, loss = 0.27710601\n",
      "Iteration 16, loss = 0.27124760\n",
      "Iteration 17, loss = 0.26814518\n",
      "Iteration 18, loss = 0.27494164\n",
      "Iteration 19, loss = 0.27865161\n",
      "Iteration 20, loss = 0.27068859\n",
      "Iteration 21, loss = 0.26518025\n",
      "Iteration 22, loss = 0.26931815\n",
      "Iteration 23, loss = 0.26274321\n",
      "Iteration 24, loss = 0.26806973\n",
      "Iteration 25, loss = 0.26914704\n",
      "Iteration 26, loss = 0.26481809\n",
      "Iteration 27, loss = 0.26363173\n",
      "Iteration 28, loss = 0.25824601\n",
      "Iteration 29, loss = 0.26876245\n",
      "Iteration 30, loss = 0.26608196\n",
      "Iteration 31, loss = 0.26632558\n",
      "Iteration 32, loss = 0.26291151\n",
      "Iteration 33, loss = 0.26366030\n",
      "Iteration 34, loss = 0.26617174\n",
      "Iteration 35, loss = 0.26174809\n",
      "Iteration 36, loss = 0.26166652\n",
      "Iteration 37, loss = 0.25816570\n",
      "Iteration 38, loss = 0.25820141\n",
      "Iteration 39, loss = 0.26088375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33151311\n",
      "Iteration 2, loss = 0.28947453\n",
      "Iteration 3, loss = 0.27905612\n",
      "Iteration 4, loss = 0.27487679\n",
      "Iteration 5, loss = 0.27349553\n",
      "Iteration 6, loss = 0.27349734\n",
      "Iteration 7, loss = 0.26545096\n",
      "Iteration 8, loss = 0.26780056\n",
      "Iteration 9, loss = 0.27019535\n",
      "Iteration 10, loss = 0.26352828\n",
      "Iteration 11, loss = 0.26251522\n",
      "Iteration 12, loss = 0.26131800\n",
      "Iteration 13, loss = 0.26319210\n",
      "Iteration 14, loss = 0.26076037\n",
      "Iteration 15, loss = 0.26463225\n",
      "Iteration 16, loss = 0.26420030\n",
      "Iteration 17, loss = 0.26245610\n",
      "Iteration 18, loss = 0.25809431\n",
      "Iteration 19, loss = 0.25912869\n",
      "Iteration 20, loss = 0.25764032\n",
      "Iteration 21, loss = 0.25622206\n",
      "Iteration 22, loss = 0.26324904\n",
      "Iteration 23, loss = 0.25670276\n",
      "Iteration 24, loss = 0.25505707\n",
      "Iteration 25, loss = 0.25654553\n",
      "Iteration 26, loss = 0.25630235\n",
      "Iteration 27, loss = 0.25776975\n",
      "Iteration 28, loss = 0.25645046\n",
      "Iteration 29, loss = 0.25730915\n",
      "Iteration 30, loss = 0.25621657\n",
      "Iteration 31, loss = 0.25539488\n",
      "Iteration 32, loss = 0.25624228\n",
      "Iteration 33, loss = 0.25360930\n",
      "Iteration 34, loss = 0.25557961\n",
      "Iteration 35, loss = 0.25553889\n",
      "Iteration 36, loss = 0.25778810\n",
      "Iteration 37, loss = 0.25508887\n",
      "Iteration 38, loss = 0.25414387\n",
      "Iteration 39, loss = 0.25342958\n",
      "Iteration 40, loss = 0.26147934\n",
      "Iteration 41, loss = 0.25905275\n",
      "Iteration 42, loss = 0.25461603\n",
      "Iteration 43, loss = 0.25411535\n",
      "Iteration 44, loss = 0.25944967\n",
      "Iteration 45, loss = 0.25393417\n",
      "Iteration 46, loss = 0.25428147\n",
      "Iteration 47, loss = 0.25308785\n",
      "Iteration 48, loss = 0.25401726\n",
      "Iteration 49, loss = 0.25564002\n",
      "Iteration 50, loss = 0.25591395\n",
      "Iteration 51, loss = 0.25577974\n",
      "Iteration 52, loss = 0.25217546\n",
      "Iteration 53, loss = 0.25247073\n",
      "Iteration 54, loss = 0.25170026\n",
      "Iteration 55, loss = 0.25297521\n",
      "Iteration 56, loss = 0.25436681\n",
      "Iteration 57, loss = 0.25361548\n",
      "Iteration 58, loss = 0.25426128\n",
      "Iteration 59, loss = 0.25122683\n",
      "Iteration 60, loss = 0.25344580\n",
      "Iteration 61, loss = 0.25222897\n",
      "Iteration 62, loss = 0.25551096\n",
      "Iteration 63, loss = 0.25327813\n",
      "Iteration 64, loss = 0.25419450\n",
      "Iteration 65, loss = 0.25240072\n",
      "Iteration 66, loss = 0.25018247\n",
      "Iteration 67, loss = 0.25251785\n",
      "Iteration 68, loss = 0.25390283\n",
      "Iteration 69, loss = 0.25459611\n",
      "Iteration 70, loss = 0.25112403\n",
      "Iteration 71, loss = 0.25181009\n",
      "Iteration 72, loss = 0.25375163\n",
      "Iteration 73, loss = 0.25159532\n",
      "Iteration 74, loss = 0.25060620\n",
      "Iteration 75, loss = 0.25239002\n",
      "Iteration 76, loss = 0.25120615\n",
      "Iteration 77, loss = 0.25238745\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34046450\n",
      "Iteration 2, loss = 0.29284170\n",
      "Iteration 3, loss = 0.29636928\n",
      "Iteration 4, loss = 0.29038097\n",
      "Iteration 5, loss = 0.28046988\n",
      "Iteration 6, loss = 0.28027077\n",
      "Iteration 7, loss = 0.27241974\n",
      "Iteration 8, loss = 0.26979722\n",
      "Iteration 9, loss = 0.27498592\n",
      "Iteration 10, loss = 0.27103554\n",
      "Iteration 11, loss = 0.27342860\n",
      "Iteration 12, loss = 0.26846742\n",
      "Iteration 13, loss = 0.27085779\n",
      "Iteration 14, loss = 0.27121645\n",
      "Iteration 15, loss = 0.26490563\n",
      "Iteration 16, loss = 0.26872473\n",
      "Iteration 17, loss = 0.26643400\n",
      "Iteration 18, loss = 0.26899433\n",
      "Iteration 19, loss = 0.26454798\n",
      "Iteration 20, loss = 0.27697933\n",
      "Iteration 21, loss = 0.26856130\n",
      "Iteration 22, loss = 0.26171339\n",
      "Iteration 23, loss = 0.25994784\n",
      "Iteration 24, loss = 0.25915033\n",
      "Iteration 25, loss = 0.26295082\n",
      "Iteration 26, loss = 0.26389108\n",
      "Iteration 27, loss = 0.26106324\n",
      "Iteration 28, loss = 0.26101430\n",
      "Iteration 29, loss = 0.26235098\n",
      "Iteration 30, loss = 0.26197620\n",
      "Iteration 31, loss = 0.26112588\n",
      "Iteration 32, loss = 0.26748913\n",
      "Iteration 33, loss = 0.26332203\n",
      "Iteration 34, loss = 0.25861028\n",
      "Iteration 35, loss = 0.25973752\n",
      "Iteration 36, loss = 0.25778477\n",
      "Iteration 37, loss = 0.25819609\n",
      "Iteration 38, loss = 0.25727670\n",
      "Iteration 39, loss = 0.25645869\n",
      "Iteration 40, loss = 0.25629221\n",
      "Iteration 41, loss = 0.26045788\n",
      "Iteration 42, loss = 0.25970154\n",
      "Iteration 43, loss = 0.25997620\n",
      "Iteration 44, loss = 0.25779954\n",
      "Iteration 45, loss = 0.25869989\n",
      "Iteration 46, loss = 0.25951138\n",
      "Iteration 47, loss = 0.25679167\n",
      "Iteration 48, loss = 0.25538106\n",
      "Iteration 49, loss = 0.25529408\n",
      "Iteration 50, loss = 0.25724414\n",
      "Iteration 51, loss = 0.25846311\n",
      "Iteration 52, loss = 0.25817688\n",
      "Iteration 53, loss = 0.25264486\n",
      "Iteration 54, loss = 0.25580812\n",
      "Iteration 55, loss = 0.26435318\n",
      "Iteration 56, loss = 0.25749681\n",
      "Iteration 57, loss = 0.25702260\n",
      "Iteration 58, loss = 0.25832108\n",
      "Iteration 59, loss = 0.25497284\n",
      "Iteration 60, loss = 0.25501213\n",
      "Iteration 61, loss = 0.25485345\n",
      "Iteration 62, loss = 0.25547245\n",
      "Iteration 63, loss = 0.25398175\n",
      "Iteration 64, loss = 0.25532931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40000571\n",
      "Iteration 2, loss = 0.37220302\n",
      "Iteration 3, loss = 0.32016539\n",
      "Iteration 4, loss = 0.30737767\n",
      "Iteration 5, loss = 0.30925138\n",
      "Iteration 6, loss = 0.29328554\n",
      "Iteration 7, loss = 0.30448788\n",
      "Iteration 8, loss = 0.29331085\n",
      "Iteration 9, loss = 0.29151593\n",
      "Iteration 10, loss = 0.29175163\n",
      "Iteration 11, loss = 0.29018645\n",
      "Iteration 12, loss = 0.27903685\n",
      "Iteration 13, loss = 0.28337694\n",
      "Iteration 14, loss = 0.28741029\n",
      "Iteration 15, loss = 0.28446943\n",
      "Iteration 16, loss = 0.28255836\n",
      "Iteration 17, loss = 0.28286942\n",
      "Iteration 18, loss = 0.28012104\n",
      "Iteration 19, loss = 0.28975265\n",
      "Iteration 20, loss = 0.28423648\n",
      "Iteration 21, loss = 0.28825179\n",
      "Iteration 22, loss = 0.28541930\n",
      "Iteration 23, loss = 0.28465797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33242854\n",
      "Iteration 2, loss = 0.31074654\n",
      "Iteration 3, loss = 0.29896558\n",
      "Iteration 4, loss = 0.28379778\n",
      "Iteration 5, loss = 0.28541444\n",
      "Iteration 6, loss = 0.28944530\n",
      "Iteration 7, loss = 0.29107797\n",
      "Iteration 8, loss = 0.28482574\n",
      "Iteration 9, loss = 0.27932808\n",
      "Iteration 10, loss = 0.28020947\n",
      "Iteration 11, loss = 0.27995085\n",
      "Iteration 12, loss = 0.28132452\n",
      "Iteration 13, loss = 0.28209399\n",
      "Iteration 14, loss = 0.29129478\n",
      "Iteration 15, loss = 0.28026812\n",
      "Iteration 16, loss = 0.27967520\n",
      "Iteration 17, loss = 0.27887279\n",
      "Iteration 18, loss = 0.27645326\n",
      "Iteration 19, loss = 0.27746107\n",
      "Iteration 20, loss = 0.28138703\n",
      "Iteration 21, loss = 0.27854840\n",
      "Iteration 22, loss = 0.28120016\n",
      "Iteration 23, loss = 0.28509941\n",
      "Iteration 24, loss = 0.28299080\n",
      "Iteration 25, loss = 0.27729659\n",
      "Iteration 26, loss = 0.27421597\n",
      "Iteration 27, loss = 0.27801730\n",
      "Iteration 28, loss = 0.28217224\n",
      "Iteration 29, loss = 0.27550744\n",
      "Iteration 30, loss = 0.27389538\n",
      "Iteration 31, loss = 0.27250447\n",
      "Iteration 32, loss = 0.27927032\n",
      "Iteration 33, loss = 0.28050985\n",
      "Iteration 34, loss = 0.27376671\n",
      "Iteration 35, loss = 0.27917285\n",
      "Iteration 36, loss = 0.28395756\n",
      "Iteration 37, loss = 0.27420055\n",
      "Iteration 38, loss = 0.27695567\n",
      "Iteration 39, loss = 0.28316998\n",
      "Iteration 40, loss = 0.27682415\n",
      "Iteration 41, loss = 0.27439887\n",
      "Iteration 42, loss = 0.28077775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.42151894\n",
      "Iteration 2, loss = 0.34186727\n",
      "Iteration 3, loss = 0.31679228\n",
      "Iteration 4, loss = 0.30287305\n",
      "Iteration 5, loss = 0.29309950\n",
      "Iteration 6, loss = 0.28576152\n",
      "Iteration 7, loss = 0.28978015\n",
      "Iteration 8, loss = 0.28198268\n",
      "Iteration 9, loss = 0.28089363\n",
      "Iteration 10, loss = 0.27712700\n",
      "Iteration 11, loss = 0.27628600\n",
      "Iteration 12, loss = 0.29583465\n",
      "Iteration 13, loss = 0.28339407\n",
      "Iteration 14, loss = 0.28101336\n",
      "Iteration 15, loss = 0.27754933\n",
      "Iteration 16, loss = 0.27840929\n",
      "Iteration 17, loss = 0.28164941\n",
      "Iteration 18, loss = 0.28252430\n",
      "Iteration 19, loss = 0.27841760\n",
      "Iteration 20, loss = 0.28397716\n",
      "Iteration 21, loss = 0.27713089\n",
      "Iteration 22, loss = 0.27606080\n",
      "Iteration 23, loss = 0.27220012\n",
      "Iteration 24, loss = 0.26823327\n",
      "Iteration 25, loss = 0.27431739\n",
      "Iteration 26, loss = 0.26965281\n",
      "Iteration 27, loss = 0.26360360\n",
      "Iteration 28, loss = 0.26928454\n",
      "Iteration 29, loss = 0.26748154\n",
      "Iteration 30, loss = 0.26909094\n",
      "Iteration 31, loss = 0.26898226\n",
      "Iteration 32, loss = 0.26927090\n",
      "Iteration 33, loss = 0.26961169\n",
      "Iteration 34, loss = 0.27268473\n",
      "Iteration 35, loss = 0.26548275\n",
      "Iteration 36, loss = 0.26328813\n",
      "Iteration 37, loss = 0.26824222\n",
      "Iteration 38, loss = 0.26555288\n",
      "Iteration 39, loss = 0.26849856\n",
      "Iteration 40, loss = 0.26605283\n",
      "Iteration 41, loss = 0.26944256\n",
      "Iteration 42, loss = 0.26130335\n",
      "Iteration 43, loss = 0.26077314\n",
      "Iteration 44, loss = 0.25786360\n",
      "Iteration 45, loss = 0.26107840\n",
      "Iteration 46, loss = 0.26205652\n",
      "Iteration 47, loss = 0.26292893\n",
      "Iteration 48, loss = 0.26386465\n",
      "Iteration 49, loss = 0.26263117\n",
      "Iteration 50, loss = 0.26438390\n",
      "Iteration 51, loss = 0.26133263\n",
      "Iteration 52, loss = 0.26193811\n",
      "Iteration 53, loss = 0.26047349\n",
      "Iteration 54, loss = 0.26515849\n",
      "Iteration 55, loss = 0.26781119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32434942\n",
      "Iteration 2, loss = 0.29985038\n",
      "Iteration 3, loss = 0.29090230\n",
      "Iteration 4, loss = 0.28850457\n",
      "Iteration 5, loss = 0.27272719\n",
      "Iteration 6, loss = 0.27055070\n",
      "Iteration 7, loss = 0.27346628\n",
      "Iteration 8, loss = 0.27117008\n",
      "Iteration 9, loss = 0.26830509\n",
      "Iteration 10, loss = 0.26956159\n",
      "Iteration 11, loss = 0.26846116\n",
      "Iteration 12, loss = 0.27014146\n",
      "Iteration 13, loss = 0.27058057\n",
      "Iteration 14, loss = 0.27079866\n",
      "Iteration 15, loss = 0.26668562\n",
      "Iteration 16, loss = 0.26012376\n",
      "Iteration 17, loss = 0.25996659\n",
      "Iteration 18, loss = 0.26242659\n",
      "Iteration 19, loss = 0.26204022\n",
      "Iteration 20, loss = 0.26099314\n",
      "Iteration 21, loss = 0.26173423\n",
      "Iteration 22, loss = 0.25898780\n",
      "Iteration 23, loss = 0.25884459\n",
      "Iteration 24, loss = 0.25740253\n",
      "Iteration 25, loss = 0.25961267\n",
      "Iteration 26, loss = 0.25906162\n",
      "Iteration 27, loss = 0.25823804\n",
      "Iteration 28, loss = 0.25670945\n",
      "Iteration 29, loss = 0.25745858\n",
      "Iteration 30, loss = 0.25844053\n",
      "Iteration 31, loss = 0.25661233\n",
      "Iteration 32, loss = 0.25714210\n",
      "Iteration 33, loss = 0.25988589\n",
      "Iteration 34, loss = 0.26075137\n",
      "Iteration 35, loss = 0.26219823\n",
      "Iteration 36, loss = 0.25861344\n",
      "Iteration 37, loss = 0.25743115\n",
      "Iteration 38, loss = 0.26019598\n",
      "Iteration 39, loss = 0.25649596\n",
      "Iteration 40, loss = 0.25749612\n",
      "Iteration 41, loss = 0.26454367\n",
      "Iteration 42, loss = 0.26070327\n",
      "Iteration 43, loss = 0.25566664\n",
      "Iteration 44, loss = 0.25738582\n",
      "Iteration 45, loss = 0.25571767\n",
      "Iteration 46, loss = 0.25669669\n",
      "Iteration 47, loss = 0.25949312\n",
      "Iteration 48, loss = 0.25554268\n",
      "Iteration 49, loss = 0.25494504\n",
      "Iteration 50, loss = 0.25725210\n",
      "Iteration 51, loss = 0.25634936\n",
      "Iteration 52, loss = 0.25539986\n",
      "Iteration 53, loss = 0.25883489\n",
      "Iteration 54, loss = 0.25959323\n",
      "Iteration 55, loss = 0.26029152\n",
      "Iteration 56, loss = 0.25601634\n",
      "Iteration 57, loss = 0.25934361\n",
      "Iteration 58, loss = 0.25375830\n",
      "Iteration 59, loss = 0.25648503\n",
      "Iteration 60, loss = 0.25246916\n",
      "Iteration 61, loss = 0.25528138\n",
      "Iteration 62, loss = 0.26105891\n",
      "Iteration 63, loss = 0.25807591\n",
      "Iteration 64, loss = 0.25602675\n",
      "Iteration 65, loss = 0.25378010\n",
      "Iteration 66, loss = 0.25463779\n",
      "Iteration 67, loss = 0.25567661\n",
      "Iteration 68, loss = 0.25425159\n",
      "Iteration 69, loss = 0.25357130\n",
      "Iteration 70, loss = 0.25369497\n",
      "Iteration 71, loss = 0.25404609\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34871220\n",
      "Iteration 2, loss = 0.29981910\n",
      "Iteration 3, loss = 0.29185756\n",
      "Iteration 4, loss = 0.30017587\n",
      "Iteration 5, loss = 0.29834249\n",
      "Iteration 6, loss = 0.29422057\n",
      "Iteration 7, loss = 0.30070252\n",
      "Iteration 8, loss = 0.28407037\n",
      "Iteration 9, loss = 0.27963147\n",
      "Iteration 10, loss = 0.28080668\n",
      "Iteration 11, loss = 0.28355050\n",
      "Iteration 12, loss = 0.28023564\n",
      "Iteration 13, loss = 0.28010103\n",
      "Iteration 14, loss = 0.27455026\n",
      "Iteration 15, loss = 0.28526906\n",
      "Iteration 16, loss = 0.27852308\n",
      "Iteration 17, loss = 0.27462149\n",
      "Iteration 18, loss = 0.27743606\n",
      "Iteration 19, loss = 0.28895725\n",
      "Iteration 20, loss = 0.28475098\n",
      "Iteration 21, loss = 0.28036930\n",
      "Iteration 22, loss = 0.27169199\n",
      "Iteration 23, loss = 0.28344635\n",
      "Iteration 24, loss = 0.28039466\n",
      "Iteration 25, loss = 0.28113810\n",
      "Iteration 26, loss = 0.27874194\n",
      "Iteration 27, loss = 0.28485145\n",
      "Iteration 28, loss = 0.27753205\n",
      "Iteration 29, loss = 0.28464029\n",
      "Iteration 30, loss = 0.27865688\n",
      "Iteration 31, loss = 0.27479750\n",
      "Iteration 32, loss = 0.28201473\n",
      "Iteration 33, loss = 0.27963003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34238184\n",
      "Iteration 2, loss = 0.31384285\n",
      "Iteration 3, loss = 0.30361478\n",
      "Iteration 4, loss = 0.30548195\n",
      "Iteration 5, loss = 0.29190243\n",
      "Iteration 6, loss = 0.29103140\n",
      "Iteration 7, loss = 0.28946634\n",
      "Iteration 8, loss = 0.28631575\n",
      "Iteration 9, loss = 0.28216691\n",
      "Iteration 10, loss = 0.28566592\n",
      "Iteration 11, loss = 0.29092133\n",
      "Iteration 12, loss = 0.28104918\n",
      "Iteration 13, loss = 0.29106475\n",
      "Iteration 14, loss = 0.29157574\n",
      "Iteration 15, loss = 0.28143914\n",
      "Iteration 16, loss = 0.28289649\n",
      "Iteration 17, loss = 0.28134580\n",
      "Iteration 18, loss = 0.28604292\n",
      "Iteration 19, loss = 0.27760337\n",
      "Iteration 20, loss = 0.27864260\n",
      "Iteration 21, loss = 0.29139117\n",
      "Iteration 22, loss = 0.28209263\n",
      "Iteration 23, loss = 0.28480288\n",
      "Iteration 24, loss = 0.27685347\n",
      "Iteration 25, loss = 0.28133348\n",
      "Iteration 26, loss = 0.27697744\n",
      "Iteration 27, loss = 0.28828846\n",
      "Iteration 28, loss = 0.27967856\n",
      "Iteration 29, loss = 0.28461807\n",
      "Iteration 30, loss = 0.28111258\n",
      "Iteration 31, loss = 0.27982044\n",
      "Iteration 32, loss = 0.28103362\n",
      "Iteration 33, loss = 0.28396862\n",
      "Iteration 34, loss = 0.29236654\n",
      "Iteration 35, loss = 0.28160088\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03730475\n",
      "Iteration 2, loss = 0.52652208\n",
      "Iteration 3, loss = 0.53688611\n",
      "Iteration 4, loss = 0.51532897\n",
      "Iteration 5, loss = 0.45662602\n",
      "Iteration 6, loss = 0.44074748\n",
      "Iteration 7, loss = 0.38952303\n",
      "Iteration 8, loss = 0.35552700\n",
      "Iteration 9, loss = 0.37544397\n",
      "Iteration 10, loss = 0.36104833\n",
      "Iteration 11, loss = 0.34926239\n",
      "Iteration 12, loss = 0.35322118\n",
      "Iteration 13, loss = 0.34761807\n",
      "Iteration 14, loss = 0.37255946\n",
      "Iteration 15, loss = 0.33101116\n",
      "Iteration 16, loss = 0.31430193\n",
      "Iteration 17, loss = 0.32052047\n",
      "Iteration 18, loss = 0.30944213\n",
      "Iteration 19, loss = 0.30697693\n",
      "Iteration 20, loss = 0.33581713\n",
      "Iteration 21, loss = 0.33031619\n",
      "Iteration 22, loss = 0.32358899\n",
      "Iteration 23, loss = 0.31380988\n",
      "Iteration 24, loss = 0.30066387\n",
      "Iteration 25, loss = 0.28981257\n",
      "Iteration 26, loss = 0.31025298\n",
      "Iteration 27, loss = 0.29804076\n",
      "Iteration 28, loss = 0.29650502\n",
      "Iteration 29, loss = 0.29480945\n",
      "Iteration 30, loss = 0.28688976\n",
      "Iteration 31, loss = 0.28709372\n",
      "Iteration 32, loss = 0.27866164\n",
      "Iteration 33, loss = 0.27312111\n",
      "Iteration 34, loss = 0.31659889\n",
      "Iteration 35, loss = 0.29414948\n",
      "Iteration 36, loss = 0.28156818\n",
      "Iteration 37, loss = 0.28526314\n",
      "Iteration 38, loss = 0.31120116\n",
      "Iteration 39, loss = 0.28912902\n",
      "Iteration 40, loss = 0.27391718\n",
      "Iteration 41, loss = 0.28063642\n",
      "Iteration 42, loss = 0.28396534\n",
      "Iteration 43, loss = 0.27088316\n",
      "Iteration 44, loss = 0.27864633\n",
      "Iteration 45, loss = 0.27422212\n",
      "Iteration 46, loss = 0.28068271\n",
      "Iteration 47, loss = 0.28768431\n",
      "Iteration 48, loss = 0.28755169\n",
      "Iteration 49, loss = 0.29074401\n",
      "Iteration 50, loss = 0.26797507\n",
      "Iteration 51, loss = 0.28172250\n",
      "Iteration 52, loss = 0.27057843\n",
      "Iteration 1, loss = 0.42151894\n",
      "Iteration 2, loss = 0.34186727\n",
      "Iteration 3, loss = 0.31679228\n",
      "Iteration 4, loss = 0.30287305\n",
      "Iteration 5, loss = 0.29309950\n",
      "Iteration 6, loss = 0.28576152\n",
      "Iteration 7, loss = 0.28978015\n",
      "Iteration 8, loss = 0.28198268\n",
      "Iteration 9, loss = 0.28089363\n",
      "Iteration 10, loss = 0.27712700\n",
      "Iteration 11, loss = 0.27628600\n",
      "Iteration 12, loss = 0.29583465\n",
      "Iteration 13, loss = 0.28339407\n",
      "Iteration 14, loss = 0.28101336\n",
      "Iteration 15, loss = 0.27754933\n",
      "Iteration 16, loss = 0.27840929\n",
      "Iteration 17, loss = 0.28164941\n",
      "Iteration 18, loss = 0.28252430\n",
      "Iteration 19, loss = 0.27841760\n",
      "Iteration 20, loss = 0.28397716\n",
      "Iteration 21, loss = 0.27713089\n",
      "Iteration 22, loss = 0.27606080\n",
      "Iteration 23, loss = 0.27220012\n",
      "Iteration 24, loss = 0.26823327\n",
      "Iteration 25, loss = 0.27431739\n",
      "Iteration 26, loss = 0.26965281\n",
      "Iteration 27, loss = 0.26360360\n",
      "Iteration 28, loss = 0.26928454\n",
      "Iteration 29, loss = 0.26748154\n",
      "Iteration 30, loss = 0.26909094\n",
      "Iteration 31, loss = 0.26898226\n",
      "Iteration 32, loss = 0.26927090\n",
      "Iteration 33, loss = 0.26961169\n",
      "Iteration 34, loss = 0.27268473\n",
      "Iteration 35, loss = 0.26548275\n",
      "Iteration 36, loss = 0.26328813\n",
      "Iteration 37, loss = 0.26824222\n",
      "Iteration 38, loss = 0.26555288\n",
      "Iteration 39, loss = 0.26849856\n",
      "Iteration 40, loss = 0.26605283\n",
      "Iteration 41, loss = 0.26944256\n",
      "Iteration 42, loss = 0.26130335\n",
      "Iteration 43, loss = 0.26077314\n",
      "Iteration 44, loss = 0.25786360\n",
      "Iteration 45, loss = 0.26107840\n",
      "Iteration 46, loss = 0.26205652\n",
      "Iteration 47, loss = 0.26292893\n",
      "Iteration 48, loss = 0.26386465\n",
      "Iteration 49, loss = 0.26263117\n",
      "Iteration 50, loss = 0.26438390\n",
      "Iteration 51, loss = 0.26133263\n",
      "Iteration 52, loss = 0.26193811\n",
      "Iteration 53, loss = 0.26047349\n",
      "Iteration 54, loss = 0.26515849\n",
      "Iteration 55, loss = 0.26781119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32904531\n",
      "Iteration 2, loss = 0.29110035\n",
      "Iteration 3, loss = 0.29830697\n",
      "Iteration 4, loss = 0.28318135\n",
      "Iteration 5, loss = 0.27201083\n",
      "Iteration 6, loss = 0.27022070\n",
      "Iteration 7, loss = 0.27244079\n",
      "Iteration 8, loss = 0.27501578\n",
      "Iteration 9, loss = 0.27454934\n",
      "Iteration 10, loss = 0.27265697\n",
      "Iteration 11, loss = 0.26967268\n",
      "Iteration 12, loss = 0.26772307\n",
      "Iteration 13, loss = 0.26164884\n",
      "Iteration 14, loss = 0.26199522\n",
      "Iteration 15, loss = 0.26499844\n",
      "Iteration 16, loss = 0.26487155\n",
      "Iteration 17, loss = 0.26220143\n",
      "Iteration 18, loss = 0.26251682\n",
      "Iteration 19, loss = 0.26034051\n",
      "Iteration 20, loss = 0.26279473\n",
      "Iteration 21, loss = 0.26090456\n",
      "Iteration 22, loss = 0.25993794\n",
      "Iteration 23, loss = 0.26188459\n",
      "Iteration 24, loss = 0.25959759\n",
      "Iteration 25, loss = 0.26023603\n",
      "Iteration 26, loss = 0.25755082\n",
      "Iteration 27, loss = 0.25804581\n",
      "Iteration 28, loss = 0.25996110\n",
      "Iteration 29, loss = 0.25777890\n",
      "Iteration 30, loss = 0.25990389\n",
      "Iteration 31, loss = 0.25792748\n",
      "Iteration 32, loss = 0.26395468\n",
      "Iteration 33, loss = 0.25691033\n",
      "Iteration 34, loss = 0.25835515\n",
      "Iteration 35, loss = 0.25964617\n",
      "Iteration 36, loss = 0.25858099\n",
      "Iteration 37, loss = 0.26110598\n",
      "Iteration 38, loss = 0.25752426\n",
      "Iteration 39, loss = 0.25715615\n",
      "Iteration 40, loss = 0.25326858\n",
      "Iteration 41, loss = 0.25340135\n",
      "Iteration 42, loss = 0.25303187\n",
      "Iteration 43, loss = 0.25336714\n",
      "Iteration 44, loss = 0.25513878\n",
      "Iteration 45, loss = 0.25416924\n",
      "Iteration 46, loss = 0.25260805\n",
      "Iteration 47, loss = 0.25626126\n",
      "Iteration 48, loss = 0.25502780\n",
      "Iteration 49, loss = 0.25477410\n",
      "Iteration 50, loss = 0.25401240\n",
      "Iteration 51, loss = 0.25079106\n",
      "Iteration 52, loss = 0.25291992\n",
      "Iteration 53, loss = 0.25373357\n",
      "Iteration 54, loss = 0.25711021\n",
      "Iteration 55, loss = 0.26007343\n",
      "Iteration 56, loss = 0.25728247\n",
      "Iteration 57, loss = 0.25498677\n",
      "Iteration 58, loss = 0.25465040\n",
      "Iteration 59, loss = 0.25347917\n",
      "Iteration 60, loss = 0.25006074\n",
      "Iteration 61, loss = 0.25500249\n",
      "Iteration 62, loss = 0.25671231\n",
      "Iteration 63, loss = 0.25372548\n",
      "Iteration 64, loss = 0.25364752\n",
      "Iteration 65, loss = 0.25439743\n",
      "Iteration 66, loss = 0.25722324\n",
      "Iteration 67, loss = 0.25804147\n",
      "Iteration 68, loss = 0.25606356\n",
      "Iteration 69, loss = 0.25184108\n",
      "Iteration 70, loss = 0.25290860\n",
      "Iteration 71, loss = 0.25878823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33969944\n",
      "Iteration 2, loss = 0.31447001\n",
      "Iteration 3, loss = 0.30077515\n",
      "Iteration 4, loss = 0.29296112\n",
      "Iteration 5, loss = 0.29624165\n",
      "Iteration 6, loss = 0.28895337\n",
      "Iteration 7, loss = 0.28212130\n",
      "Iteration 8, loss = 0.29246379\n",
      "Iteration 9, loss = 0.28455260\n",
      "Iteration 10, loss = 0.28570243\n",
      "Iteration 11, loss = 0.27872090\n",
      "Iteration 12, loss = 0.27897943\n",
      "Iteration 13, loss = 0.27810370\n",
      "Iteration 14, loss = 0.28219594\n",
      "Iteration 15, loss = 0.27571478\n",
      "Iteration 16, loss = 0.27969054\n",
      "Iteration 17, loss = 0.28249519\n",
      "Iteration 18, loss = 0.28444932\n",
      "Iteration 19, loss = 0.28018518\n",
      "Iteration 20, loss = 0.27543873\n",
      "Iteration 21, loss = 0.28336504\n",
      "Iteration 22, loss = 0.28011209\n",
      "Iteration 23, loss = 0.28260725\n",
      "Iteration 24, loss = 0.28103323\n",
      "Iteration 25, loss = 0.27666036\n",
      "Iteration 26, loss = 0.27555071\n",
      "Iteration 27, loss = 0.27562886\n",
      "Iteration 28, loss = 0.27443534\n",
      "Iteration 29, loss = 0.27585572\n",
      "Iteration 30, loss = 0.28362881\n",
      "Iteration 31, loss = 0.28319067\n",
      "Iteration 32, loss = 0.27869514\n",
      "Iteration 33, loss = 0.27837626\n",
      "Iteration 34, loss = 0.27506372\n",
      "Iteration 35, loss = 0.28144086\n",
      "Iteration 36, loss = 0.28676845\n",
      "Iteration 37, loss = 0.27660267\n",
      "Iteration 38, loss = 0.28035506\n",
      "Iteration 39, loss = 0.27865599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34772470\n",
      "Iteration 2, loss = 0.30868981\n",
      "Iteration 3, loss = 0.30100217\n",
      "Iteration 4, loss = 0.29633502\n",
      "Iteration 5, loss = 0.28880930\n",
      "Iteration 6, loss = 0.29520778\n",
      "Iteration 7, loss = 0.29105342\n",
      "Iteration 8, loss = 0.28922895\n",
      "Iteration 9, loss = 0.28774358\n",
      "Iteration 10, loss = 0.29083045\n",
      "Iteration 11, loss = 0.29504800\n",
      "Iteration 12, loss = 0.28148057\n",
      "Iteration 13, loss = 0.28895930\n",
      "Iteration 14, loss = 0.28373150\n",
      "Iteration 15, loss = 0.28125956\n",
      "Iteration 16, loss = 0.28586745\n",
      "Iteration 17, loss = 0.28441730\n",
      "Iteration 18, loss = 0.28368598\n",
      "Iteration 19, loss = 0.29072603\n",
      "Iteration 20, loss = 0.28072587\n",
      "Iteration 21, loss = 0.28169707\n",
      "Iteration 22, loss = 0.28171458\n",
      "Iteration 23, loss = 0.27588566\n",
      "Iteration 24, loss = 0.28317490\n",
      "Iteration 25, loss = 0.28260859\n",
      "Iteration 26, loss = 0.27978162\n",
      "Iteration 27, loss = 0.28348853\n",
      "Iteration 28, loss = 0.27940686\n",
      "Iteration 29, loss = 0.30550173\n",
      "Iteration 30, loss = 0.28348657\n",
      "Iteration 31, loss = 0.28131422\n",
      "Iteration 32, loss = 0.29738083\n",
      "Iteration 33, loss = 0.28740456\n",
      "Iteration 34, loss = 0.28238663\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.49594265\n",
      "Iteration 2, loss = 1.22921284\n",
      "Iteration 3, loss = 0.47256405\n",
      "Iteration 4, loss = 0.45062218\n",
      "Iteration 5, loss = 0.51261721\n",
      "Iteration 6, loss = 0.44927954\n",
      "Iteration 7, loss = 0.46803254\n",
      "Iteration 8, loss = 0.40689024\n",
      "Iteration 9, loss = 0.36773671\n",
      "Iteration 10, loss = 0.35649508\n",
      "Iteration 11, loss = 0.37534321\n",
      "Iteration 12, loss = 0.33554524\n",
      "Iteration 13, loss = 0.33289111\n",
      "Iteration 14, loss = 0.33867661\n",
      "Iteration 15, loss = 0.35318457\n",
      "Iteration 16, loss = 0.32943565\n",
      "Iteration 17, loss = 0.31917649\n",
      "Iteration 18, loss = 0.30649750\n",
      "Iteration 19, loss = 0.36253560\n",
      "Iteration 20, loss = 0.35142757\n",
      "Iteration 21, loss = 0.31832238\n",
      "Iteration 22, loss = 0.32347994\n",
      "Iteration 23, loss = 0.32871071\n",
      "Iteration 24, loss = 0.31214576\n",
      "Iteration 25, loss = 0.29464860\n",
      "Iteration 26, loss = 0.31809194\n",
      "Iteration 27, loss = 0.30749369\n",
      "Iteration 28, loss = 0.32543451\n",
      "Iteration 29, loss = 0.34444654\n",
      "Iteration 30, loss = 0.31269871\n",
      "Iteration 31, loss = 0.33041913\n",
      "Iteration 32, loss = 0.30674637\n",
      "Iteration 33, loss = 0.34182164\n",
      "Iteration 34, loss = 0.30604591\n",
      "Iteration 35, loss = 0.28594169\n",
      "Iteration 36, loss = 0.30086925\n",
      "Iteration 37, loss = 0.30667731\n",
      "Iteration 38, loss = 0.30616852\n",
      "Iteration 39, loss = 0.30666560\n",
      "Iteration 40, loss = 0.30159996\n",
      "Iteration 41, loss = 0.30412169\n",
      "Iteration 42, loss = 0.28408027\n",
      "Iteration 43, loss = 0.29041946\n",
      "Iteration 44, loss = 0.29737091\n",
      "Iteration 45, loss = 0.29042439\n",
      "Iteration 46, loss = 0.28349025\n",
      "Iteration 47, loss = 0.27507149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39242055\n",
      "Iteration 2, loss = 0.31204405\n",
      "Iteration 3, loss = 0.28957022\n",
      "Iteration 4, loss = 0.29374337\n",
      "Iteration 5, loss = 0.29989757\n",
      "Iteration 6, loss = 0.29258826\n",
      "Iteration 7, loss = 0.28457235\n",
      "Iteration 8, loss = 0.29022549\n",
      "Iteration 9, loss = 0.28188179\n",
      "Iteration 10, loss = 0.26999265\n",
      "Iteration 11, loss = 0.27272760\n",
      "Iteration 12, loss = 0.27325180\n",
      "Iteration 13, loss = 0.27196838\n",
      "Iteration 14, loss = 0.28348588\n",
      "Iteration 15, loss = 0.27541411\n",
      "Iteration 16, loss = 0.27938415\n",
      "Iteration 17, loss = 0.26783011\n",
      "Iteration 18, loss = 0.26672062\n",
      "Iteration 19, loss = 0.27240025\n",
      "Iteration 20, loss = 0.27135273\n",
      "Iteration 21, loss = 0.27228484\n",
      "Iteration 22, loss = 0.27035860\n",
      "Iteration 23, loss = 0.27653132\n",
      "Iteration 24, loss = 0.26120957\n",
      "Iteration 25, loss = 0.27231464\n",
      "Iteration 26, loss = 0.26824774\n",
      "Iteration 27, loss = 0.26252530\n",
      "Iteration 28, loss = 0.27605703\n",
      "Iteration 29, loss = 0.27084002\n",
      "Iteration 30, loss = 0.26141037\n",
      "Iteration 31, loss = 0.27804010\n",
      "Iteration 32, loss = 0.26967168\n",
      "Iteration 33, loss = 0.26427760\n",
      "Iteration 34, loss = 0.26486691\n",
      "Iteration 35, loss = 0.25939272\n",
      "Iteration 36, loss = 0.26296934\n",
      "Iteration 37, loss = 0.26197350\n",
      "Iteration 38, loss = 0.26352083\n",
      "Iteration 39, loss = 0.26417965\n",
      "Iteration 40, loss = 0.25778622\n",
      "Iteration 41, loss = 0.27372531\n",
      "Iteration 42, loss = 0.27316681\n",
      "Iteration 43, loss = 0.26286818\n",
      "Iteration 44, loss = 0.26230359\n",
      "Iteration 45, loss = 0.26592481\n",
      "Iteration 46, loss = 0.26076650\n",
      "Iteration 47, loss = 0.27010382\n",
      "Iteration 48, loss = 0.26818141\n",
      "Iteration 49, loss = 0.26268712\n",
      "Iteration 50, loss = 0.26497388\n",
      "Iteration 51, loss = 0.25999259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32904531\n",
      "Iteration 2, loss = 0.29110035\n",
      "Iteration 3, loss = 0.29830697\n",
      "Iteration 4, loss = 0.28318135\n",
      "Iteration 5, loss = 0.27201083\n",
      "Iteration 6, loss = 0.27022070\n",
      "Iteration 7, loss = 0.27244079\n",
      "Iteration 8, loss = 0.27501578\n",
      "Iteration 9, loss = 0.27454934\n",
      "Iteration 10, loss = 0.27265697\n",
      "Iteration 11, loss = 0.26967268\n",
      "Iteration 12, loss = 0.26772307\n",
      "Iteration 13, loss = 0.26164884\n",
      "Iteration 14, loss = 0.26199522\n",
      "Iteration 15, loss = 0.26499844\n",
      "Iteration 16, loss = 0.26487155\n",
      "Iteration 17, loss = 0.26220143\n",
      "Iteration 18, loss = 0.26251682\n",
      "Iteration 19, loss = 0.26034051\n",
      "Iteration 20, loss = 0.26279473\n",
      "Iteration 21, loss = 0.26090456\n",
      "Iteration 22, loss = 0.25993794\n",
      "Iteration 23, loss = 0.26188459\n",
      "Iteration 24, loss = 0.25959759\n",
      "Iteration 25, loss = 0.26023603\n",
      "Iteration 26, loss = 0.25755082\n",
      "Iteration 27, loss = 0.25804581\n",
      "Iteration 28, loss = 0.25996110\n",
      "Iteration 29, loss = 0.25777890\n",
      "Iteration 30, loss = 0.25990389\n",
      "Iteration 31, loss = 0.25792748\n",
      "Iteration 32, loss = 0.26395468\n",
      "Iteration 33, loss = 0.25691033\n",
      "Iteration 34, loss = 0.25835515\n",
      "Iteration 35, loss = 0.25964617\n",
      "Iteration 36, loss = 0.25858099\n",
      "Iteration 37, loss = 0.26110598\n",
      "Iteration 38, loss = 0.25752426\n",
      "Iteration 39, loss = 0.25715615\n",
      "Iteration 40, loss = 0.25326858\n",
      "Iteration 41, loss = 0.25340135\n",
      "Iteration 42, loss = 0.25303187\n",
      "Iteration 43, loss = 0.25336714\n",
      "Iteration 44, loss = 0.25513878\n",
      "Iteration 45, loss = 0.25416924\n",
      "Iteration 46, loss = 0.25260805\n",
      "Iteration 47, loss = 0.25626126\n",
      "Iteration 48, loss = 0.25502780\n",
      "Iteration 49, loss = 0.25477410\n",
      "Iteration 50, loss = 0.25401240\n",
      "Iteration 51, loss = 0.25079106\n",
      "Iteration 52, loss = 0.25291992\n",
      "Iteration 53, loss = 0.25373357\n",
      "Iteration 54, loss = 0.25711021\n",
      "Iteration 55, loss = 0.26007343\n",
      "Iteration 56, loss = 0.25728247\n",
      "Iteration 57, loss = 0.25498677\n",
      "Iteration 58, loss = 0.25465040\n",
      "Iteration 59, loss = 0.25347917\n",
      "Iteration 60, loss = 0.25006074\n",
      "Iteration 61, loss = 0.25500249\n",
      "Iteration 62, loss = 0.25671231\n",
      "Iteration 63, loss = 0.25372548\n",
      "Iteration 64, loss = 0.25364752\n",
      "Iteration 65, loss = 0.25439743\n",
      "Iteration 66, loss = 0.25722324\n",
      "Iteration 67, loss = 0.25804147\n",
      "Iteration 68, loss = 0.25606356\n",
      "Iteration 69, loss = 0.25184108\n",
      "Iteration 70, loss = 0.25290860\n",
      "Iteration 71, loss = 0.25878823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.39443838\n",
      "Iteration 2, loss = 0.33292099\n",
      "Iteration 3, loss = 0.32900879\n",
      "Iteration 4, loss = 0.29889072\n",
      "Iteration 5, loss = 0.29166310\n",
      "Iteration 6, loss = 0.29106813\n",
      "Iteration 7, loss = 0.28847920\n",
      "Iteration 8, loss = 0.30751759\n",
      "Iteration 9, loss = 0.30405878\n",
      "Iteration 10, loss = 0.30945946\n",
      "Iteration 11, loss = 0.29037193\n",
      "Iteration 12, loss = 0.28887894\n",
      "Iteration 13, loss = 0.27965071\n",
      "Iteration 14, loss = 0.30863038\n",
      "Iteration 15, loss = 0.28092835\n",
      "Iteration 16, loss = 0.28911840\n",
      "Iteration 17, loss = 0.27822269\n",
      "Iteration 18, loss = 0.27948408\n",
      "Iteration 19, loss = 0.28948319\n",
      "Iteration 20, loss = 0.29806050\n",
      "Iteration 21, loss = 0.28314987\n",
      "Iteration 22, loss = 0.27505109\n",
      "Iteration 23, loss = 0.27916823\n",
      "Iteration 24, loss = 0.27733262\n",
      "Iteration 25, loss = 0.27643289\n",
      "Iteration 26, loss = 0.29130423\n",
      "Iteration 27, loss = 0.28522523\n",
      "Iteration 28, loss = 0.28498711\n",
      "Iteration 29, loss = 0.29282798\n",
      "Iteration 30, loss = 0.28597295\n",
      "Iteration 31, loss = 0.27376143\n",
      "Iteration 32, loss = 0.27422798\n",
      "Iteration 33, loss = 0.27979881\n",
      "Iteration 34, loss = 0.28598303\n",
      "Iteration 35, loss = 0.28640313\n",
      "Iteration 36, loss = 0.28398905\n",
      "Iteration 37, loss = 0.28586615\n",
      "Iteration 38, loss = 0.28168766\n",
      "Iteration 39, loss = 0.28092529\n",
      "Iteration 40, loss = 0.28327101\n",
      "Iteration 41, loss = 0.28525217\n",
      "Iteration 42, loss = 0.28102315\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33242854\n",
      "Iteration 2, loss = 0.31074654\n",
      "Iteration 3, loss = 0.29896558\n",
      "Iteration 4, loss = 0.28379778\n",
      "Iteration 5, loss = 0.28541444\n",
      "Iteration 6, loss = 0.28944530\n",
      "Iteration 7, loss = 0.29107797\n",
      "Iteration 8, loss = 0.28482574\n",
      "Iteration 9, loss = 0.27932808\n",
      "Iteration 10, loss = 0.28020947\n",
      "Iteration 11, loss = 0.27995085\n",
      "Iteration 12, loss = 0.28132452\n",
      "Iteration 13, loss = 0.28209399\n",
      "Iteration 14, loss = 0.29129478\n",
      "Iteration 15, loss = 0.28026812\n",
      "Iteration 16, loss = 0.27967520\n",
      "Iteration 17, loss = 0.27887279\n",
      "Iteration 18, loss = 0.27645326\n",
      "Iteration 19, loss = 0.27746107\n",
      "Iteration 20, loss = 0.28138703\n",
      "Iteration 21, loss = 0.27854840\n",
      "Iteration 22, loss = 0.28120016\n",
      "Iteration 23, loss = 0.28509941\n",
      "Iteration 24, loss = 0.28299080\n",
      "Iteration 25, loss = 0.27729659\n",
      "Iteration 26, loss = 0.27421597\n",
      "Iteration 27, loss = 0.27801730\n",
      "Iteration 28, loss = 0.28217224\n",
      "Iteration 29, loss = 0.27550744\n",
      "Iteration 30, loss = 0.27389538\n",
      "Iteration 31, loss = 0.27250447\n",
      "Iteration 32, loss = 0.27927032\n",
      "Iteration 33, loss = 0.28050985\n",
      "Iteration 34, loss = 0.27376671\n",
      "Iteration 35, loss = 0.27917285\n",
      "Iteration 36, loss = 0.28395756\n",
      "Iteration 37, loss = 0.27420055\n",
      "Iteration 38, loss = 0.27695567\n",
      "Iteration 39, loss = 0.28316998\n",
      "Iteration 40, loss = 0.27682415\n",
      "Iteration 41, loss = 0.27439887\n",
      "Iteration 42, loss = 0.28077775\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.03413256\n",
      "Iteration 2, loss = 3.07114095\n",
      "Iteration 3, loss = 1.11477156\n",
      "Iteration 4, loss = 0.44204779\n",
      "Iteration 5, loss = 0.45532293\n",
      "Iteration 6, loss = 0.40033971\n",
      "Iteration 7, loss = 0.38091723\n",
      "Iteration 8, loss = 0.39747437\n",
      "Iteration 9, loss = 0.40465199\n",
      "Iteration 10, loss = 0.41344020\n",
      "Iteration 11, loss = 0.38110640\n",
      "Iteration 12, loss = 0.39583711\n",
      "Iteration 13, loss = 0.36775032\n",
      "Iteration 14, loss = 0.39125929\n",
      "Iteration 15, loss = 0.36414286\n",
      "Iteration 16, loss = 0.36872219\n",
      "Iteration 17, loss = 0.36541256\n",
      "Iteration 18, loss = 0.36454400\n",
      "Iteration 19, loss = 0.37291661\n",
      "Iteration 20, loss = 0.35169187\n",
      "Iteration 21, loss = 0.41795901\n",
      "Iteration 22, loss = 0.32059130\n",
      "Iteration 23, loss = 0.31682976\n",
      "Iteration 24, loss = 0.32532298\n",
      "Iteration 25, loss = 0.33116416\n",
      "Iteration 26, loss = 0.31867744\n",
      "Iteration 27, loss = 0.29765014\n",
      "Iteration 28, loss = 0.32097079\n",
      "Iteration 29, loss = 0.31699989\n",
      "Iteration 30, loss = 0.31152578\n",
      "Iteration 31, loss = 0.30375591\n",
      "Iteration 32, loss = 0.30039964\n",
      "Iteration 33, loss = 0.29378106\n",
      "Iteration 34, loss = 0.30104505\n",
      "Iteration 35, loss = 0.31430391\n",
      "Iteration 36, loss = 0.32032709\n",
      "Iteration 37, loss = 0.29475830\n",
      "Iteration 38, loss = 0.30452927\n",
      "Iteration 39, loss = 0.32654990\n",
      "Iteration 40, loss = 0.31229484\n",
      "Iteration 23, loss = 0.27988077\n",
      "Iteration 24, loss = 0.28031619\n",
      "Iteration 25, loss = 0.29849990\n",
      "Iteration 26, loss = 0.28053530\n",
      "Iteration 27, loss = 0.28798878\n",
      "Iteration 28, loss = 0.28110858\n",
      "Iteration 29, loss = 0.27508688\n",
      "Iteration 30, loss = 0.28056612\n",
      "Iteration 31, loss = 0.28385828\n",
      "Iteration 32, loss = 0.28053998\n",
      "Iteration 33, loss = 0.28835195\n",
      "Iteration 34, loss = 0.27869765\n",
      "Iteration 35, loss = 0.28081795\n",
      "Iteration 36, loss = 0.28706433\n",
      "Iteration 37, loss = 0.28518791\n",
      "Iteration 38, loss = 0.28930397\n",
      "Iteration 39, loss = 0.28047430\n",
      "Iteration 40, loss = 0.27632530\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.62706357\n",
      "Iteration 2, loss = 0.55629896\n",
      "Iteration 3, loss = 0.41722030\n",
      "Iteration 4, loss = 0.40907008\n",
      "Iteration 5, loss = 0.41138787\n",
      "Iteration 6, loss = 0.39820108\n",
      "Iteration 7, loss = 0.36213394\n",
      "Iteration 8, loss = 0.36245695\n",
      "Iteration 9, loss = 0.37257134\n",
      "Iteration 10, loss = 0.38364261\n",
      "Iteration 11, loss = 0.33257169\n",
      "Iteration 12, loss = 0.34460911\n",
      "Iteration 13, loss = 0.34227834\n",
      "Iteration 14, loss = 0.33524577\n",
      "Iteration 15, loss = 0.33397612\n",
      "Iteration 16, loss = 0.33797543\n",
      "Iteration 17, loss = 0.32768054\n",
      "Iteration 18, loss = 0.31329356\n",
      "Iteration 19, loss = 0.30191182\n",
      "Iteration 20, loss = 0.30947770\n",
      "Iteration 21, loss = 0.30191768\n",
      "Iteration 22, loss = 0.29710415\n",
      "Iteration 23, loss = 0.28373661\n",
      "Iteration 24, loss = 0.28913701\n",
      "Iteration 25, loss = 0.27867094\n",
      "Iteration 26, loss = 0.28857820\n",
      "Iteration 27, loss = 0.26955110\n",
      "Iteration 28, loss = 0.27159739\n",
      "Iteration 29, loss = 0.26655253\n",
      "Iteration 30, loss = 0.29026483\n",
      "Iteration 31, loss = 0.28287326\n",
      "Iteration 32, loss = 0.26922311\n",
      "Iteration 33, loss = 0.27111372\n",
      "Iteration 34, loss = 0.27848335\n",
      "Iteration 35, loss = 0.26453662\n",
      "Iteration 36, loss = 0.27167554\n",
      "Iteration 37, loss = 0.27504083\n",
      "Iteration 38, loss = 0.27304957\n",
      "Iteration 39, loss = 0.26102637\n",
      "Iteration 40, loss = 0.26355637\n",
      "Iteration 41, loss = 0.25886670\n",
      "Iteration 42, loss = 0.27197636\n",
      "Iteration 43, loss = 0.26509269\n",
      "Iteration 44, loss = 0.27040923\n",
      "Iteration 45, loss = 0.26027314\n",
      "Iteration 46, loss = 0.26578179\n",
      "Iteration 47, loss = 0.26304403\n",
      "Iteration 48, loss = 0.27098182\n",
      "Iteration 49, loss = 0.25686861\n",
      "Iteration 50, loss = 0.25887245\n",
      "Iteration 51, loss = 0.26960251\n",
      "Iteration 52, loss = 0.27005669\n",
      "Iteration 53, loss = 0.25519648\n",
      "Iteration 54, loss = 0.26327628\n",
      "Iteration 55, loss = 0.25664010\n",
      "Iteration 56, loss = 0.26070010\n",
      "Iteration 57, loss = 0.25805642\n",
      "Iteration 58, loss = 0.25466238\n",
      "Iteration 59, loss = 0.27238738\n",
      "Iteration 60, loss = 0.25996688\n",
      "Iteration 61, loss = 0.26579419\n",
      "Iteration 62, loss = 0.26373347\n",
      "Iteration 63, loss = 0.25838941\n",
      "Iteration 64, loss = 0.26116575\n",
      "Iteration 65, loss = 0.26035650\n",
      "Iteration 66, loss = 0.25889587\n",
      "Iteration 67, loss = 0.25870187\n",
      "Iteration 68, loss = 0.25710754\n",
      "Iteration 69, loss = 0.25478785\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.38533449\n",
      "Iteration 2, loss = 0.78497038\n",
      "Iteration 3, loss = 0.40781880\n",
      "Iteration 4, loss = 0.36660022\n",
      "Iteration 5, loss = 0.37169451\n",
      "Iteration 6, loss = 0.42997659\n",
      "Iteration 7, loss = 0.40835345\n",
      "Iteration 8, loss = 0.41248069\n",
      "Iteration 9, loss = 0.41981221\n",
      "Iteration 10, loss = 0.41149161\n",
      "Iteration 11, loss = 0.39686428\n",
      "Iteration 12, loss = 0.39961607\n",
      "Iteration 13, loss = 0.39299466\n",
      "Iteration 14, loss = 0.38896295\n",
      "Iteration 15, loss = 0.39922846\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.54253066\n",
      "Iteration 2, loss = 2.52914855\n",
      "Iteration 3, loss = 2.55054785\n",
      "Iteration 4, loss = 1.71393858\n",
      "Iteration 5, loss = 0.93363453\n",
      "Iteration 6, loss = 0.61990076\n",
      "Iteration 7, loss = 0.32450744\n",
      "Iteration 8, loss = 0.31591583\n",
      "Iteration 9, loss = 0.31176526\n",
      "Iteration 10, loss = 0.31547522\n",
      "Iteration 11, loss = 0.30345723\n",
      "Iteration 12, loss = 0.32107919\n",
      "Iteration 13, loss = 0.33835823\n",
      "Iteration 14, loss = 0.33754201\n",
      "Iteration 15, loss = 0.31552879\n",
      "Iteration 16, loss = 0.34924676\n",
      "Iteration 17, loss = 0.32793207\n",
      "Iteration 18, loss = 0.34031884\n",
      "Iteration 19, loss = 0.34067932\n",
      "Iteration 20, loss = 0.34898794\n",
      "Iteration 21, loss = 0.34937620\n",
      "Iteration 22, loss = 0.35436421\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.93477381\n",
      "Iteration 2, loss = 0.45963750\n",
      "Iteration 3, loss = 0.43059504\n",
      "Iteration 4, loss = 0.42807264\n",
      "Iteration 5, loss = 0.43102386\n",
      "Iteration 6, loss = 0.44070639\n",
      "Iteration 7, loss = 0.43935673\n",
      "Iteration 8, loss = 0.39118774\n",
      "Iteration 9, loss = 0.36298326\n",
      "Iteration 10, loss = 0.35317001\n",
      "Iteration 11, loss = 0.34614297\n",
      "Iteration 12, loss = 0.33337969\n",
      "Iteration 13, loss = 0.32435777\n",
      "Iteration 14, loss = 0.35465160\n",
      "Iteration 15, loss = 0.34302293\n",
      "Iteration 16, loss = 0.30878997\n",
      "Iteration 17, loss = 0.31243914\n",
      "Iteration 18, loss = 0.31110067\n",
      "Iteration 19, loss = 0.31140267\n",
      "Iteration 20, loss = 0.32592510\n",
      "Iteration 21, loss = 0.31942046\n",
      "Iteration 22, loss = 0.29082931\n",
      "Iteration 23, loss = 0.29568169\n",
      "Iteration 24, loss = 0.30215217\n",
      "Iteration 25, loss = 0.29562651\n",
      "Iteration 26, loss = 0.30644527\n",
      "Iteration 27, loss = 0.29200685\n",
      "Iteration 28, loss = 0.30050760\n",
      "Iteration 29, loss = 0.28680879\n",
      "Iteration 30, loss = 0.27936613\n",
      "Iteration 31, loss = 0.29455102\n",
      "Iteration 32, loss = 0.28358411\n",
      "Iteration 33, loss = 0.27456133\n",
      "Iteration 34, loss = 0.27873207\n",
      "Iteration 35, loss = 0.29094966\n",
      "Iteration 36, loss = 0.28498096\n",
      "Iteration 37, loss = 0.28406208\n",
      "Iteration 38, loss = 0.29751181\n",
      "Iteration 39, loss = 0.27387838\n",
      "Iteration 40, loss = 0.28445138\n",
      "Iteration 41, loss = 0.28204166\n",
      "Iteration 42, loss = 0.27344077\n",
      "Iteration 43, loss = 0.28147335\n",
      "Iteration 44, loss = 0.28697449\n",
      "Iteration 45, loss = 0.28020272\n",
      "Iteration 46, loss = 0.28472694\n",
      "Iteration 47, loss = 0.29006092\n",
      "Iteration 48, loss = 0.28493134\n",
      "Iteration 49, loss = 0.28207002\n",
      "Iteration 50, loss = 0.26908924\n",
      "Iteration 51, loss = 0.27693988\n",
      "Iteration 52, loss = 0.26722338\n",
      "Iteration 53, loss = 0.27586417\n",
      "Iteration 54, loss = 0.27420734\n",
      "Iteration 55, loss = 0.26905369\n",
      "Iteration 56, loss = 0.27216774\n",
      "Iteration 57, loss = 0.30027229\n",
      "Iteration 58, loss = 0.28713191\n",
      "Iteration 59, loss = 0.28089439\n",
      "Iteration 60, loss = 0.29629246\n",
      "Iteration 61, loss = 0.27814092\n",
      "Iteration 62, loss = 0.31247171\n",
      "Iteration 63, loss = 0.32421701\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.68665274\n",
      "Iteration 2, loss = 3.89114516\n",
      "Iteration 3, loss = 2.22759245\n",
      "Iteration 4, loss = 0.77720727\n",
      "Iteration 5, loss = 0.54876126\n",
      "Iteration 6, loss = 0.52227800\n",
      "Iteration 7, loss = 0.43890457\n",
      "Iteration 8, loss = 0.43432380\n",
      "Iteration 9, loss = 0.42406803\n",
      "Iteration 10, loss = 0.41877661\n",
      "Iteration 11, loss = 0.43140491\n",
      "Iteration 12, loss = 0.42316950\n",
      "Iteration 13, loss = 0.40203399\n",
      "Iteration 14, loss = 0.40307887\n",
      "Iteration 15, loss = 0.43665594\n",
      "Iteration 16, loss = 0.40266738\n",
      "Iteration 17, loss = 0.38802340\n",
      "Iteration 18, loss = 0.40108343\n",
      "Iteration 19, loss = 0.38483767\n",
      "Iteration 20, loss = 0.33089557\n",
      "Iteration 21, loss = 0.38923103\n",
      "Iteration 22, loss = 0.36196799\n",
      "Iteration 23, loss = 0.34387786\n",
      "Iteration 24, loss = 0.34435720\n",
      "Iteration 25, loss = 0.35019657\n",
      "Iteration 26, loss = 0.35432173\n",
      "Iteration 27, loss = 0.37415821\n",
      "Iteration 28, loss = 0.37493382\n",
      "Iteration 29, loss = 0.38185301\n",
      "Iteration 30, loss = 0.41033261\n",
      "Iteration 31, loss = 0.39827739\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.10930715\n",
      "Iteration 2, loss = 4.36821206\n",
      "Iteration 3, loss = 4.39131603\n",
      "Iteration 4, loss = 3.79918226\n",
      "Iteration 5, loss = 3.66638127\n",
      "Iteration 6, loss = 4.39996863\n",
      "Iteration 7, loss = 4.35359181\n",
      "Iteration 8, loss = 4.01722371\n",
      "Iteration 9, loss = 3.63590415\n",
      "Iteration 10, loss = 3.91636384\n",
      "Iteration 11, loss = 3.76894974\n",
      "Iteration 12, loss = 3.67037918\n",
      "Iteration 13, loss = 3.66821195\n",
      "Iteration 14, loss = 3.83913512\n",
      "Iteration 15, loss = 4.01906860\n",
      "Iteration 16, loss = 3.82959755\n",
      "Iteration 17, loss = 3.36431140\n",
      "Iteration 18, loss = 3.81678926\n",
      "Iteration 19, loss = 3.92410408\n",
      "Iteration 20, loss = 3.73265039\n",
      "Iteration 21, loss = 3.71394174\n",
      "Iteration 22, loss = 3.59675965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.28042078\n",
      "Iteration 21, loss = 0.27569442\n",
      "Iteration 22, loss = 0.27933167\n",
      "Iteration 23, loss = 0.27792424\n",
      "Iteration 24, loss = 0.27968770\n",
      "Iteration 25, loss = 0.27708390\n",
      "Iteration 26, loss = 0.28308456\n",
      "Iteration 27, loss = 0.28120179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.26128764\n",
      "Iteration 2, loss = 1.03597093\n",
      "Iteration 3, loss = 0.63149425\n",
      "Iteration 4, loss = 0.59722453\n",
      "Iteration 5, loss = 0.59435127\n",
      "Iteration 6, loss = 0.58778426\n",
      "Iteration 7, loss = 0.58463501\n",
      "Iteration 8, loss = 0.56724102\n",
      "Iteration 9, loss = 0.57630242\n",
      "Iteration 10, loss = 0.57626226\n",
      "Iteration 11, loss = 0.55846420\n",
      "Iteration 12, loss = 0.54905214\n",
      "Iteration 13, loss = 0.56347716\n",
      "Iteration 14, loss = 0.56043639\n",
      "Iteration 15, loss = 0.46378976\n",
      "Iteration 16, loss = 0.41582934\n",
      "Iteration 17, loss = 0.41831270\n",
      "Iteration 18, loss = 0.39833979\n",
      "Iteration 19, loss = 0.39844558\n",
      "Iteration 20, loss = 0.42383444\n",
      "Iteration 21, loss = 0.44510972\n",
      "Iteration 22, loss = 0.50244599\n",
      "Iteration 23, loss = 0.38534115\n",
      "Iteration 24, loss = 0.40304438\n",
      "Iteration 25, loss = 0.39241295\n",
      "Iteration 26, loss = 0.46135475\n",
      "Iteration 27, loss = 0.39051306\n",
      "Iteration 28, loss = 0.37279027\n",
      "Iteration 29, loss = 0.37221762\n",
      "Iteration 30, loss = 0.36212196\n",
      "Iteration 31, loss = 0.37824355\n",
      "Iteration 32, loss = 0.38944722\n",
      "Iteration 33, loss = 0.36154203\n",
      "Iteration 34, loss = 0.36825157\n",
      "Iteration 35, loss = 0.37821589\n",
      "Iteration 36, loss = 0.35497234\n",
      "Iteration 37, loss = 0.35888621\n",
      "Iteration 38, loss = 0.39704523\n",
      "Iteration 39, loss = 0.34288308\n",
      "Iteration 40, loss = 0.47014984\n",
      "Iteration 41, loss = 0.54898191\n",
      "Iteration 42, loss = 0.54605685\n",
      "Iteration 43, loss = 0.42687051\n",
      "Iteration 44, loss = 0.36358539\n",
      "Iteration 45, loss = 0.36888797\n",
      "Iteration 46, loss = 0.35386674\n",
      "Iteration 47, loss = 0.40136121\n",
      "Iteration 48, loss = 0.38024464\n",
      "Iteration 49, loss = 0.34901344\n",
      "Iteration 50, loss = 0.35513469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.09942500\n",
      "Iteration 2, loss = 2.70574001\n",
      "Iteration 3, loss = 0.62048009\n",
      "Iteration 4, loss = 0.43295366\n",
      "Iteration 5, loss = 0.36098918\n",
      "Iteration 6, loss = 0.42314021\n",
      "Iteration 7, loss = 0.37728536\n",
      "Iteration 8, loss = 0.38944177\n",
      "Iteration 9, loss = 0.36988345\n",
      "Iteration 10, loss = 0.39445837\n",
      "Iteration 11, loss = 0.37833830\n",
      "Iteration 12, loss = 0.41667994\n",
      "Iteration 13, loss = 0.40590750\n",
      "Iteration 14, loss = 0.39924996\n",
      "Iteration 15, loss = 0.37488251\n",
      "Iteration 16, loss = 0.40506877\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.79314259\n",
      "Iteration 2, loss = 4.41274373\n",
      "Iteration 3, loss = 3.83466502\n",
      "Iteration 4, loss = 4.03073543\n",
      "Iteration 5, loss = 3.82297806\n",
      "Iteration 6, loss = 3.97197669\n",
      "Iteration 7, loss = 3.77466575\n",
      "Iteration 8, loss = 3.83693643\n",
      "Iteration 9, loss = 3.51879992\n",
      "Iteration 10, loss = 3.61812599\n",
      "Iteration 11, loss = 3.45569295\n",
      "Iteration 12, loss = 3.58472828\n",
      "Iteration 13, loss = 3.46007027\n",
      "Iteration 14, loss = 3.71769804\n",
      "Iteration 15, loss = 3.58305320\n",
      "Iteration 16, loss = 3.94872858\n",
      "Iteration 17, loss = 3.15400239\n",
      "Iteration 18, loss = 3.78860536\n",
      "Iteration 19, loss = 3.74534977\n",
      "Iteration 20, loss = 3.57607956\n",
      "Iteration 21, loss = 3.87954875\n",
      "Iteration 22, loss = 3.10683174\n",
      "Iteration 23, loss = 3.33360521\n",
      "Iteration 24, loss = 3.33762608\n",
      "Iteration 25, loss = 3.53083842\n",
      "Iteration 26, loss = 3.19636200\n",
      "Iteration 27, loss = 3.17241434\n",
      "Iteration 28, loss = 3.19516814\n",
      "Iteration 29, loss = 3.37906069\n",
      "Iteration 30, loss = 3.15244447\n",
      "Iteration 31, loss = 3.71087611\n",
      "Iteration 32, loss = 3.72093389\n",
      "Iteration 33, loss = 3.50986623\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.17825869\n",
      "Iteration 2, loss = 0.62578469\n",
      "Iteration 3, loss = 0.50415622\n",
      "Iteration 4, loss = 0.49180423\n",
      "Iteration 5, loss = 0.49744428\n",
      "Iteration 6, loss = 0.41131888\n",
      "Iteration 7, loss = 0.37701041\n",
      "Iteration 8, loss = 0.39525768\n",
      "Iteration 9, loss = 0.36269040\n",
      "Iteration 10, loss = 0.37889527\n",
      "Iteration 11, loss = 0.38749152\n",
      "Iteration 12, loss = 0.38215274\n",
      "Iteration 13, loss = 0.44328627\n",
      "Iteration 14, loss = 0.36939454\n",
      "Iteration 15, loss = 0.35285661\n",
      "Iteration 16, loss = 0.35394960\n",
      "Iteration 17, loss = 0.36774084\n",
      "Iteration 18, loss = 0.40975432\n",
      "Iteration 19, loss = 0.38528926\n",
      "Iteration 20, loss = 0.40859657\n",
      "Iteration 21, loss = 0.36343969\n",
      "Iteration 22, loss = 0.36786118\n",
      "Iteration 23, loss = 0.36525911\n",
      "Iteration 24, loss = 0.37132325\n",
      "Iteration 25, loss = 0.36782948\n",
      "Iteration 26, loss = 0.34797754\n",
      "Iteration 27, loss = 0.34616891\n",
      "Iteration 28, loss = 0.36118271\n",
      "Iteration 29, loss = 0.33362927\n",
      "Iteration 30, loss = 0.31982255\n",
      "Iteration 31, loss = 0.32403086\n",
      "Iteration 32, loss = 0.32026498\n",
      "Iteration 33, loss = 0.33572858\n",
      "Iteration 34, loss = 0.32860074\n",
      "Iteration 35, loss = 0.32944647\n",
      "Iteration 36, loss = 0.32618801\n",
      "Iteration 37, loss = 0.32182157\n",
      "Iteration 38, loss = 0.34863975\n",
      "Iteration 39, loss = 0.31818420\n",
      "Iteration 40, loss = 0.31201336\n",
      "Iteration 41, loss = 0.33214920\n",
      "Iteration 42, loss = 0.32457873\n",
      "Iteration 43, loss = 0.32129459\n",
      "Iteration 44, loss = 0.31903197\n",
      "Iteration 45, loss = 0.33076629\n",
      "Iteration 46, loss = 0.31008246\n",
      "Iteration 47, loss = 0.31113027\n",
      "Iteration 48, loss = 0.30891656\n",
      "Iteration 49, loss = 0.29685750\n",
      "Iteration 50, loss = 0.30426244\n",
      "Iteration 51, loss = 0.31767059\n",
      "Iteration 52, loss = 0.30287546\n",
      "Iteration 53, loss = 0.30247036\n",
      "Iteration 54, loss = 0.29808782\n",
      "Iteration 55, loss = 0.31894005\n",
      "Iteration 56, loss = 0.29961985\n",
      "Iteration 57, loss = 0.29422164\n",
      "Iteration 58, loss = 0.27938241\n",
      "Iteration 59, loss = 0.30487597\n",
      "Iteration 60, loss = 0.30500330\n",
      "Iteration 61, loss = 0.27341541\n",
      "Iteration 62, loss = 0.27191543\n",
      "Iteration 63, loss = 0.27543449\n",
      "Iteration 64, loss = 0.34630695\n",
      "Iteration 65, loss = 0.30044369\n",
      "Iteration 66, loss = 0.27695412\n",
      "Iteration 67, loss = 0.27007211\n",
      "Iteration 68, loss = 0.28086233\n",
      "Iteration 69, loss = 0.26821375\n",
      "Iteration 70, loss = 0.26104198\n",
      "Iteration 71, loss = 0.27116898\n",
      "Iteration 72, loss = 0.26487204\n",
      "Iteration 73, loss = 0.26950214\n",
      "Iteration 74, loss = 0.28040846\n",
      "Iteration 75, loss = 0.30254775\n",
      "Iteration 76, loss = 0.26908930\n",
      "Iteration 77, loss = 0.27809652\n",
      "Iteration 78, loss = 0.27108279\n",
      "Iteration 79, loss = 0.27550834\n",
      "Iteration 80, loss = 0.26680365\n",
      "Iteration 81, loss = 0.30900032\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.64656992\n",
      "Iteration 2, loss = 2.85644078\n",
      "Iteration 3, loss = 0.56463944\n",
      "Iteration 4, loss = 0.38881809\n",
      "Iteration 5, loss = 0.41137671\n",
      "Iteration 6, loss = 0.44509022\n",
      "Iteration 7, loss = 0.44822282\n",
      "Iteration 8, loss = 0.42628329\n",
      "Iteration 9, loss = 0.44315188\n",
      "Iteration 10, loss = 0.44267914\n",
      "Iteration 11, loss = 0.45026918\n",
      "Iteration 12, loss = 0.44896291\n",
      "Iteration 13, loss = 0.43305744\n",
      "Iteration 14, loss = 0.41699642\n",
      "Iteration 15, loss = 0.38998699\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.13141820\n",
      "Iteration 2, loss = 4.65737259\n",
      "Iteration 3, loss = 4.15162905\n",
      "Iteration 4, loss = 5.00824936\n",
      "Iteration 5, loss = 4.59070631\n",
      "Iteration 6, loss = 4.10651095\n",
      "Iteration 7, loss = 4.36415005\n",
      "Iteration 8, loss = 3.94395569\n",
      "Iteration 9, loss = 3.86260786\n",
      "Iteration 10, loss = 3.55305842\n",
      "Iteration 11, loss = 4.19611880\n",
      "Iteration 12, loss = 4.01240149\n",
      "Iteration 13, loss = 3.45597005\n",
      "Iteration 14, loss = 3.80199982\n",
      "Iteration 15, loss = 3.68434207\n",
      "Iteration 16, loss = 4.07876279\n",
      "Iteration 17, loss = 3.86092840\n",
      "Iteration 18, loss = 3.87733321\n",
      "Iteration 19, loss = 3.55352376\n",
      "Iteration 20, loss = 3.96036538\n",
      "Iteration 21, loss = 3.70642126\n",
      "Iteration 22, loss = 3.65447851\n",
      "Iteration 23, loss = 3.27971356\n",
      "Iteration 24, loss = 3.11963025\n",
      "Iteration 25, loss = 3.23969687\n",
      "Iteration 26, loss = 3.32286648\n",
      "Iteration 27, loss = 3.27228271\n",
      "Iteration 28, loss = 3.66950440\n",
      "Iteration 29, loss = 3.17762340\n",
      "Iteration 30, loss = 3.05375218\n",
      "Iteration 31, loss = 2.92791251\n",
      "Iteration 32, loss = 3.44660393\n",
      "Iteration 33, loss = 3.15442891\n",
      "Iteration 34, loss = 3.28159611\n",
      "Iteration 35, loss = 3.51574597\n",
      "Iteration 36, loss = 3.27738230\n",
      "Iteration 37, loss = 3.14180988\n",
      "Iteration 12, loss = 0.33843686\n",
      "Iteration 13, loss = 0.33073238\n",
      "Iteration 14, loss = 0.34857813\n",
      "Iteration 15, loss = 0.32466532\n",
      "Iteration 16, loss = 0.31929802\n",
      "Iteration 17, loss = 0.30489573\n",
      "Iteration 18, loss = 0.30278815\n",
      "Iteration 19, loss = 0.30386948\n",
      "Iteration 20, loss = 0.31009412\n",
      "Iteration 21, loss = 0.29126980\n",
      "Iteration 22, loss = 0.29962275\n",
      "Iteration 23, loss = 0.29676968\n",
      "Iteration 24, loss = 0.28835326\n",
      "Iteration 25, loss = 0.29539973\n",
      "Iteration 26, loss = 0.30785738\n",
      "Iteration 27, loss = 0.29848506\n",
      "Iteration 28, loss = 0.28663958\n",
      "Iteration 29, loss = 0.29018834\n",
      "Iteration 30, loss = 0.28354246\n",
      "Iteration 31, loss = 0.28385747\n",
      "Iteration 32, loss = 0.28169338\n",
      "Iteration 33, loss = 0.27131886\n",
      "Iteration 34, loss = 0.27636691\n",
      "Iteration 35, loss = 0.29458031\n",
      "Iteration 36, loss = 0.28171488\n",
      "Iteration 37, loss = 0.27392482\n",
      "Iteration 38, loss = 0.27618959\n",
      "Iteration 39, loss = 0.26012561\n",
      "Iteration 40, loss = 0.26886888\n",
      "Iteration 41, loss = 0.26996396\n",
      "Iteration 42, loss = 0.27737112\n",
      "Iteration 43, loss = 0.26490474\n",
      "Iteration 44, loss = 0.27714178\n",
      "Iteration 45, loss = 0.26608518\n",
      "Iteration 46, loss = 0.31660378\n",
      "Iteration 47, loss = 0.31589898\n",
      "Iteration 48, loss = 0.29907910\n",
      "Iteration 49, loss = 0.29248184\n",
      "Iteration 50, loss = 0.27749405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.57966557\n",
      "Iteration 2, loss = 3.19750499\n",
      "Iteration 3, loss = 1.89897742\n",
      "Iteration 4, loss = 0.45881206\n",
      "Iteration 5, loss = 0.41815436\n",
      "Iteration 6, loss = 0.37998628\n",
      "Iteration 7, loss = 0.35688794\n",
      "Iteration 8, loss = 0.37105300\n",
      "Iteration 9, loss = 0.36066253\n",
      "Iteration 10, loss = 0.36604555\n",
      "Iteration 11, loss = 0.38309959\n",
      "Iteration 12, loss = 0.36295484\n",
      "Iteration 13, loss = 0.34017160\n",
      "Iteration 14, loss = 0.35272253\n",
      "Iteration 15, loss = 0.34061423\n",
      "Iteration 16, loss = 0.33001668\n",
      "Iteration 17, loss = 0.32292533\n",
      "Iteration 18, loss = 0.32856059\n",
      "Iteration 19, loss = 0.32916748\n",
      "Iteration 20, loss = 0.32870208\n",
      "Iteration 21, loss = 0.31851248\n",
      "Iteration 22, loss = 0.30788555\n",
      "Iteration 23, loss = 0.31577360\n",
      "Iteration 24, loss = 0.29546881\n",
      "Iteration 25, loss = 0.29685307\n",
      "Iteration 26, loss = 0.30363836\n",
      "Iteration 27, loss = 0.29798555\n",
      "Iteration 28, loss = 0.30986874\n",
      "Iteration 29, loss = 0.30019612\n",
      "Iteration 30, loss = 0.30620369\n",
      "Iteration 31, loss = 0.31120203\n",
      "Iteration 32, loss = 0.32033342\n",
      "Iteration 33, loss = 0.31602087\n",
      "Iteration 34, loss = 0.35109913\n",
      "Iteration 35, loss = 0.35595903\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.59549668\n",
      "Iteration 2, loss = 0.63276781\n",
      "Iteration 3, loss = 0.54859803\n",
      "Iteration 4, loss = 0.52967872\n",
      "Iteration 5, loss = 0.52633184\n",
      "Iteration 6, loss = 0.48850707\n",
      "Iteration 7, loss = 0.44820327\n",
      "Iteration 8, loss = 0.39805811\n",
      "Iteration 9, loss = 0.38317355\n",
      "Iteration 10, loss = 0.40259630\n",
      "Iteration 11, loss = 0.38459660\n",
      "Iteration 12, loss = 0.35909248\n",
      "Iteration 13, loss = 0.35604715\n",
      "Iteration 14, loss = 0.32144682\n",
      "Iteration 15, loss = 0.38436114\n",
      "Iteration 16, loss = 0.35814025\n",
      "Iteration 17, loss = 0.33235443\n",
      "Iteration 18, loss = 0.36034877\n",
      "Iteration 19, loss = 0.34682141\n",
      "Iteration 20, loss = 0.35356440\n",
      "Iteration 21, loss = 0.34509070\n",
      "Iteration 22, loss = 0.34017861\n",
      "Iteration 23, loss = 0.33304916\n",
      "Iteration 24, loss = 0.34683576\n",
      "Iteration 25, loss = 0.35108543\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.91398744\n",
      "Iteration 2, loss = 1.46154348\n",
      "Iteration 3, loss = 0.59383843\n",
      "Iteration 4, loss = 0.42093987\n",
      "Iteration 5, loss = 0.36803431\n",
      "Iteration 6, loss = 0.44155170\n",
      "Iteration 7, loss = 0.41007733\n",
      "Iteration 8, loss = 0.37638517\n",
      "Iteration 9, loss = 0.35262382\n",
      "Iteration 10, loss = 0.36787421\n",
      "Iteration 11, loss = 0.33500303\n",
      "Iteration 12, loss = 0.33842903\n",
      "Iteration 13, loss = 0.33053883\n",
      "Iteration 14, loss = 0.32952708\n",
      "Iteration 15, loss = 0.33429349\n",
      "Iteration 16, loss = 0.31785721\n",
      "Iteration 17, loss = 0.34269949\n",
      "Iteration 18, loss = 0.30667370\n",
      "Iteration 19, loss = 0.29521135\n",
      "Iteration 20, loss = 0.30052193\n",
      "Iteration 21, loss = 0.28906328\n",
      "Iteration 22, loss = 0.30537362\n",
      "Iteration 23, loss = 0.29297789\n",
      "Iteration 24, loss = 0.29106727\n",
      "Iteration 25, loss = 0.28649571\n",
      "Iteration 26, loss = 0.28832692\n",
      "Iteration 27, loss = 0.29004134\n",
      "Iteration 28, loss = 0.29531996\n",
      "Iteration 29, loss = 0.29543124\n",
      "Iteration 30, loss = 0.28360393\n",
      "Iteration 31, loss = 0.30283322\n",
      "Iteration 32, loss = 0.29026206\n",
      "Iteration 33, loss = 0.30274090\n",
      "Iteration 34, loss = 0.28939621\n",
      "Iteration 35, loss = 0.28651716\n",
      "Iteration 36, loss = 0.29638962\n",
      "Iteration 37, loss = 0.29102460\n",
      "Iteration 38, loss = 0.28734001\n",
      "Iteration 39, loss = 0.29076583\n",
      "Iteration 40, loss = 0.28300318\n",
      "Iteration 41, loss = 0.28598011\n",
      "Iteration 42, loss = 0.30050065\n",
      "Iteration 43, loss = 0.28886998\n",
      "Iteration 44, loss = 0.31064871\n",
      "Iteration 45, loss = 0.29268079\n",
      "Iteration 46, loss = 0.28967595\n",
      "Iteration 47, loss = 0.28855428\n",
      "Iteration 48, loss = 0.30144263\n",
      "Iteration 49, loss = 0.28588218\n",
      "Iteration 50, loss = 0.28689155\n",
      "Iteration 51, loss = 0.29238200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.21753662\n",
      "Iteration 2, loss = 1.49440650\n",
      "Iteration 3, loss = 0.41123100\n",
      "Iteration 4, loss = 0.36783297\n",
      "Iteration 5, loss = 0.39434308\n",
      "Iteration 6, loss = 0.41729132\n",
      "Iteration 7, loss = 0.39953083\n",
      "Iteration 8, loss = 0.41725396\n",
      "Iteration 9, loss = 0.43025526\n",
      "Iteration 10, loss = 0.38391686\n",
      "Iteration 11, loss = 0.39682283\n",
      "Iteration 12, loss = 0.37311052\n",
      "Iteration 13, loss = 0.37005024\n",
      "Iteration 14, loss = 0.38344750\n",
      "Iteration 15, loss = 0.36540997\n",
      "Iteration 16, loss = 0.36977932\n",
      "Iteration 17, loss = 0.36844927\n",
      "Iteration 18, loss = 0.34325364\n",
      "Iteration 19, loss = 0.36366560\n",
      "Iteration 20, loss = 0.35801090\n",
      "Iteration 21, loss = 0.34999234\n",
      "Iteration 22, loss = 0.32970164\n",
      "Iteration 23, loss = 0.32073256\n",
      "Iteration 24, loss = 0.32153505\n",
      "Iteration 25, loss = 0.34255540\n",
      "Iteration 26, loss = 0.35243624\n",
      "Iteration 27, loss = 0.34835685\n",
      "Iteration 28, loss = 0.37441150\n",
      "Iteration 29, loss = 0.35425253\n",
      "Iteration 30, loss = 0.36507331\n",
      "Iteration 31, loss = 0.36070357\n",
      "Iteration 32, loss = 0.34156110\n",
      "Iteration 33, loss = 0.36827163\n",
      "Iteration 34, loss = 0.35022545\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 0.28155430\n",
      "Iteration 49, loss = 0.27600521\n",
      "Iteration 50, loss = 0.26775796\n",
      "Iteration 51, loss = 0.26161241\n",
      "Iteration 52, loss = 0.28481188\n",
      "Iteration 53, loss = 0.29014417\n",
      "Iteration 54, loss = 0.27205913\n",
      "Iteration 55, loss = 0.26466583\n",
      "Iteration 56, loss = 0.25918039\n",
      "Iteration 57, loss = 0.26151096\n",
      "Iteration 58, loss = 0.26228186\n",
      "Iteration 59, loss = 0.26331525\n",
      "Iteration 60, loss = 0.25875009\n",
      "Iteration 61, loss = 0.25641850\n",
      "Iteration 62, loss = 0.26125741\n",
      "Iteration 63, loss = 0.26637000\n",
      "Iteration 64, loss = 0.26322905\n",
      "Iteration 65, loss = 0.25856132\n",
      "Iteration 66, loss = 0.25992137\n",
      "Iteration 67, loss = 0.26505743\n",
      "Iteration 68, loss = 0.25805234\n",
      "Iteration 69, loss = 0.26054176\n",
      "Iteration 70, loss = 0.25538632\n",
      "Iteration 71, loss = 0.26537055\n",
      "Iteration 72, loss = 0.25992874\n",
      "Iteration 73, loss = 0.26612860\n",
      "Iteration 74, loss = 0.25960155\n",
      "Iteration 75, loss = 0.25761366\n",
      "Iteration 76, loss = 0.25550727\n",
      "Iteration 77, loss = 0.26007770\n",
      "Iteration 78, loss = 0.26051240\n",
      "Iteration 79, loss = 0.25833742\n",
      "Iteration 80, loss = 0.26243087\n",
      "Iteration 81, loss = 0.25738038\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.79314259\n",
      "Iteration 2, loss = 4.41274373\n",
      "Iteration 3, loss = 3.83466502\n",
      "Iteration 4, loss = 4.03073543\n",
      "Iteration 5, loss = 3.82297806\n",
      "Iteration 6, loss = 3.97197669\n",
      "Iteration 7, loss = 3.77466575\n",
      "Iteration 8, loss = 3.83693643\n",
      "Iteration 9, loss = 3.51879992\n",
      "Iteration 10, loss = 3.61812599\n",
      "Iteration 11, loss = 3.45569295\n",
      "Iteration 12, loss = 3.58472828\n",
      "Iteration 13, loss = 3.46007027\n",
      "Iteration 14, loss = 3.71769804\n",
      "Iteration 15, loss = 3.58305320\n",
      "Iteration 16, loss = 3.94872858\n",
      "Iteration 17, loss = 3.15400239\n",
      "Iteration 18, loss = 3.78860536\n",
      "Iteration 19, loss = 3.74534977\n",
      "Iteration 20, loss = 3.57607956\n",
      "Iteration 21, loss = 3.87954875\n",
      "Iteration 22, loss = 3.10683174\n",
      "Iteration 23, loss = 3.33360521\n",
      "Iteration 24, loss = 3.33762608\n",
      "Iteration 25, loss = 3.53083842\n",
      "Iteration 26, loss = 3.19636200\n",
      "Iteration 27, loss = 3.17241434\n",
      "Iteration 28, loss = 3.19516814\n",
      "Iteration 29, loss = 3.37906069\n",
      "Iteration 30, loss = 3.15244447\n",
      "Iteration 31, loss = 3.71087611\n",
      "Iteration 32, loss = 3.72093389\n",
      "Iteration 33, loss = 3.50986623\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.91398744\n",
      "Iteration 2, loss = 1.46154348\n",
      "Iteration 3, loss = 0.59383843\n",
      "Iteration 4, loss = 0.42093987\n",
      "Iteration 5, loss = 0.36803431\n",
      "Iteration 6, loss = 0.44155170\n",
      "Iteration 7, loss = 0.41007733\n",
      "Iteration 8, loss = 0.37638517\n",
      "Iteration 9, loss = 0.35262382\n",
      "Iteration 10, loss = 0.36787421\n",
      "Iteration 11, loss = 0.33500303\n",
      "Iteration 12, loss = 0.33842903\n",
      "Iteration 13, loss = 0.33053883\n",
      "Iteration 14, loss = 0.32952708\n",
      "Iteration 15, loss = 0.33429349\n",
      "Iteration 16, loss = 0.31785721\n",
      "Iteration 17, loss = 0.34269949\n",
      "Iteration 18, loss = 0.30667370\n",
      "Iteration 19, loss = 0.29521135\n",
      "Iteration 20, loss = 0.30052193\n",
      "Iteration 21, loss = 0.28906328\n",
      "Iteration 22, loss = 0.30537362\n",
      "Iteration 23, loss = 0.29297789\n",
      "Iteration 24, loss = 0.29106727\n",
      "Iteration 25, loss = 0.28649571\n",
      "Iteration 26, loss = 0.28832692\n",
      "Iteration 27, loss = 0.29004134\n",
      "Iteration 28, loss = 0.29531996\n",
      "Iteration 29, loss = 0.29543124\n",
      "Iteration 30, loss = 0.28360393\n",
      "Iteration 31, loss = 0.30283322\n",
      "Iteration 32, loss = 0.29026206\n",
      "Iteration 33, loss = 0.30274090\n",
      "Iteration 34, loss = 0.28939621\n",
      "Iteration 35, loss = 0.28651716\n",
      "Iteration 36, loss = 0.29638962\n",
      "Iteration 37, loss = 0.29102460\n",
      "Iteration 38, loss = 0.28734001\n",
      "Iteration 39, loss = 0.29076583\n",
      "Iteration 40, loss = 0.28300318\n",
      "Iteration 41, loss = 0.28598011\n",
      "Iteration 42, loss = 0.30050065\n",
      "Iteration 43, loss = 0.28886998\n",
      "Iteration 44, loss = 0.31064871\n",
      "Iteration 45, loss = 0.29268079\n",
      "Iteration 46, loss = 0.28967595\n",
      "Iteration 47, loss = 0.28855428\n",
      "Iteration 48, loss = 0.30144263\n",
      "Iteration 49, loss = 0.28588218\n",
      "Iteration 50, loss = 0.28689155\n",
      "Iteration 51, loss = 0.29238200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.42016330\n",
      "Iteration 2, loss = 2.10049625\n",
      "Iteration 3, loss = 0.57970451\n",
      "Iteration 4, loss = 0.48060755\n",
      "Iteration 5, loss = 0.42975520\n",
      "Iteration 6, loss = 0.44813971\n",
      "Iteration 7, loss = 0.40906134\n",
      "Iteration 8, loss = 0.43798888\n",
      "Iteration 9, loss = 0.43839032\n",
      "Iteration 10, loss = 0.43532763\n",
      "Iteration 11, loss = 0.43562327\n",
      "Iteration 12, loss = 0.41781084\n",
      "Iteration 13, loss = 0.42620083\n",
      "Iteration 14, loss = 0.44178357\n",
      "Iteration 15, loss = 0.41138721\n",
      "Iteration 16, loss = 0.40683153\n",
      "Iteration 17, loss = 0.39917287\n",
      "Iteration 18, loss = 0.38283113\n",
      "Iteration 19, loss = 0.39567180\n",
      "Iteration 20, loss = 0.36296543\n",
      "Iteration 21, loss = 0.36233920\n",
      "Iteration 22, loss = 0.36649478\n",
      "Iteration 23, loss = 0.35447110\n",
      "Iteration 24, loss = 0.34446977\n",
      "Iteration 25, loss = 0.34759664\n",
      "Iteration 26, loss = 0.35501177\n",
      "Iteration 27, loss = 0.33202787\n",
      "Iteration 28, loss = 0.36435560\n",
      "Iteration 29, loss = 0.34332833\n",
      "Iteration 30, loss = 0.31531000\n",
      "Iteration 31, loss = 0.35045442\n",
      "Iteration 32, loss = 0.34509376\n",
      "Iteration 33, loss = 0.34103446\n",
      "Iteration 34, loss = 0.34742211\n",
      "Iteration 35, loss = 0.35069711\n",
      "Iteration 36, loss = 0.35869847\n",
      "Iteration 37, loss = 0.31822824\n",
      "Iteration 38, loss = 0.34400329\n",
      "Iteration 39, loss = 0.34895976\n",
      "Iteration 40, loss = 0.36190276\n",
      "Iteration 41, loss = 0.36515641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.30300115\n",
      "Iteration 42, loss = 0.31049387\n",
      "Iteration 43, loss = 0.31248265\n",
      "Iteration 44, loss = 0.31201241\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.20899925\n",
      "Iteration 2, loss = 0.65856363\n",
      "Iteration 3, loss = 0.48165801\n",
      "Iteration 4, loss = 0.46737801\n",
      "Iteration 5, loss = 0.48225891\n",
      "Iteration 6, loss = 0.51062058\n",
      "Iteration 7, loss = 0.42735684\n",
      "Iteration 8, loss = 0.47928670\n",
      "Iteration 9, loss = 0.41597692\n",
      "Iteration 10, loss = 0.37359306\n",
      "Iteration 11, loss = 0.40741364\n",
      "Iteration 12, loss = 0.38983454\n",
      "Iteration 13, loss = 0.39554396\n",
      "Iteration 14, loss = 0.43475731\n",
      "Iteration 15, loss = 0.40322389\n",
      "Iteration 16, loss = 0.38275737\n",
      "Iteration 17, loss = 0.36958765\n",
      "Iteration 18, loss = 0.36317078\n",
      "Iteration 19, loss = 0.34899029\n",
      "Iteration 20, loss = 0.38540267\n",
      "Iteration 21, loss = 0.36138041\n",
      "Iteration 22, loss = 0.34986616\n",
      "Iteration 23, loss = 0.35504899\n",
      "Iteration 24, loss = 0.34098256\n",
      "Iteration 25, loss = 0.31767887\n",
      "Iteration 26, loss = 0.35272722\n",
      "Iteration 27, loss = 0.32289229\n",
      "Iteration 28, loss = 0.33547339\n",
      "Iteration 29, loss = 0.32672522\n",
      "Iteration 30, loss = 0.32249429\n",
      "Iteration 31, loss = 0.32759010\n",
      "Iteration 32, loss = 0.31041436\n",
      "Iteration 33, loss = 0.33212879\n",
      "Iteration 34, loss = 0.35880059\n",
      "Iteration 35, loss = 0.32828649\n",
      "Iteration 36, loss = 0.33833941\n",
      "Iteration 37, loss = 0.32515363\n",
      "Iteration 38, loss = 0.33129814\n",
      "Iteration 39, loss = 0.34474944\n",
      "Iteration 40, loss = 0.33740601\n",
      "Iteration 41, loss = 0.34373288\n",
      "Iteration 42, loss = 0.37064532\n",
      "Iteration 43, loss = 0.33518871\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.64656992\n",
      "Iteration 2, loss = 2.85644078\n",
      "Iteration 3, loss = 0.56463944\n",
      "Iteration 4, loss = 0.38881809\n",
      "Iteration 5, loss = 0.41137671\n",
      "Iteration 6, loss = 0.44509022\n",
      "Iteration 7, loss = 0.44822282\n",
      "Iteration 8, loss = 0.42628329\n",
      "Iteration 9, loss = 0.44315188\n",
      "Iteration 10, loss = 0.44267914\n",
      "Iteration 11, loss = 0.45026918\n",
      "Iteration 12, loss = 0.44896291\n",
      "Iteration 13, loss = 0.43305744\n",
      "Iteration 14, loss = 0.41699642\n",
      "Iteration 15, loss = 0.38998699\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.17633273\n",
      "Iteration 2, loss = 2.90299961\n",
      "Iteration 3, loss = 1.32735331\n",
      "Iteration 4, loss = 0.42579510\n",
      "Iteration 5, loss = 0.41564170\n",
      "Iteration 6, loss = 0.39684540\n",
      "Iteration 7, loss = 0.42422992\n",
      "Iteration 8, loss = 0.41486771\n",
      "Iteration 9, loss = 0.43783232\n",
      "Iteration 10, loss = 0.41768865\n",
      "Iteration 11, loss = 0.43662762\n",
      "Iteration 12, loss = 0.39961253\n",
      "Iteration 13, loss = 0.41312635\n",
      "Iteration 14, loss = 0.43039274\n",
      "Iteration 15, loss = 0.41818409\n",
      "Iteration 16, loss = 0.41696041\n",
      "Iteration 17, loss = 0.45601176\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.51532897\n",
      "Iteration 5, loss = 0.45662602\n",
      "Iteration 6, loss = 0.44074748\n",
      "Iteration 7, loss = 0.38952303\n",
      "Iteration 8, loss = 0.35552700\n",
      "Iteration 9, loss = 0.37544397\n",
      "Iteration 10, loss = 0.36104833\n",
      "Iteration 11, loss = 0.34926239\n",
      "Iteration 12, loss = 0.35322118\n",
      "Iteration 13, loss = 0.34761807\n",
      "Iteration 14, loss = 0.37255946\n",
      "Iteration 15, loss = 0.33101116\n",
      "Iteration 16, loss = 0.31430193\n",
      "Iteration 17, loss = 0.32052047\n",
      "Iteration 18, loss = 0.30944213\n",
      "Iteration 19, loss = 0.30697693\n",
      "Iteration 20, loss = 0.33581713\n",
      "Iteration 21, loss = 0.33031619\n",
      "Iteration 22, loss = 0.32358899\n",
      "Iteration 23, loss = 0.31380988\n",
      "Iteration 24, loss = 0.30066387\n",
      "Iteration 25, loss = 0.28981257\n",
      "Iteration 26, loss = 0.31025298\n",
      "Iteration 27, loss = 0.29804076\n",
      "Iteration 28, loss = 0.29650502\n",
      "Iteration 29, loss = 0.29480945\n",
      "Iteration 30, loss = 0.28688976\n",
      "Iteration 31, loss = 0.28709372\n",
      "Iteration 32, loss = 0.27866164\n",
      "Iteration 33, loss = 0.27312111\n",
      "Iteration 34, loss = 0.31659889\n",
      "Iteration 35, loss = 0.29414948\n",
      "Iteration 36, loss = 0.28156818\n",
      "Iteration 37, loss = 0.28526314\n",
      "Iteration 38, loss = 0.31120116\n",
      "Iteration 39, loss = 0.28912902\n",
      "Iteration 40, loss = 0.27391718\n",
      "Iteration 41, loss = 0.28063642\n",
      "Iteration 42, loss = 0.28396534\n",
      "Iteration 43, loss = 0.27088316\n",
      "Iteration 44, loss = 0.27864633\n",
      "Iteration 45, loss = 0.27422212\n",
      "Iteration 46, loss = 0.28068271\n",
      "Iteration 47, loss = 0.28768431\n",
      "Iteration 48, loss = 0.28755169\n",
      "Iteration 49, loss = 0.29074401\n",
      "Iteration 50, loss = 0.26797507\n",
      "Iteration 51, loss = 0.28172250\n",
      "Iteration 52, loss = 0.27057843\n",
      "Iteration 53, loss = 0.26722382\n",
      "Iteration 54, loss = 0.26322614\n",
      "Iteration 55, loss = 0.26639811\n",
      "Iteration 56, loss = 0.26285160\n",
      "Iteration 57, loss = 0.26835541\n",
      "Iteration 58, loss = 0.25492556\n",
      "Iteration 59, loss = 0.26085761\n",
      "Iteration 60, loss = 0.26058261\n",
      "Iteration 61, loss = 0.26051932\n",
      "Iteration 62, loss = 0.26700199\n",
      "Iteration 63, loss = 0.27049569\n",
      "Iteration 64, loss = 0.25868281\n",
      "Iteration 65, loss = 0.26020009\n",
      "Iteration 66, loss = 0.26465336\n",
      "Iteration 67, loss = 0.26305809\n",
      "Iteration 68, loss = 0.25809537\n",
      "Iteration 69, loss = 0.25490357\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.38533449\n",
      "Iteration 2, loss = 0.78497038\n",
      "Iteration 3, loss = 0.40781880\n",
      "Iteration 4, loss = 0.36660022\n",
      "Iteration 5, loss = 0.37169451\n",
      "Iteration 6, loss = 0.42997659\n",
      "Iteration 7, loss = 0.40835345\n",
      "Iteration 8, loss = 0.41248069\n",
      "Iteration 9, loss = 0.41981221\n",
      "Iteration 10, loss = 0.41149161\n",
      "Iteration 11, loss = 0.39686428\n",
      "Iteration 12, loss = 0.39961607\n",
      "Iteration 13, loss = 0.39299466\n",
      "Iteration 14, loss = 0.38896295\n",
      "Iteration 15, loss = 0.39922846\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.09942500\n",
      "Iteration 2, loss = 2.70574001\n",
      "Iteration 3, loss = 0.62048009\n",
      "Iteration 4, loss = 0.43295366\n",
      "Iteration 5, loss = 0.36098918\n",
      "Iteration 6, loss = 0.42314021\n",
      "Iteration 7, loss = 0.37728536\n",
      "Iteration 8, loss = 0.38944177\n",
      "Iteration 9, loss = 0.36988345\n",
      "Iteration 10, loss = 0.39445837\n",
      "Iteration 11, loss = 0.37833830\n",
      "Iteration 12, loss = 0.41667994\n",
      "Iteration 13, loss = 0.40590750\n",
      "Iteration 14, loss = 0.39924996\n",
      "Iteration 15, loss = 0.37488251\n",
      "Iteration 16, loss = 0.40506877\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.93477381\n",
      "Iteration 2, loss = 0.45963750\n",
      "Iteration 3, loss = 0.43059504\n",
      "Iteration 4, loss = 0.42807264\n",
      "Iteration 5, loss = 0.43102386\n",
      "Iteration 6, loss = 0.44070639\n",
      "Iteration 7, loss = 0.43935673\n",
      "Iteration 8, loss = 0.39118774\n",
      "Iteration 9, loss = 0.36298326\n",
      "Iteration 10, loss = 0.35317001\n",
      "Iteration 11, loss = 0.34614297\n",
      "Iteration 12, loss = 0.33337969\n",
      "Iteration 13, loss = 0.32435777\n",
      "Iteration 14, loss = 0.35465160\n",
      "Iteration 15, loss = 0.34302293\n",
      "Iteration 16, loss = 0.30878997\n",
      "Iteration 17, loss = 0.31243914\n",
      "Iteration 18, loss = 0.31110067\n",
      "Iteration 19, loss = 0.31140267\n",
      "Iteration 20, loss = 0.32592510\n",
      "Iteration 21, loss = 0.31942046\n",
      "Iteration 22, loss = 0.29082931\n",
      "Iteration 23, loss = 0.29568169\n",
      "Iteration 24, loss = 0.30215217\n",
      "Iteration 25, loss = 0.29562651\n",
      "Iteration 26, loss = 0.30644527\n",
      "Iteration 27, loss = 0.29200685\n",
      "Iteration 28, loss = 0.30050760\n",
      "Iteration 29, loss = 0.28680879\n",
      "Iteration 30, loss = 0.27936613\n",
      "Iteration 31, loss = 0.29455102\n",
      "Iteration 32, loss = 0.28358411\n",
      "Iteration 33, loss = 0.27456133\n",
      "Iteration 34, loss = 0.27873207\n",
      "Iteration 35, loss = 0.29094966\n",
      "Iteration 36, loss = 0.28498096\n",
      "Iteration 37, loss = 0.28406208\n",
      "Iteration 38, loss = 0.29751181\n",
      "Iteration 39, loss = 0.27387838\n",
      "Iteration 40, loss = 0.28445138\n",
      "Iteration 41, loss = 0.28204166\n",
      "Iteration 42, loss = 0.27344077\n",
      "Iteration 43, loss = 0.28147335\n",
      "Iteration 44, loss = 0.28697449\n",
      "Iteration 45, loss = 0.28020272\n",
      "Iteration 46, loss = 0.28472694\n",
      "Iteration 47, loss = 0.29006092\n",
      "Iteration 48, loss = 0.28493134\n",
      "Iteration 49, loss = 0.28207002\n",
      "Iteration 50, loss = 0.26908924\n",
      "Iteration 51, loss = 0.27693988\n",
      "Iteration 52, loss = 0.26722338\n",
      "Iteration 53, loss = 0.27586417\n",
      "Iteration 54, loss = 0.27420734\n",
      "Iteration 55, loss = 0.26905369\n",
      "Iteration 56, loss = 0.27216774\n",
      "Iteration 57, loss = 0.30027229\n",
      "Iteration 58, loss = 0.28713191\n",
      "Iteration 59, loss = 0.28089439\n",
      "Iteration 60, loss = 0.29629246\n",
      "Iteration 61, loss = 0.27814092\n",
      "Iteration 62, loss = 0.31247171\n",
      "Iteration 63, loss = 0.32421701\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.42016330\n",
      "Iteration 2, loss = 2.10049625\n",
      "Iteration 3, loss = 0.57970451\n",
      "Iteration 4, loss = 0.48060755\n",
      "Iteration 5, loss = 0.42975520\n",
      "Iteration 6, loss = 0.44813971\n",
      "Iteration 7, loss = 0.40906134\n",
      "Iteration 8, loss = 0.43798888\n",
      "Iteration 9, loss = 0.43839032\n",
      "Iteration 10, loss = 0.43532763\n",
      "Iteration 11, loss = 0.43562327\n",
      "Iteration 12, loss = 0.41781084\n",
      "Iteration 13, loss = 0.42620083\n",
      "Iteration 14, loss = 0.44178357\n",
      "Iteration 15, loss = 0.41138721\n",
      "Iteration 16, loss = 0.40683153\n",
      "Iteration 17, loss = 0.39917287\n",
      "Iteration 18, loss = 0.38283113\n",
      "Iteration 19, loss = 0.39567180\n",
      "Iteration 20, loss = 0.36296543\n",
      "Iteration 21, loss = 0.36233920\n",
      "Iteration 22, loss = 0.36649478\n",
      "Iteration 23, loss = 0.35447110\n",
      "Iteration 24, loss = 0.34446977\n",
      "Iteration 25, loss = 0.34759664\n",
      "Iteration 26, loss = 0.35501177\n",
      "Iteration 27, loss = 0.33202787\n",
      "Iteration 28, loss = 0.36435560\n",
      "Iteration 29, loss = 0.34332833\n",
      "Iteration 30, loss = 0.31531000\n",
      "Iteration 31, loss = 0.35045442\n",
      "Iteration 32, loss = 0.34509376\n",
      "Iteration 33, loss = 0.34103446\n",
      "Iteration 34, loss = 0.34742211\n",
      "Iteration 35, loss = 0.35069711\n",
      "Iteration 36, loss = 0.35869847\n",
      "Iteration 37, loss = 0.31822824\n",
      "Iteration 38, loss = 0.34400329\n",
      "Iteration 39, loss = 0.34895976\n",
      "Iteration 40, loss = 0.36190276\n",
      "Iteration 41, loss = 0.36515641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.26722382\n",
      "Iteration 54, loss = 0.26322614\n",
      "Iteration 55, loss = 0.26639811\n",
      "Iteration 56, loss = 0.26285160\n",
      "Iteration 57, loss = 0.26835541\n",
      "Iteration 58, loss = 0.25492556\n",
      "Iteration 59, loss = 0.26085761\n",
      "Iteration 60, loss = 0.26058261\n",
      "Iteration 61, loss = 0.26051932\n",
      "Iteration 62, loss = 0.26700199\n",
      "Iteration 63, loss = 0.27049569\n",
      "Iteration 64, loss = 0.25868281\n",
      "Iteration 65, loss = 0.26020009\n",
      "Iteration 66, loss = 0.26465336\n",
      "Iteration 67, loss = 0.26305809\n",
      "Iteration 68, loss = 0.25809537\n",
      "Iteration 69, loss = 0.25490357\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.57966557\n",
      "Iteration 2, loss = 3.19750499\n",
      "Iteration 3, loss = 1.89897742\n",
      "Iteration 4, loss = 0.45881206\n",
      "Iteration 5, loss = 0.41815436\n",
      "Iteration 6, loss = 0.37998628\n",
      "Iteration 7, loss = 0.35688794\n",
      "Iteration 8, loss = 0.37105300\n",
      "Iteration 9, loss = 0.36066253\n",
      "Iteration 10, loss = 0.36604555\n",
      "Iteration 11, loss = 0.38309959\n",
      "Iteration 12, loss = 0.36295484\n",
      "Iteration 13, loss = 0.34017160\n",
      "Iteration 14, loss = 0.35272253\n",
      "Iteration 15, loss = 0.34061423\n",
      "Iteration 16, loss = 0.33001668\n",
      "Iteration 17, loss = 0.32292533\n",
      "Iteration 18, loss = 0.32856059\n",
      "Iteration 19, loss = 0.32916748\n",
      "Iteration 20, loss = 0.32870208\n",
      "Iteration 21, loss = 0.31851248\n",
      "Iteration 22, loss = 0.30788555\n",
      "Iteration 23, loss = 0.31577360\n",
      "Iteration 24, loss = 0.29546881\n",
      "Iteration 25, loss = 0.29685307\n",
      "Iteration 26, loss = 0.30363836\n",
      "Iteration 27, loss = 0.29798555\n",
      "Iteration 28, loss = 0.30986874\n",
      "Iteration 29, loss = 0.30019612\n",
      "Iteration 30, loss = 0.30620369\n",
      "Iteration 31, loss = 0.31120203\n",
      "Iteration 32, loss = 0.32033342\n",
      "Iteration 33, loss = 0.31602087\n",
      "Iteration 34, loss = 0.35109913\n",
      "Iteration 35, loss = 0.35595903\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.59549668\n",
      "Iteration 2, loss = 0.63276781\n",
      "Iteration 3, loss = 0.54859803\n",
      "Iteration 4, loss = 0.52967872\n",
      "Iteration 5, loss = 0.52633184\n",
      "Iteration 6, loss = 0.48850707\n",
      "Iteration 7, loss = 0.44820327\n",
      "Iteration 8, loss = 0.39805811\n",
      "Iteration 9, loss = 0.38317355\n",
      "Iteration 10, loss = 0.40259630\n",
      "Iteration 11, loss = 0.38459660\n",
      "Iteration 12, loss = 0.35909248\n",
      "Iteration 13, loss = 0.35604715\n",
      "Iteration 14, loss = 0.32144682\n",
      "Iteration 15, loss = 0.38436114\n",
      "Iteration 16, loss = 0.35814025\n",
      "Iteration 17, loss = 0.33235443\n",
      "Iteration 18, loss = 0.36034877\n",
      "Iteration 19, loss = 0.34682141\n",
      "Iteration 20, loss = 0.35356440\n",
      "Iteration 21, loss = 0.34509070\n",
      "Iteration 22, loss = 0.34017861\n",
      "Iteration 23, loss = 0.33304916\n",
      "Iteration 24, loss = 0.34683576\n",
      "Iteration 25, loss = 0.35108543\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.46538807\n",
      "Iteration 2, loss = 1.33071117\n",
      "Iteration 3, loss = 0.58969596\n",
      "Iteration 4, loss = 0.59870268\n",
      "Iteration 5, loss = 0.50863552\n",
      "Iteration 6, loss = 0.44253398\n",
      "Iteration 7, loss = 0.42469279\n",
      "Iteration 8, loss = 0.41601616\n",
      "Iteration 9, loss = 0.40065707\n",
      "Iteration 10, loss = 0.37452007\n",
      "Iteration 11, loss = 0.34852594\n",
      "Iteration 12, loss = 0.39387575\n",
      "Iteration 13, loss = 0.40470873\n",
      "Iteration 14, loss = 0.37239574\n",
      "Iteration 15, loss = 0.39015353\n",
      "Iteration 16, loss = 0.36535318\n",
      "Iteration 17, loss = 0.41607654\n",
      "Iteration 18, loss = 0.39264905\n",
      "Iteration 19, loss = 0.39533615\n",
      "Iteration 20, loss = 0.39668761\n",
      "Iteration 21, loss = 0.36462175\n",
      "Iteration 22, loss = 0.35300379\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.20899925\n",
      "Iteration 2, loss = 0.65856363\n",
      "Iteration 3, loss = 0.48165801\n",
      "Iteration 4, loss = 0.46737801\n",
      "Iteration 5, loss = 0.48225891\n",
      "Iteration 6, loss = 0.51062058\n",
      "Iteration 7, loss = 0.42735684\n",
      "Iteration 8, loss = 0.47928670\n",
      "Iteration 9, loss = 0.41597692\n",
      "Iteration 10, loss = 0.37359306\n",
      "Iteration 11, loss = 0.40741364\n",
      "Iteration 12, loss = 0.38983454\n",
      "Iteration 13, loss = 0.39554396\n",
      "Iteration 14, loss = 0.43475731\n",
      "Iteration 15, loss = 0.40322389\n",
      "Iteration 16, loss = 0.38275737\n",
      "Iteration 17, loss = 0.36958765\n",
      "Iteration 18, loss = 0.36317078\n",
      "Iteration 19, loss = 0.34899029\n",
      "Iteration 20, loss = 0.38540267\n",
      "Iteration 21, loss = 0.36138041\n",
      "Iteration 22, loss = 0.34986616\n",
      "Iteration 23, loss = 0.35504899\n",
      "Iteration 24, loss = 0.34098256\n",
      "Iteration 25, loss = 0.31767887\n",
      "Iteration 26, loss = 0.35272722\n",
      "Iteration 27, loss = 0.32289229\n",
      "Iteration 28, loss = 0.33547339\n",
      "Iteration 29, loss = 0.32672522\n",
      "Iteration 30, loss = 0.32249429\n",
      "Iteration 31, loss = 0.32759010\n",
      "Iteration 32, loss = 0.31041436\n",
      "Iteration 33, loss = 0.33212879\n",
      "Iteration 34, loss = 0.35880059\n",
      "Iteration 35, loss = 0.32828649\n",
      "Iteration 36, loss = 0.33833941\n",
      "Iteration 37, loss = 0.32515363\n",
      "Iteration 38, loss = 0.33129814\n",
      "Iteration 39, loss = 0.34474944\n",
      "Iteration 40, loss = 0.33740601\n",
      "Iteration 41, loss = 0.34373288\n",
      "Iteration 42, loss = 0.37064532\n",
      "Iteration 43, loss = 0.33518871\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.17633273\n",
      "Iteration 2, loss = 2.90299961\n",
      "Iteration 3, loss = 1.32735331\n",
      "Iteration 4, loss = 0.42579510\n",
      "Iteration 5, loss = 0.41564170\n",
      "Iteration 6, loss = 0.39684540\n",
      "Iteration 7, loss = 0.42422992\n",
      "Iteration 8, loss = 0.41486771\n",
      "Iteration 9, loss = 0.43783232\n",
      "Iteration 10, loss = 0.41768865\n",
      "Iteration 11, loss = 0.43662762\n",
      "Iteration 12, loss = 0.39961253\n",
      "Iteration 13, loss = 0.41312635\n",
      "Iteration 14, loss = 0.43039274\n",
      "Iteration 15, loss = 0.41818409\n",
      "Iteration 16, loss = 0.41696041\n",
      "Iteration 17, loss = 0.45601176\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.19590162\n",
      "Iteration 2, loss = 4.91209489\n",
      "Iteration 3, loss = 4.25464976\n",
      "Iteration 4, loss = 4.25657706\n",
      "Iteration 5, loss = 4.30405371\n",
      "Iteration 6, loss = 3.84573388\n",
      "Iteration 7, loss = 3.93858292\n",
      "Iteration 8, loss = 4.08247743\n",
      "Iteration 9, loss = 3.83390388\n",
      "Iteration 10, loss = 3.67889351\n",
      "Iteration 11, loss = 3.82252767\n",
      "Iteration 12, loss = 3.58430224\n",
      "Iteration 13, loss = 4.08572761\n",
      "Iteration 14, loss = 4.07822708\n",
      "Iteration 15, loss = 4.10690992\n",
      "Iteration 16, loss = 3.94561221\n",
      "Iteration 17, loss = 3.86299827\n",
      "Iteration 18, loss = 3.55328299\n",
      "Iteration 19, loss = 3.70575437\n",
      "Iteration 20, loss = 3.54216546\n",
      "Iteration 21, loss = 3.75291160\n",
      "Iteration 22, loss = 3.82372573\n",
      "Iteration 23, loss = 3.45524703\n",
      "Iteration 24, loss = 3.46854275\n",
      "Iteration 25, loss = 3.62264498\n",
      "Iteration 26, loss = 3.58029215\n",
      "Iteration 27, loss = 4.22127356\n",
      "Iteration 28, loss = 3.62517990\n",
      "Iteration 29, loss = 3.76363082\n",
      "Iteration 30, loss = 3.70451077\n",
      "Iteration 31, loss = 3.85279669\n",
      "Iteration 32, loss = 4.01545379\n",
      "Iteration 33, loss = 3.74784261\n",
      "Iteration 34, loss = 3.69917742\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23, loss = 0.28182723\n",
      "Iteration 24, loss = 0.27821578\n",
      "Iteration 25, loss = 0.28814125\n",
      "Iteration 26, loss = 0.28696574\n",
      "Iteration 27, loss = 0.27912478\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.54433163\n",
      "Iteration 2, loss = 1.23369870\n",
      "Iteration 3, loss = 0.54347917\n",
      "Iteration 4, loss = 0.46718929\n",
      "Iteration 5, loss = 0.44809008\n",
      "Iteration 6, loss = 0.40219987\n",
      "Iteration 7, loss = 0.43904978\n",
      "Iteration 8, loss = 0.36968434\n",
      "Iteration 9, loss = 0.35539988\n",
      "Iteration 10, loss = 0.33723355\n",
      "Iteration 11, loss = 0.37337154\n",
      "Iteration 12, loss = 0.32009673\n",
      "Iteration 13, loss = 0.36363730\n",
      "Iteration 14, loss = 0.32885513\n",
      "Iteration 15, loss = 0.33645709\n",
      "Iteration 16, loss = 0.33316510\n",
      "Iteration 17, loss = 0.35240865\n",
      "Iteration 18, loss = 0.33077383\n",
      "Iteration 19, loss = 0.34145935\n",
      "Iteration 20, loss = 0.34463674\n",
      "Iteration 21, loss = 0.34317202\n",
      "Iteration 22, loss = 0.32428196\n",
      "Iteration 23, loss = 0.30767372\n",
      "Iteration 24, loss = 0.32360772\n",
      "Iteration 25, loss = 0.31472858\n",
      "Iteration 26, loss = 0.31729762\n",
      "Iteration 27, loss = 0.30017170\n",
      "Iteration 28, loss = 0.32431325\n",
      "Iteration 29, loss = 0.31151224\n",
      "Iteration 30, loss = 0.32782614\n",
      "Iteration 31, loss = 0.32251985\n",
      "Iteration 32, loss = 0.30293319\n",
      "Iteration 33, loss = 0.30679752\n",
      "Iteration 34, loss = 0.30093440\n",
      "Iteration 35, loss = 0.37189336\n",
      "Iteration 36, loss = 0.31392622\n",
      "Iteration 37, loss = 0.32460557\n",
      "Iteration 38, loss = 0.31621157\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.03413256\n",
      "Iteration 2, loss = 3.07114095\n",
      "Iteration 3, loss = 1.11477156\n",
      "Iteration 4, loss = 0.44204779\n",
      "Iteration 5, loss = 0.45532293\n",
      "Iteration 6, loss = 0.40033971\n",
      "Iteration 7, loss = 0.38091723\n",
      "Iteration 8, loss = 0.39747437\n",
      "Iteration 9, loss = 0.40465199\n",
      "Iteration 10, loss = 0.41344020\n",
      "Iteration 11, loss = 0.38110640\n",
      "Iteration 12, loss = 0.39583711\n",
      "Iteration 13, loss = 0.36775032\n",
      "Iteration 14, loss = 0.39125929\n",
      "Iteration 15, loss = 0.36414286\n",
      "Iteration 16, loss = 0.36872219\n",
      "Iteration 17, loss = 0.36541256\n",
      "Iteration 18, loss = 0.36454400\n",
      "Iteration 19, loss = 0.37291661\n",
      "Iteration 20, loss = 0.35169187\n",
      "Iteration 21, loss = 0.41795901\n",
      "Iteration 22, loss = 0.32059130\n",
      "Iteration 23, loss = 0.31682976\n",
      "Iteration 24, loss = 0.32532298\n",
      "Iteration 25, loss = 0.33116416\n",
      "Iteration 26, loss = 0.31867744\n",
      "Iteration 27, loss = 0.29765014\n",
      "Iteration 28, loss = 0.32097079\n",
      "Iteration 29, loss = 0.31699989\n",
      "Iteration 30, loss = 0.31152578\n",
      "Iteration 31, loss = 0.30375591\n",
      "Iteration 32, loss = 0.30039964\n",
      "Iteration 33, loss = 0.29378106\n",
      "Iteration 34, loss = 0.30104505\n",
      "Iteration 35, loss = 0.31430391\n",
      "Iteration 36, loss = 0.32032709\n",
      "Iteration 37, loss = 0.29475830\n",
      "Iteration 38, loss = 0.30452927\n",
      "Iteration 39, loss = 0.32654990\n",
      "Iteration 40, loss = 0.31229484\n",
      "Iteration 41, loss = 0.30300115\n",
      "Iteration 42, loss = 0.31049387\n",
      "Iteration 43, loss = 0.31248265\n",
      "Iteration 44, loss = 0.31201241\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.46538807\n",
      "Iteration 2, loss = 1.33071117\n",
      "Iteration 3, loss = 0.58969596\n",
      "Iteration 4, loss = 0.59870268\n",
      "Iteration 5, loss = 0.50863552\n",
      "Iteration 6, loss = 0.44253398\n",
      "Iteration 7, loss = 0.42469279\n",
      "Iteration 8, loss = 0.41601616\n",
      "Iteration 9, loss = 0.40065707\n",
      "Iteration 10, loss = 0.37452007\n",
      "Iteration 11, loss = 0.34852594\n",
      "Iteration 12, loss = 0.39387575\n",
      "Iteration 13, loss = 0.40470873\n",
      "Iteration 14, loss = 0.37239574\n",
      "Iteration 15, loss = 0.39015353\n",
      "Iteration 16, loss = 0.36535318\n",
      "Iteration 17, loss = 0.41607654\n",
      "Iteration 18, loss = 0.39264905\n",
      "Iteration 19, loss = 0.39533615\n",
      "Iteration 20, loss = 0.39668761\n",
      "Iteration 21, loss = 0.36462175\n",
      "Iteration 22, loss = 0.35300379\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.21753662\n",
      "Iteration 2, loss = 1.49440650\n",
      "Iteration 3, loss = 0.41123100\n",
      "Iteration 4, loss = 0.36783297\n",
      "Iteration 5, loss = 0.39434308\n",
      "Iteration 6, loss = 0.41729132\n",
      "Iteration 7, loss = 0.39953083\n",
      "Iteration 8, loss = 0.41725396\n",
      "Iteration 9, loss = 0.43025526\n",
      "Iteration 10, loss = 0.38391686\n",
      "Iteration 11, loss = 0.39682283\n",
      "Iteration 12, loss = 0.37311052\n",
      "Iteration 13, loss = 0.37005024\n",
      "Iteration 14, loss = 0.38344750\n",
      "Iteration 15, loss = 0.36540997\n",
      "Iteration 16, loss = 0.36977932\n",
      "Iteration 17, loss = 0.36844927\n",
      "Iteration 18, loss = 0.34325364\n",
      "Iteration 19, loss = 0.36366560\n",
      "Iteration 20, loss = 0.35801090\n",
      "Iteration 21, loss = 0.34999234\n",
      "Iteration 22, loss = 0.32970164\n",
      "Iteration 23, loss = 0.32073256\n",
      "Iteration 24, loss = 0.32153505\n",
      "Iteration 25, loss = 0.34255540\n",
      "Iteration 26, loss = 0.35243624\n",
      "Iteration 27, loss = 0.34835685\n",
      "Iteration 28, loss = 0.37441150\n",
      "Iteration 29, loss = 0.35425253\n",
      "Iteration 30, loss = 0.36507331\n",
      "Iteration 31, loss = 0.36070357\n",
      "Iteration 32, loss = 0.34156110\n",
      "Iteration 33, loss = 0.36827163\n",
      "Iteration 34, loss = 0.35022545\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.19590162\n",
      "Iteration 2, loss = 4.91209489\n",
      "Iteration 3, loss = 4.25464976\n",
      "Iteration 4, loss = 4.25657706\n",
      "Iteration 5, loss = 4.30405371\n",
      "Iteration 6, loss = 3.84573388\n",
      "Iteration 7, loss = 3.93858292\n",
      "Iteration 8, loss = 4.08247743\n",
      "Iteration 9, loss = 3.83390388\n",
      "Iteration 10, loss = 3.67889351\n",
      "Iteration 11, loss = 3.82252767\n",
      "Iteration 12, loss = 3.58430224\n",
      "Iteration 13, loss = 4.08572761\n",
      "Iteration 14, loss = 4.07822708\n",
      "Iteration 15, loss = 4.10690992\n",
      "Iteration 16, loss = 3.94561221\n",
      "Iteration 17, loss = 3.86299827\n",
      "Iteration 18, loss = 3.55328299\n",
      "Iteration 19, loss = 3.70575437\n",
      "Iteration 20, loss = 3.54216546\n",
      "Iteration 21, loss = 3.75291160\n",
      "Iteration 22, loss = 3.82372573\n",
      "Iteration 23, loss = 3.45524703\n",
      "Iteration 24, loss = 3.46854275\n",
      "Iteration 25, loss = 3.62264498\n",
      "Iteration 26, loss = 3.58029215\n",
      "Iteration 27, loss = 4.22127356\n",
      "Iteration 28, loss = 3.62517990\n",
      "Iteration 29, loss = 3.76363082\n",
      "Iteration 30, loss = 3.70451077\n",
      "Iteration 31, loss = 3.85279669\n",
      "Iteration 32, loss = 4.01545379\n",
      "Iteration 33, loss = 3.74784261\n",
      "Iteration 34, loss = 3.69917742\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.26886888\n",
      "Iteration 41, loss = 0.26996396\n",
      "Iteration 42, loss = 0.27737112\n",
      "Iteration 43, loss = 0.26490474\n",
      "Iteration 44, loss = 0.27714178\n",
      "Iteration 45, loss = 0.26608518\n",
      "Iteration 46, loss = 0.31660378\n",
      "Iteration 47, loss = 0.31589898\n",
      "Iteration 48, loss = 0.29907910\n",
      "Iteration 49, loss = 0.29248184\n",
      "Iteration 50, loss = 0.27749405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.54433163\n",
      "Iteration 2, loss = 1.23369870\n",
      "Iteration 3, loss = 0.54347917\n",
      "Iteration 4, loss = 0.46718929\n",
      "Iteration 5, loss = 0.44809008\n",
      "Iteration 6, loss = 0.40219987\n",
      "Iteration 7, loss = 0.43904978\n",
      "Iteration 8, loss = 0.36968434\n",
      "Iteration 9, loss = 0.35539988\n",
      "Iteration 10, loss = 0.33723355\n",
      "Iteration 11, loss = 0.37337154\n",
      "Iteration 12, loss = 0.32009673\n",
      "Iteration 13, loss = 0.36363730\n",
      "Iteration 14, loss = 0.32885513\n",
      "Iteration 15, loss = 0.33645709\n",
      "Iteration 16, loss = 0.33316510\n",
      "Iteration 17, loss = 0.35240865\n",
      "Iteration 18, loss = 0.33077383\n",
      "Iteration 19, loss = 0.34145935\n",
      "Iteration 20, loss = 0.34463674\n",
      "Iteration 21, loss = 0.34317202\n",
      "Iteration 22, loss = 0.32428196\n",
      "Iteration 23, loss = 0.30767372\n",
      "Iteration 24, loss = 0.32360772\n",
      "Iteration 25, loss = 0.31472858\n",
      "Iteration 26, loss = 0.31729762\n",
      "Iteration 27, loss = 0.30017170\n",
      "Iteration 28, loss = 0.32431325\n",
      "Iteration 29, loss = 0.31151224\n",
      "Iteration 30, loss = 0.32782614\n",
      "Iteration 31, loss = 0.32251985\n",
      "Iteration 32, loss = 0.30293319\n",
      "Iteration 33, loss = 0.30679752\n",
      "Iteration 34, loss = 0.30093440\n",
      "Iteration 35, loss = 0.37189336\n",
      "Iteration 36, loss = 0.31392622\n",
      "Iteration 37, loss = 0.32460557\n",
      "Iteration 38, loss = 0.31621157\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.54253066\n",
      "Iteration 2, loss = 2.52914855\n",
      "Iteration 3, loss = 2.55054785\n",
      "Iteration 4, loss = 1.71393858\n",
      "Iteration 5, loss = 0.93363453\n",
      "Iteration 6, loss = 0.61990076\n",
      "Iteration 7, loss = 0.32450744\n",
      "Iteration 8, loss = 0.31591583\n",
      "Iteration 9, loss = 0.31176526\n",
      "Iteration 10, loss = 0.31547522\n",
      "Iteration 11, loss = 0.30345723\n",
      "Iteration 12, loss = 0.32107919\n",
      "Iteration 13, loss = 0.33835823\n",
      "Iteration 14, loss = 0.33754201\n",
      "Iteration 15, loss = 0.31552879\n",
      "Iteration 16, loss = 0.34924676\n",
      "Iteration 17, loss = 0.32793207\n",
      "Iteration 18, loss = 0.34031884\n",
      "Iteration 19, loss = 0.34067932\n",
      "Iteration 20, loss = 0.34898794\n",
      "Iteration 21, loss = 0.34937620\n",
      "Iteration 22, loss = 0.35436421\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.37398098\n",
      "Iteration 2, loss = 4.90374058\n",
      "Iteration 3, loss = 4.58748924\n",
      "Iteration 4, loss = 4.01410781\n",
      "Iteration 5, loss = 4.08404351\n",
      "Iteration 6, loss = 4.20448858\n",
      "Iteration 7, loss = 4.02262764\n",
      "Iteration 8, loss = 3.83087678\n",
      "Iteration 9, loss = 4.26451066\n",
      "Iteration 10, loss = 3.72117783\n",
      "Iteration 11, loss = 3.65182835\n",
      "Iteration 12, loss = 3.44922449\n",
      "Iteration 13, loss = 3.74572569\n",
      "Iteration 14, loss = 3.71573199\n",
      "Iteration 15, loss = 3.66143587\n",
      "Iteration 16, loss = 3.96159290\n",
      "Iteration 17, loss = 3.62269570\n",
      "Iteration 18, loss = 4.03616277\n",
      "Iteration 19, loss = 3.92312012\n",
      "Iteration 20, loss = 3.73454363\n",
      "Iteration 21, loss = 3.45361179\n",
      "Iteration 22, loss = 3.76848446\n",
      "Iteration 23, loss = 3.35642469\n",
      "Iteration 24, loss = 3.45716218\n",
      "Iteration 25, loss = 3.38619034\n",
      "Iteration 26, loss = 3.16788810\n",
      "Iteration 27, loss = 3.25595869\n",
      "Iteration 28, loss = 2.99337294\n",
      "Iteration 29, loss = 3.67742217\n",
      "Iteration 30, loss = 3.28091744\n",
      "Iteration 31, loss = 3.46333482\n",
      "Iteration 32, loss = 3.05641889\n",
      "Iteration 33, loss = 3.32343219\n",
      "Iteration 34, loss = 3.16346010\n",
      "Iteration 35, loss = 2.89451583\n",
      "Iteration 36, loss = 2.84003087\n",
      "Iteration 37, loss = 3.11601688\n",
      "Iteration 38, loss = 3.17501195\n",
      "Iteration 39, loss = 3.32709986\n",
      "Iteration 40, loss = 3.40325336\n",
      "Iteration 41, loss = 3.43052738\n",
      "Iteration 42, loss = 3.44373942\n",
      "Iteration 43, loss = 3.53224768\n",
      "Iteration 44, loss = 3.52620584\n",
      "Iteration 45, loss = 3.20580492\n",
      "Iteration 46, loss = 3.65488211\n",
      "Iteration 47, loss = 3.39938172\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.68665274\n",
      "Iteration 2, loss = 3.89114516\n",
      "Iteration 3, loss = 2.22759245\n",
      "Iteration 4, loss = 0.77720727\n",
      "Iteration 5, loss = 0.54876126\n",
      "Iteration 6, loss = 0.52227800\n",
      "Iteration 7, loss = 0.43890457\n",
      "Iteration 8, loss = 0.43432380\n",
      "Iteration 9, loss = 0.42406803\n",
      "Iteration 10, loss = 0.41877661\n",
      "Iteration 11, loss = 0.43140491\n",
      "Iteration 12, loss = 0.42316950\n",
      "Iteration 13, loss = 0.40203399\n",
      "Iteration 14, loss = 0.40307887\n",
      "Iteration 15, loss = 0.43665594\n",
      "Iteration 16, loss = 0.40266738\n",
      "Iteration 17, loss = 0.38802340\n",
      "Iteration 18, loss = 0.40108343\n",
      "Iteration 19, loss = 0.38483767\n",
      "Iteration 20, loss = 0.33089557\n",
      "Iteration 21, loss = 0.38923103\n",
      "Iteration 22, loss = 0.36196799\n",
      "Iteration 23, loss = 0.34387786\n",
      "Iteration 24, loss = 0.34435720\n",
      "Iteration 25, loss = 0.35019657\n",
      "Iteration 26, loss = 0.35432173\n",
      "Iteration 27, loss = 0.37415821\n",
      "Iteration 28, loss = 0.37493382\n",
      "Iteration 29, loss = 0.38185301\n",
      "Iteration 30, loss = 0.41033261\n",
      "Iteration 31, loss = 0.39827739\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.13141820\n",
      "Iteration 2, loss = 4.65737259\n",
      "Iteration 3, loss = 4.15162905\n",
      "Iteration 4, loss = 5.00824936\n",
      "Iteration 5, loss = 4.59070631\n",
      "Iteration 6, loss = 4.10651095\n",
      "Iteration 7, loss = 4.36415005\n",
      "Iteration 8, loss = 3.94395569\n",
      "Iteration 9, loss = 3.86260786\n",
      "Iteration 10, loss = 3.55305842\n",
      "Iteration 11, loss = 4.19611880\n",
      "Iteration 12, loss = 4.01240149\n",
      "Iteration 13, loss = 3.45597005\n",
      "Iteration 14, loss = 3.80199982\n",
      "Iteration 15, loss = 3.68434207\n",
      "Iteration 16, loss = 4.07876279\n",
      "Iteration 17, loss = 3.86092840\n",
      "Iteration 18, loss = 3.87733321\n",
      "Iteration 19, loss = 3.55352376\n",
      "Iteration 20, loss = 3.96036538\n",
      "Iteration 21, loss = 3.70642126\n",
      "Iteration 22, loss = 3.65447851\n",
      "Iteration 23, loss = 3.27971356\n",
      "Iteration 24, loss = 3.11963025\n",
      "Iteration 25, loss = 3.23969687\n",
      "Iteration 26, loss = 3.32286648\n",
      "Iteration 27, loss = 3.27228271\n",
      "Iteration 28, loss = 3.66950440\n",
      "Iteration 29, loss = 3.17762340\n",
      "Iteration 30, loss = 3.05375218\n",
      "Iteration 31, loss = 2.92791251\n",
      "Iteration 32, loss = 3.44660393\n",
      "Iteration 33, loss = 3.15442891\n",
      "Iteration 34, loss = 3.28159611\n",
      "Iteration 35, loss = 3.51574597\n",
      "Iteration 36, loss = 3.27738230\n",
      "Iteration 37, loss = 3.14180988\n",
      "Iteration 38, loss = 3.01954100\n",
      "Iteration 39, loss = 2.71593628\n",
      "Iteration 40, loss = 2.99600454\n",
      "Iteration 41, loss = 2.97198114\n",
      "Iteration 42, loss = 3.21064866\n",
      "Iteration 43, loss = 3.11409320\n",
      "Iteration 44, loss = 3.20966350\n",
      "Iteration 45, loss = 2.89522614\n",
      "Iteration 46, loss = 3.17020376\n",
      "Iteration 47, loss = 3.09535598\n",
      "Iteration 48, loss = 3.06522461\n",
      "Iteration 49, loss = 3.12517354\n",
      "Iteration 50, loss = 2.85844845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 3.01954100\n",
      "Iteration 39, loss = 2.71593628\n",
      "Iteration 40, loss = 2.99600454\n",
      "Iteration 41, loss = 2.97198114\n",
      "Iteration 42, loss = 3.21064866\n",
      "Iteration 43, loss = 3.11409320\n",
      "Iteration 44, loss = 3.20966350\n",
      "Iteration 45, loss = 2.89522614\n",
      "Iteration 46, loss = 3.17020376\n",
      "Iteration 47, loss = 3.09535598\n",
      "Iteration 48, loss = 3.06522461\n",
      "Iteration 49, loss = 3.12517354\n",
      "Iteration 50, loss = 2.85844845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.26128764\n",
      "Iteration 2, loss = 1.03597093\n",
      "Iteration 3, loss = 0.63149425\n",
      "Iteration 4, loss = 0.59722453\n",
      "Iteration 5, loss = 0.59435127\n",
      "Iteration 6, loss = 0.58778426\n",
      "Iteration 7, loss = 0.58463501\n",
      "Iteration 8, loss = 0.56724102\n",
      "Iteration 9, loss = 0.57630242\n",
      "Iteration 10, loss = 0.57626226\n",
      "Iteration 11, loss = 0.55846420\n",
      "Iteration 12, loss = 0.54905214\n",
      "Iteration 13, loss = 0.56347716\n",
      "Iteration 14, loss = 0.56043639\n",
      "Iteration 15, loss = 0.46378976\n",
      "Iteration 16, loss = 0.41582934\n",
      "Iteration 17, loss = 0.41831270\n",
      "Iteration 18, loss = 0.39833979\n",
      "Iteration 19, loss = 0.39844558\n",
      "Iteration 20, loss = 0.42383444\n",
      "Iteration 21, loss = 0.44510972\n",
      "Iteration 22, loss = 0.50244599\n",
      "Iteration 23, loss = 0.38534115\n",
      "Iteration 24, loss = 0.40304438\n",
      "Iteration 25, loss = 0.39241295\n",
      "Iteration 26, loss = 0.46135475\n",
      "Iteration 27, loss = 0.39051306\n",
      "Iteration 28, loss = 0.37279027\n",
      "Iteration 29, loss = 0.37221762\n",
      "Iteration 30, loss = 0.36212196\n",
      "Iteration 31, loss = 0.37824355\n",
      "Iteration 32, loss = 0.38944722\n",
      "Iteration 33, loss = 0.36154203\n",
      "Iteration 34, loss = 0.36825157\n",
      "Iteration 35, loss = 0.37821589\n",
      "Iteration 36, loss = 0.35497234\n",
      "Iteration 37, loss = 0.35888621\n",
      "Iteration 38, loss = 0.39704523\n",
      "Iteration 39, loss = 0.34288308\n",
      "Iteration 40, loss = 0.47014984\n",
      "Iteration 41, loss = 0.54898191\n",
      "Iteration 42, loss = 0.54605685\n",
      "Iteration 43, loss = 0.42687051\n",
      "Iteration 44, loss = 0.36358539\n",
      "Iteration 45, loss = 0.36888797\n",
      "Iteration 46, loss = 0.35386674\n",
      "Iteration 47, loss = 0.40136121\n",
      "Iteration 48, loss = 0.38024464\n",
      "Iteration 49, loss = 0.34901344\n",
      "Iteration 50, loss = 0.35513469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.07546820\n",
      "Iteration 2, loss = 2.97577840\n",
      "Iteration 3, loss = 0.48796539\n",
      "Iteration 4, loss = 0.38080380\n",
      "Iteration 5, loss = 0.41527376\n",
      "Iteration 6, loss = 0.41150811\n",
      "Iteration 7, loss = 0.42337959\n",
      "Iteration 8, loss = 0.39849934\n",
      "Iteration 9, loss = 0.39455736\n",
      "Iteration 10, loss = 0.38813883\n",
      "Iteration 11, loss = 0.41830484\n",
      "Iteration 12, loss = 0.39991301\n",
      "Iteration 13, loss = 0.41016003\n",
      "Iteration 14, loss = 0.40459811\n",
      "Iteration 15, loss = 0.39901760\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.68297390\n",
      "Iteration 2, loss = 5.15130256\n",
      "Iteration 3, loss = 5.10833344\n",
      "Iteration 4, loss = 4.97184428\n",
      "Iteration 5, loss = 4.93906049\n",
      "Iteration 6, loss = 4.40668518\n",
      "Iteration 7, loss = 4.54476335\n",
      "Iteration 8, loss = 4.23964141\n",
      "Iteration 9, loss = 4.37008180\n",
      "Iteration 10, loss = 4.04693743\n",
      "Iteration 11, loss = 4.10396258\n",
      "Iteration 12, loss = 4.17448935\n",
      "Iteration 13, loss = 4.42297489\n",
      "Iteration 14, loss = 4.27259664\n",
      "Iteration 15, loss = 3.85125916\n",
      "Iteration 16, loss = 4.21845526\n",
      "Iteration 17, loss = 4.35359881\n",
      "Iteration 18, loss = 3.87701439\n",
      "Iteration 19, loss = 4.09817355\n",
      "Iteration 20, loss = 4.10854906\n",
      "Iteration 21, loss = 3.84892243\n",
      "Iteration 22, loss = 4.08661211\n",
      "Iteration 23, loss = 3.70055204\n",
      "Iteration 24, loss = 3.77054182\n",
      "Iteration 25, loss = 3.98808561\n",
      "Iteration 26, loss = 3.83297536\n",
      "Iteration 27, loss = 4.34264671\n",
      "Iteration 28, loss = 4.37412022\n",
      "Iteration 29, loss = 3.63502627\n",
      "Iteration 30, loss = 3.92959913\n",
      "Iteration 31, loss = 3.75533515\n",
      "Iteration 32, loss = 3.85262933\n",
      "Iteration 33, loss = 4.12682192\n",
      "Iteration 34, loss = 3.70698933\n",
      "Iteration 35, loss = 4.18174241\n",
      "Iteration 36, loss = 3.81858992\n",
      "Iteration 37, loss = 3.68122244\n",
      "Iteration 38, loss = 3.73646916\n",
      "Iteration 39, loss = 3.45258268\n",
      "Iteration 40, loss = 3.38354169\n",
      "Iteration 41, loss = 3.49280564\n",
      "Iteration 42, loss = 3.50978912\n",
      "Iteration 43, loss = 3.41298202\n",
      "Iteration 44, loss = 3.46255983\n",
      "Iteration 45, loss = 3.35545029\n",
      "Iteration 46, loss = 3.78929359\n",
      "Iteration 47, loss = 3.55956303\n",
      "Iteration 48, loss = 3.83409742\n",
      "Iteration 49, loss = 3.43049060\n",
      "Iteration 50, loss = 3.25336280\n",
      "Iteration 51, loss = 3.56387704\n",
      "Iteration 52, loss = 3.78606694\n",
      "Iteration 53, loss = 3.79456307\n",
      "Iteration 54, loss = 3.72211642\n",
      "Iteration 55, loss = 3.50501157\n",
      "Iteration 56, loss = 3.62327830\n",
      "Iteration 57, loss = 3.60225254\n",
      "Iteration 58, loss = 3.24288701\n",
      "Iteration 59, loss = 3.44954623\n",
      "Iteration 60, loss = 3.55131845\n",
      "Iteration 61, loss = 3.18505935\n",
      "Iteration 62, loss = 3.84583495\n",
      "Iteration 63, loss = 3.44261337\n",
      "Iteration 64, loss = 3.58150200\n",
      "Iteration 65, loss = 3.59571792\n",
      "Iteration 66, loss = 3.43534924\n",
      "Iteration 67, loss = 3.26089268\n",
      "Iteration 68, loss = 3.22437494\n",
      "Iteration 69, loss = 3.57361612\n",
      "Iteration 70, loss = 3.26984703\n",
      "Iteration 71, loss = 3.05295256\n",
      "Iteration 72, loss = 3.44723179\n",
      "Iteration 73, loss = 3.16677372\n",
      "Iteration 74, loss = 3.28513682\n",
      "Iteration 75, loss = 3.16765232\n",
      "Iteration 76, loss = 3.33588147\n",
      "Iteration 77, loss = 3.29145564\n",
      "Iteration 78, loss = 3.32588649\n",
      "Iteration 79, loss = 3.31303781\n",
      "Iteration 80, loss = 3.22679447\n",
      "Iteration 81, loss = 3.42113092\n",
      "Iteration 82, loss = 3.25587418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.10930715\n",
      "Iteration 2, loss = 4.36821206\n",
      "Iteration 3, loss = 4.39131603\n",
      "Iteration 4, loss = 3.79918226\n",
      "Iteration 5, loss = 3.66638127\n",
      "Iteration 6, loss = 4.39996863\n",
      "Iteration 7, loss = 4.35359181\n",
      "Iteration 8, loss = 4.01722371\n",
      "Iteration 9, loss = 3.63590415\n",
      "Iteration 10, loss = 3.91636384\n",
      "Iteration 11, loss = 3.76894974\n",
      "Iteration 12, loss = 3.67037918\n",
      "Iteration 13, loss = 3.66821195\n",
      "Iteration 14, loss = 3.83913512\n",
      "Iteration 15, loss = 4.01906860\n",
      "Iteration 16, loss = 3.82959755\n",
      "Iteration 17, loss = 3.36431140\n",
      "Iteration 18, loss = 3.81678926\n",
      "Iteration 19, loss = 3.92410408\n",
      "Iteration 20, loss = 3.73265039\n",
      "Iteration 21, loss = 3.71394174\n",
      "Iteration 22, loss = 3.59675965\n",
      "Iteration 23, loss = 3.34993043\n",
      "Iteration 24, loss = 3.29169209\n",
      "Iteration 25, loss = 3.24860656\n",
      "Iteration 26, loss = 3.08749401\n",
      "Iteration 27, loss = 3.67701335\n",
      "Iteration 28, loss = 3.80522275\n",
      "Iteration 29, loss = 3.37692404\n",
      "Iteration 30, loss = 3.36253970\n",
      "Iteration 31, loss = 2.97187341\n",
      "Iteration 32, loss = 3.14354539\n",
      "Iteration 33, loss = 3.08318277\n",
      "Iteration 34, loss = 2.98646216\n",
      "Iteration 35, loss = 2.74333783\n",
      "Iteration 36, loss = 2.70702681\n",
      "Iteration 37, loss = 2.91573641\n",
      "Iteration 38, loss = 2.67161120\n",
      "Iteration 39, loss = 2.91060821\n",
      "Iteration 40, loss = 2.69312687\n",
      "Iteration 41, loss = 2.99262619\n",
      "Iteration 42, loss = 2.78418267\n",
      "Iteration 43, loss = 2.83550022\n",
      "Iteration 44, loss = 3.19752975\n",
      "Iteration 45, loss = 3.02783753\n",
      "Iteration 46, loss = 2.71906293\n",
      "Iteration 47, loss = 2.52888708\n",
      "Iteration 48, loss = 2.95024350\n",
      "Iteration 49, loss = 2.71608652\n",
      "Iteration 50, loss = 2.69660208\n",
      "Iteration 51, loss = 3.22901808\n",
      "Iteration 52, loss = 3.08428801\n",
      "Iteration 53, loss = 2.72817903\n",
      "Iteration 54, loss = 2.82967482\n",
      "Iteration 55, loss = 2.66706696\n",
      "Iteration 56, loss = 2.63063200\n",
      "Iteration 57, loss = 2.49914447\n",
      "Iteration 58, loss = 2.61158148\n",
      "Iteration 59, loss = 2.43665842\n",
      "Iteration 60, loss = 2.33779531\n",
      "Iteration 61, loss = 2.46311873\n",
      "Iteration 62, loss = 2.34954536\n",
      "Iteration 63, loss = 2.18057208\n",
      "Iteration 64, loss = 2.05281335\n",
      "Iteration 65, loss = 2.34250488\n",
      "Iteration 66, loss = 2.54925422\n",
      "Iteration 67, loss = 3.05442539\n",
      "Iteration 68, loss = 2.59826320\n",
      "Iteration 69, loss = 2.41259319\n",
      "Iteration 70, loss = 2.55070102\n",
      "Iteration 71, loss = 2.50724283\n",
      "Iteration 72, loss = 2.32471077\n",
      "Iteration 73, loss = 2.07791300\n",
      "Iteration 74, loss = 2.49118496\n",
      "Iteration 75, loss = 2.08821253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 3.34993043\n",
      "Iteration 24, loss = 3.29169209\n",
      "Iteration 25, loss = 3.24860656\n",
      "Iteration 26, loss = 3.08749401\n",
      "Iteration 27, loss = 3.67701335\n",
      "Iteration 28, loss = 3.80522275\n",
      "Iteration 29, loss = 3.37692404\n",
      "Iteration 30, loss = 3.36253970\n",
      "Iteration 31, loss = 2.97187341\n",
      "Iteration 32, loss = 3.14354539\n",
      "Iteration 33, loss = 3.08318277\n",
      "Iteration 34, loss = 2.98646216\n",
      "Iteration 35, loss = 2.74333783\n",
      "Iteration 36, loss = 2.70702681\n",
      "Iteration 37, loss = 2.91573641\n",
      "Iteration 38, loss = 2.67161120\n",
      "Iteration 39, loss = 2.91060821\n",
      "Iteration 40, loss = 2.69312687\n",
      "Iteration 41, loss = 2.99262619\n",
      "Iteration 42, loss = 2.78418267\n",
      "Iteration 43, loss = 2.83550022\n",
      "Iteration 44, loss = 3.19752975\n",
      "Iteration 45, loss = 3.02783753\n",
      "Iteration 46, loss = 2.71906293\n",
      "Iteration 47, loss = 2.52888708\n",
      "Iteration 48, loss = 2.95024350\n",
      "Iteration 49, loss = 2.71608652\n",
      "Iteration 50, loss = 2.69660208\n",
      "Iteration 51, loss = 3.22901808\n",
      "Iteration 52, loss = 3.08428801\n",
      "Iteration 53, loss = 2.72817903\n",
      "Iteration 54, loss = 2.82967482\n",
      "Iteration 55, loss = 2.66706696\n",
      "Iteration 56, loss = 2.63063200\n",
      "Iteration 57, loss = 2.49914447\n",
      "Iteration 58, loss = 2.61158148\n",
      "Iteration 59, loss = 2.43665842\n",
      "Iteration 60, loss = 2.33779531\n",
      "Iteration 61, loss = 2.46311873\n",
      "Iteration 62, loss = 2.34954536\n",
      "Iteration 63, loss = 2.18057208\n",
      "Iteration 64, loss = 2.05281335\n",
      "Iteration 65, loss = 2.34250488\n",
      "Iteration 66, loss = 2.54925422\n",
      "Iteration 67, loss = 3.05442539\n",
      "Iteration 68, loss = 2.59826320\n",
      "Iteration 69, loss = 2.41259319\n",
      "Iteration 70, loss = 2.55070102\n",
      "Iteration 71, loss = 2.50724283\n",
      "Iteration 72, loss = 2.32471077\n",
      "Iteration 73, loss = 2.07791300\n",
      "Iteration 74, loss = 2.49118496\n",
      "Iteration 75, loss = 2.08821253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.56605442\n",
      "Iteration 2, loss = 1.07074700\n",
      "Iteration 3, loss = 0.46292353\n",
      "Iteration 4, loss = 0.40393295\n",
      "Iteration 5, loss = 0.37326466\n",
      "Iteration 6, loss = 0.33711878\n",
      "Iteration 7, loss = 0.33769246\n",
      "Iteration 8, loss = 0.34503669\n",
      "Iteration 9, loss = 0.33290654\n",
      "Iteration 10, loss = 0.32389803\n",
      "Iteration 11, loss = 0.32224167\n",
      "Iteration 12, loss = 0.30627045\n",
      "Iteration 13, loss = 0.30285902\n",
      "Iteration 14, loss = 0.30704280\n",
      "Iteration 15, loss = 0.30248151\n",
      "Iteration 16, loss = 0.29078148\n",
      "Iteration 17, loss = 0.30065069\n",
      "Iteration 18, loss = 0.30811866\n",
      "Iteration 19, loss = 0.31075496\n",
      "Iteration 20, loss = 0.29370550\n",
      "Iteration 21, loss = 0.28932057\n",
      "Iteration 22, loss = 0.31667767\n",
      "Iteration 23, loss = 0.28917095\n",
      "Iteration 24, loss = 0.29381728\n",
      "Iteration 25, loss = 0.30530100\n",
      "Iteration 26, loss = 0.30034973\n",
      "Iteration 27, loss = 0.27716184\n",
      "Iteration 28, loss = 0.28090427\n",
      "Iteration 29, loss = 0.27790793\n",
      "Iteration 30, loss = 0.27939208\n",
      "Iteration 31, loss = 0.27257999\n",
      "Iteration 32, loss = 0.28995761\n",
      "Iteration 33, loss = 0.26886044\n",
      "Iteration 34, loss = 0.27562950\n",
      "Iteration 35, loss = 0.26686923\n",
      "Iteration 36, loss = 0.28277444\n",
      "Iteration 37, loss = 0.27054953\n",
      "Iteration 38, loss = 0.27617885\n",
      "Iteration 39, loss = 0.26995424\n",
      "Iteration 40, loss = 0.26825402\n",
      "Iteration 41, loss = 0.26710678\n",
      "Iteration 42, loss = 0.26911195\n",
      "Iteration 43, loss = 0.27389860\n",
      "Iteration 44, loss = 0.26477016\n",
      "Iteration 45, loss = 0.27154487\n",
      "Iteration 46, loss = 0.27492326\n",
      "Iteration 47, loss = 0.28112396\n",
      "Iteration 48, loss = 0.26579300\n",
      "Iteration 49, loss = 0.27537812\n",
      "Iteration 50, loss = 0.26561454\n",
      "Iteration 51, loss = 0.26347717\n",
      "Iteration 52, loss = 0.26167398\n",
      "Iteration 53, loss = 0.25746101\n",
      "Iteration 54, loss = 0.26069094\n",
      "Iteration 55, loss = 0.25812354\n",
      "Iteration 56, loss = 0.25910546\n",
      "Iteration 57, loss = 0.25783787\n",
      "Iteration 58, loss = 0.25277393\n",
      "Iteration 59, loss = 0.25508134\n",
      "Iteration 60, loss = 0.26071897\n",
      "Iteration 61, loss = 0.25118706\n",
      "Iteration 62, loss = 0.25498669\n",
      "Iteration 63, loss = 0.25423486\n",
      "Iteration 64, loss = 0.25280313\n",
      "Iteration 65, loss = 0.25105930\n",
      "Iteration 66, loss = 0.24877720\n",
      "Iteration 67, loss = 0.24778428\n",
      "Iteration 68, loss = 0.25212594\n",
      "Iteration 69, loss = 0.24936085\n",
      "Iteration 70, loss = 0.24716142\n",
      "Iteration 71, loss = 0.24877364\n",
      "Iteration 72, loss = 0.24600522\n",
      "Iteration 73, loss = 0.24882282\n",
      "Iteration 74, loss = 0.24483268\n",
      "Iteration 75, loss = 0.25658905\n",
      "\n",
      "Iteration 6, loss = 0.29103140\n",
      "Iteration 7, loss = 0.28946634\n",
      "Iteration 8, loss = 0.28631575\n",
      "Iteration 9, loss = 0.28216691\n",
      "Iteration 10, loss = 0.28566592\n",
      "Iteration 11, loss = 0.29092133\n",
      "Iteration 12, loss = 0.28104918\n",
      "Iteration 13, loss = 0.29106475\n",
      "Iteration 14, loss = 0.29157574\n",
      "Iteration 15, loss = 0.28143914\n",
      "Iteration 16, loss = 0.28289649\n",
      "Iteration 17, loss = 0.28134580\n",
      "Iteration 18, loss = 0.28604292\n",
      "Iteration 19, loss = 0.27760337\n",
      "Iteration 20, loss = 0.27864260\n",
      "Iteration 21, loss = 0.29139117\n",
      "Iteration 22, loss = 0.28209263\n",
      "Iteration 23, loss = 0.28480288\n",
      "Iteration 24, loss = 0.27685347\n",
      "Iteration 25, loss = 0.28133348\n",
      "Iteration 26, loss = 0.27697744\n",
      "Iteration 27, loss = 0.28828846\n",
      "Iteration 28, loss = 0.27967856\n",
      "Iteration 29, loss = 0.28461807\n",
      "Iteration 30, loss = 0.28111258\n",
      "Iteration 31, loss = 0.27982044\n",
      "Iteration 32, loss = 0.28103362\n",
      "Iteration 33, loss = 0.28396862\n",
      "Iteration 34, loss = 0.29236654\n",
      "Iteration 35, loss = 0.28160088\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.49594265\n",
      "Iteration 2, loss = 1.22921284\n",
      "Iteration 3, loss = 0.47256405\n",
      "Iteration 4, loss = 0.45062218\n",
      "Iteration 5, loss = 0.51261721\n",
      "Iteration 6, loss = 0.44927954\n",
      "Iteration 7, loss = 0.46803254\n",
      "Iteration 8, loss = 0.40689024\n",
      "Iteration 9, loss = 0.36773671\n",
      "Iteration 10, loss = 0.35649508\n",
      "Iteration 11, loss = 0.37534321\n",
      "Iteration 12, loss = 0.33554524\n",
      "Iteration 13, loss = 0.33289111\n",
      "Iteration 14, loss = 0.33867661\n",
      "Iteration 15, loss = 0.35318457\n",
      "Iteration 16, loss = 0.32943565\n",
      "Iteration 17, loss = 0.31917649\n",
      "Iteration 18, loss = 0.30649750\n",
      "Iteration 19, loss = 0.36253560\n",
      "Iteration 20, loss = 0.35142757\n",
      "Iteration 21, loss = 0.31832238\n",
      "Iteration 22, loss = 0.32347994\n",
      "Iteration 23, loss = 0.32871071\n",
      "Iteration 24, loss = 0.31214576\n",
      "Iteration 25, loss = 0.29464860\n",
      "Iteration 26, loss = 0.31809194\n",
      "Iteration 27, loss = 0.30749369\n",
      "Iteration 28, loss = 0.32543451\n",
      "Iteration 29, loss = 0.34444654\n",
      "Iteration 30, loss = 0.31269871\n",
      "Iteration 31, loss = 0.33041913\n",
      "Iteration 32, loss = 0.30674637\n",
      "Iteration 33, loss = 0.34182164\n",
      "Iteration 34, loss = 0.30604591\n",
      "Iteration 35, loss = 0.28594169\n",
      "Iteration 36, loss = 0.30086925\n",
      "Iteration 37, loss = 0.30667731\n",
      "Iteration 38, loss = 0.30616852\n",
      "Iteration 39, loss = 0.30666560\n",
      "Iteration 40, loss = 0.30159996\n",
      "Iteration 41, loss = 0.30412169\n",
      "Iteration 42, loss = 0.28408027\n",
      "Iteration 43, loss = 0.29041946\n",
      "Iteration 44, loss = 0.29737091\n",
      "Iteration 45, loss = 0.29042439\n",
      "Iteration 46, loss = 0.28349025\n",
      "Iteration 47, loss = 0.27507149\n",
      "Iteration 48, loss = 0.28155430\n",
      "Iteration 49, loss = 0.27600521\n",
      "Iteration 50, loss = 0.26775796\n",
      "Iteration 51, loss = 0.26161241\n",
      "Iteration 52, loss = 0.28481188\n",
      "Iteration 53, loss = 0.29014417\n",
      "Iteration 54, loss = 0.27205913\n",
      "Iteration 55, loss = 0.26466583\n",
      "Iteration 56, loss = 0.25918039\n",
      "Iteration 57, loss = 0.26151096\n",
      "Iteration 58, loss = 0.26228186\n",
      "Iteration 59, loss = 0.26331525\n",
      "Iteration 60, loss = 0.25875009\n",
      "Iteration 61, loss = 0.25641850\n",
      "Iteration 62, loss = 0.26125741\n",
      "Iteration 63, loss = 0.26637000\n",
      "Iteration 64, loss = 0.26322905\n",
      "Iteration 65, loss = 0.25856132\n",
      "Iteration 66, loss = 0.25992137\n",
      "Iteration 67, loss = 0.26505743\n",
      "Iteration 68, loss = 0.25805234\n",
      "Iteration 69, loss = 0.26054176\n",
      "Iteration 70, loss = 0.25538632\n",
      "Iteration 71, loss = 0.26537055\n",
      "Iteration 72, loss = 0.25992874\n",
      "Iteration 73, loss = 0.26612860\n",
      "Iteration 74, loss = 0.25960155\n",
      "Iteration 75, loss = 0.25761366\n",
      "Iteration 76, loss = 0.25550727\n",
      "Iteration 77, loss = 0.26007770\n",
      "Iteration 78, loss = 0.26051240\n",
      "Iteration 79, loss = 0.25833742\n",
      "Iteration 80, loss = 0.26243087\n",
      "Iteration 81, loss = 0.25738038\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.68297390\n",
      "Iteration 2, loss = 5.15130256\n",
      "Iteration 3, loss = 5.10833344\n",
      "Iteration 4, loss = 4.97184428\n",
      "Iteration 5, loss = 4.93906049\n",
      "Iteration 6, loss = 4.40668518\n",
      "Iteration 7, loss = 4.54476335\n",
      "Iteration 8, loss = 4.23964141\n",
      "Iteration 9, loss = 4.37008180\n",
      "Iteration 10, loss = 4.04693743\n",
      "Iteration 11, loss = 4.10396258\n",
      "Iteration 12, loss = 4.17448935\n",
      "Iteration 13, loss = 4.42297489\n",
      "Iteration 14, loss = 4.27259664\n",
      "Iteration 15, loss = 3.85125916\n",
      "Iteration 16, loss = 4.21845526\n",
      "Iteration 17, loss = 4.35359881\n",
      "Iteration 18, loss = 3.87701439\n",
      "Iteration 19, loss = 4.09817355\n",
      "Iteration 20, loss = 4.10854906\n",
      "Iteration 21, loss = 3.84892243\n",
      "Iteration 22, loss = 4.08661211\n",
      "Iteration 23, loss = 3.70055204\n",
      "Iteration 24, loss = 3.77054182\n",
      "Iteration 25, loss = 3.98808561\n",
      "Iteration 26, loss = 3.83297536\n",
      "Iteration 27, loss = 4.34264671\n",
      "Iteration 28, loss = 4.37412022\n",
      "Iteration 29, loss = 3.63502627\n",
      "Iteration 30, loss = 3.92959913\n",
      "Iteration 31, loss = 3.75533515\n",
      "Iteration 32, loss = 3.85262933\n",
      "Iteration 33, loss = 4.12682192\n",
      "Iteration 34, loss = 3.70698933\n",
      "Iteration 35, loss = 4.18174241\n",
      "Iteration 36, loss = 3.81858992\n",
      "Iteration 37, loss = 3.68122244\n",
      "Iteration 38, loss = 3.73646916\n",
      "Iteration 39, loss = 3.45258268\n",
      "Iteration 40, loss = 3.38354169\n",
      "Iteration 41, loss = 3.49280564\n",
      "Iteration 42, loss = 3.50978912\n",
      "Iteration 43, loss = 3.41298202\n",
      "Iteration 44, loss = 3.46255983\n",
      "Iteration 45, loss = 3.35545029\n",
      "Iteration 46, loss = 3.78929359\n",
      "Iteration 47, loss = 3.55956303\n",
      "Iteration 48, loss = 3.83409742\n",
      "Iteration 49, loss = 3.43049060\n",
      "Iteration 50, loss = 3.25336280\n",
      "Iteration 51, loss = 3.56387704\n",
      "Iteration 52, loss = 3.78606694\n",
      "Iteration 53, loss = 3.79456307\n",
      "Iteration 54, loss = 3.72211642\n",
      "Iteration 55, loss = 3.50501157\n",
      "Iteration 56, loss = 3.62327830\n",
      "Iteration 57, loss = 3.60225254\n",
      "Iteration 58, loss = 3.24288701\n",
      "Iteration 59, loss = 3.44954623\n",
      "Iteration 60, loss = 3.55131845\n",
      "Iteration 61, loss = 3.18505935\n",
      "Iteration 62, loss = 3.84583495\n",
      "Iteration 63, loss = 3.44261337\n",
      "Iteration 64, loss = 3.58150200\n",
      "Iteration 65, loss = 3.59571792\n",
      "Iteration 66, loss = 3.43534924\n",
      "Iteration 67, loss = 3.26089268\n",
      "Iteration 68, loss = 3.22437494\n",
      "Iteration 69, loss = 3.57361612\n",
      "Iteration 70, loss = 3.26984703\n",
      "Iteration 71, loss = 3.05295256\n",
      "Iteration 72, loss = 3.44723179\n",
      "Iteration 73, loss = 3.16677372\n",
      "Iteration 74, loss = 3.28513682\n",
      "Iteration 75, loss = 3.16765232\n",
      "Iteration 76, loss = 3.33588147\n",
      "Iteration 77, loss = 3.29145564\n",
      "Iteration 78, loss = 3.32588649\n",
      "Iteration 79, loss = 3.31303781\n",
      "Iteration 80, loss = 3.22679447\n",
      "Iteration 81, loss = 3.42113092\n",
      "Iteration 82, loss = 3.25587418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.45927659\n",
      "Iteration 2, loss = 3.41882408\n",
      "Iteration 3, loss = 2.57548731\n",
      "Iteration 4, loss = 2.14007672\n",
      "Iteration 5, loss = 1.30566053\n",
      "Iteration 6, loss = 0.45681398\n",
      "Iteration 7, loss = 0.39696358\n",
      "Iteration 8, loss = 0.36068454\n",
      "Iteration 9, loss = 0.35795099\n",
      "Iteration 10, loss = 0.38182498\n",
      "Iteration 11, loss = 0.40029549\n",
      "Iteration 12, loss = 0.39508752\n",
      "Iteration 13, loss = 0.38449860\n",
      "Iteration 14, loss = 0.37374706\n",
      "Iteration 15, loss = 0.35783196\n",
      "Iteration 16, loss = 0.37253641\n",
      "Iteration 17, loss = 0.37317077\n",
      "Iteration 18, loss = 0.36690837\n",
      "Iteration 19, loss = 0.35972483\n",
      "Iteration 20, loss = 0.35630316\n",
      "Iteration 21, loss = 0.36473011\n",
      "Iteration 22, loss = 0.36230860\n",
      "Iteration 23, loss = 0.37197341\n",
      "Iteration 24, loss = 0.36004322\n",
      "Iteration 25, loss = 0.35660408\n",
      "Iteration 26, loss = 0.38216435\n",
      "Iteration 27, loss = 0.35583512\n",
      "Iteration 28, loss = 0.36873147\n",
      "Iteration 29, loss = 0.34983127\n",
      "Iteration 30, loss = 0.33550910\n",
      "Iteration 31, loss = 0.31884274\n",
      "Iteration 32, loss = 0.31197761\n",
      "Iteration 33, loss = 0.31536338\n",
      "Iteration 34, loss = 0.32152337\n",
      "Iteration 35, loss = 0.32504618\n",
      "Iteration 36, loss = 0.31235352\n",
      "Iteration 37, loss = 0.31157748\n",
      "Iteration 38, loss = 0.32717176\n",
      "Iteration 39, loss = 0.33000933\n",
      "Iteration 40, loss = 0.31753608\n",
      "Iteration 41, loss = 0.31319301\n",
      "Iteration 42, loss = 0.32347450\n",
      "Iteration 43, loss = 0.32298486\n",
      "Iteration 44, loss = 0.33394642\n",
      "Iteration 45, loss = 0.31807914\n",
      "Iteration 46, loss = 0.32215400\n",
      "Iteration 47, loss = 0.31475379\n",
      "Iteration 48, loss = 0.31271033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76, loss = 0.25320183\n",
      "Iteration 77, loss = 0.25100673\n",
      "Iteration 78, loss = 0.24707538\n",
      "Iteration 79, loss = 0.24724871\n",
      "Iteration 80, loss = 0.24598297\n",
      "Iteration 81, loss = 0.24631310\n",
      "Iteration 82, loss = 0.25814868\n",
      "Iteration 83, loss = 0.25727503\n",
      "Iteration 84, loss = 0.24900450\n",
      "Iteration 85, loss = 0.25339651\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP Scaled Best Params found through grid search cv are:\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'constant'}\n",
      "MLP-Gridsearch metrics:\n",
      "Accuracy Score: 0.828581668087841\n",
      "F1 score: 0.799191976639672\n",
      "MLP Confusion Matrix-Gridsearch\n",
      "[[8.37722260e-01 3.63718208e-02 1.25905919e-01]\n",
      " [3.15690964e-01 1.13652906e-01 5.70656130e-01]\n",
      " [5.88176661e-02 6.64512539e-05 9.41115883e-01]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.26922311\n",
      "Iteration 33, loss = 0.27111372\n",
      "Iteration 34, loss = 0.27848335\n",
      "Iteration 35, loss = 0.26453662\n",
      "Iteration 36, loss = 0.27167554\n",
      "Iteration 37, loss = 0.27504083\n",
      "Iteration 38, loss = 0.27304957\n",
      "Iteration 39, loss = 0.26102637\n",
      "Iteration 40, loss = 0.26355637\n",
      "Iteration 41, loss = 0.25886670\n",
      "Iteration 42, loss = 0.27197636\n",
      "Iteration 43, loss = 0.26509269\n",
      "Iteration 44, loss = 0.27040923\n",
      "Iteration 45, loss = 0.26027314\n",
      "Iteration 46, loss = 0.26578179\n",
      "Iteration 47, loss = 0.26304403\n",
      "Iteration 48, loss = 0.27098182\n",
      "Iteration 49, loss = 0.25686861\n",
      "Iteration 50, loss = 0.25887245\n",
      "Iteration 51, loss = 0.26960251\n",
      "Iteration 52, loss = 0.27005669\n",
      "Iteration 53, loss = 0.25519648\n",
      "Iteration 54, loss = 0.26327628\n",
      "Iteration 55, loss = 0.25664010\n",
      "Iteration 56, loss = 0.26070010\n",
      "Iteration 57, loss = 0.25805642\n",
      "Iteration 58, loss = 0.25466238\n",
      "Iteration 59, loss = 0.27238738\n",
      "Iteration 60, loss = 0.25996688\n",
      "Iteration 61, loss = 0.26579419\n",
      "Iteration 62, loss = 0.26373347\n",
      "Iteration 63, loss = 0.25838941\n",
      "Iteration 64, loss = 0.26116575\n",
      "Iteration 65, loss = 0.26035650\n",
      "Iteration 66, loss = 0.25889587\n",
      "Iteration 67, loss = 0.25870187\n",
      "Iteration 68, loss = 0.25710754\n",
      "Iteration 69, loss = 0.25478785\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.07546820\n",
      "Iteration 2, loss = 2.97577840\n",
      "Iteration 3, loss = 0.48796539\n",
      "Iteration 4, loss = 0.38080380\n",
      "Iteration 5, loss = 0.41527376\n",
      "Iteration 6, loss = 0.41150811\n",
      "Iteration 7, loss = 0.42337959\n",
      "Iteration 8, loss = 0.39849934\n",
      "Iteration 9, loss = 0.39455736\n",
      "Iteration 10, loss = 0.38813883\n",
      "Iteration 11, loss = 0.41830484\n",
      "Iteration 12, loss = 0.39991301\n",
      "Iteration 13, loss = 0.41016003\n",
      "Iteration 14, loss = 0.40459811\n",
      "Iteration 15, loss = 0.39901760\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.37398098\n",
      "Iteration 2, loss = 4.90374058\n",
      "Iteration 3, loss = 4.58748924\n",
      "Iteration 4, loss = 4.01410781\n",
      "Iteration 5, loss = 4.08404351\n",
      "Iteration 6, loss = 4.20448858\n",
      "Iteration 7, loss = 4.02262764\n",
      "Iteration 8, loss = 3.83087678\n",
      "Iteration 9, loss = 4.26451066\n",
      "Iteration 10, loss = 3.72117783\n",
      "Iteration 11, loss = 3.65182835\n",
      "Iteration 12, loss = 3.44922449\n",
      "Iteration 13, loss = 3.74572569\n",
      "Iteration 14, loss = 3.71573199\n",
      "Iteration 15, loss = 3.66143587\n",
      "Iteration 16, loss = 3.96159290\n",
      "Iteration 17, loss = 3.62269570\n",
      "Iteration 18, loss = 4.03616277\n",
      "Iteration 19, loss = 3.92312012\n",
      "Iteration 20, loss = 3.73454363\n",
      "Iteration 21, loss = 3.45361179\n",
      "Iteration 22, loss = 3.76848446\n",
      "Iteration 23, loss = 3.35642469\n",
      "Iteration 24, loss = 3.45716218\n",
      "Iteration 25, loss = 3.38619034\n",
      "Iteration 26, loss = 3.16788810\n",
      "Iteration 27, loss = 3.25595869\n",
      "Iteration 28, loss = 2.99337294\n",
      "Iteration 29, loss = 3.67742217\n",
      "Iteration 30, loss = 3.28091744\n",
      "Iteration 31, loss = 3.46333482\n",
      "Iteration 32, loss = 3.05641889\n",
      "Iteration 33, loss = 3.32343219\n",
      "Iteration 34, loss = 3.16346010\n",
      "Iteration 35, loss = 2.89451583\n",
      "Iteration 36, loss = 2.84003087\n",
      "Iteration 37, loss = 3.11601688\n",
      "Iteration 38, loss = 3.17501195\n",
      "Iteration 39, loss = 3.32709986\n",
      "Iteration 40, loss = 3.40325336\n",
      "Iteration 41, loss = 3.43052738\n",
      "Iteration 42, loss = 3.44373942\n",
      "Iteration 43, loss = 3.53224768\n",
      "Iteration 44, loss = 3.52620584\n",
      "Iteration 45, loss = 3.20580492\n",
      "Iteration 46, loss = 3.65488211\n",
      "Iteration 47, loss = 3.39938172\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.17825869\n",
      "Iteration 2, loss = 0.62578469\n",
      "Iteration 3, loss = 0.50415622\n",
      "Iteration 4, loss = 0.49180423\n",
      "Iteration 5, loss = 0.49744428\n",
      "Iteration 6, loss = 0.41131888\n",
      "Iteration 7, loss = 0.37701041\n",
      "Iteration 8, loss = 0.39525768\n",
      "Iteration 9, loss = 0.36269040\n",
      "Iteration 10, loss = 0.37889527\n",
      "Iteration 11, loss = 0.38749152\n",
      "Iteration 12, loss = 0.38215274\n",
      "Iteration 13, loss = 0.44328627\n",
      "Iteration 14, loss = 0.36939454\n",
      "Iteration 15, loss = 0.35285661\n",
      "Iteration 16, loss = 0.35394960\n",
      "Iteration 17, loss = 0.36774084\n",
      "Iteration 18, loss = 0.40975432\n",
      "Iteration 19, loss = 0.38528926\n",
      "Iteration 20, loss = 0.40859657\n",
      "Iteration 21, loss = 0.36343969\n",
      "Iteration 22, loss = 0.36786118\n",
      "Iteration 23, loss = 0.36525911\n",
      "Iteration 24, loss = 0.37132325\n",
      "Iteration 25, loss = 0.36782948\n",
      "Iteration 26, loss = 0.34797754\n",
      "Iteration 27, loss = 0.34616891\n",
      "Iteration 28, loss = 0.36118271\n",
      "Iteration 29, loss = 0.33362927\n",
      "Iteration 30, loss = 0.31982255\n",
      "Iteration 31, loss = 0.32403086\n",
      "Iteration 32, loss = 0.32026498\n",
      "Iteration 33, loss = 0.33572858\n",
      "Iteration 34, loss = 0.32860074\n",
      "Iteration 35, loss = 0.32944647\n",
      "Iteration 36, loss = 0.32618801\n",
      "Iteration 37, loss = 0.32182157\n",
      "Iteration 38, loss = 0.34863975\n",
      "Iteration 39, loss = 0.31818420\n",
      "Iteration 40, loss = 0.31201336\n",
      "Iteration 41, loss = 0.33214920\n",
      "Iteration 42, loss = 0.32457873\n",
      "Iteration 43, loss = 0.32129459\n",
      "Iteration 44, loss = 0.31903197\n",
      "Iteration 45, loss = 0.33076629\n",
      "Iteration 46, loss = 0.31008246\n",
      "Iteration 47, loss = 0.31113027\n",
      "Iteration 48, loss = 0.30891656\n",
      "Iteration 49, loss = 0.29685750\n",
      "Iteration 50, loss = 0.30426244\n",
      "Iteration 51, loss = 0.31767059\n",
      "Iteration 52, loss = 0.30287546\n",
      "Iteration 53, loss = 0.30247036\n",
      "Iteration 54, loss = 0.29808782\n",
      "Iteration 55, loss = 0.31894005\n",
      "Iteration 56, loss = 0.29961985\n",
      "Iteration 57, loss = 0.29422164\n",
      "Iteration 58, loss = 0.27938241\n",
      "Iteration 59, loss = 0.30487597\n",
      "Iteration 60, loss = 0.30500330\n",
      "Iteration 61, loss = 0.27341541\n",
      "Iteration 62, loss = 0.27191543\n",
      "Iteration 63, loss = 0.27543449\n",
      "Iteration 64, loss = 0.34630695\n",
      "Iteration 65, loss = 0.30044369\n",
      "Iteration 66, loss = 0.27695412\n",
      "Iteration 67, loss = 0.27007211\n",
      "Iteration 68, loss = 0.28086233\n",
      "Iteration 69, loss = 0.26821375\n",
      "Iteration 70, loss = 0.26104198\n",
      "Iteration 71, loss = 0.27116898\n",
      "Iteration 72, loss = 0.26487204\n",
      "Iteration 73, loss = 0.26950214\n",
      "Iteration 74, loss = 0.28040846\n",
      "Iteration 75, loss = 0.30254775\n",
      "Iteration 76, loss = 0.26908930\n",
      "Iteration 77, loss = 0.27809652\n",
      "Iteration 78, loss = 0.27108279\n",
      "Iteration 79, loss = 0.27550834\n",
      "Iteration 80, loss = 0.26680365\n",
      "Iteration 81, loss = 0.30900032\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.45927659\n",
      "Iteration 2, loss = 3.41882408\n",
      "Iteration 3, loss = 2.57548731\n",
      "Iteration 4, loss = 2.14007672\n",
      "Iteration 5, loss = 1.30566053\n",
      "Iteration 6, loss = 0.45681398\n",
      "Iteration 7, loss = 0.39696358\n",
      "Iteration 8, loss = 0.36068454\n",
      "Iteration 9, loss = 0.35795099\n",
      "Iteration 10, loss = 0.38182498\n",
      "Iteration 11, loss = 0.40029549\n",
      "Iteration 12, loss = 0.39508752\n",
      "Iteration 13, loss = 0.38449860\n",
      "Iteration 14, loss = 0.37374706\n",
      "Iteration 15, loss = 0.35783196\n",
      "Iteration 16, loss = 0.37253641\n",
      "Iteration 17, loss = 0.37317077\n",
      "Iteration 18, loss = 0.36690837\n",
      "Iteration 19, loss = 0.35972483\n",
      "Iteration 20, loss = 0.35630316\n",
      "Iteration 21, loss = 0.36473011\n",
      "Iteration 22, loss = 0.36230860\n",
      "Iteration 23, loss = 0.37197341\n",
      "Iteration 24, loss = 0.36004322\n",
      "Iteration 25, loss = 0.35660408\n",
      "Iteration 26, loss = 0.38216435\n",
      "Iteration 27, loss = 0.35583512\n",
      "Iteration 28, loss = 0.36873147\n",
      "Iteration 29, loss = 0.34983127\n",
      "Iteration 30, loss = 0.33550910\n",
      "Iteration 31, loss = 0.31884274\n",
      "Iteration 32, loss = 0.31197761\n",
      "Iteration 33, loss = 0.31536338\n",
      "Iteration 34, loss = 0.32152337\n",
      "Iteration 35, loss = 0.32504618\n",
      "Iteration 36, loss = 0.31235352\n",
      "Iteration 37, loss = 0.31157748\n",
      "Iteration 38, loss = 0.32717176\n",
      "Iteration 39, loss = 0.33000933\n",
      "Iteration 40, loss = 0.31753608\n",
      "Iteration 41, loss = 0.31319301\n",
      "Iteration 42, loss = 0.32347450\n",
      "Iteration 43, loss = 0.32298486\n",
      "Iteration 44, loss = 0.33394642\n",
      "Iteration 45, loss = 0.31807914\n",
      "Iteration 46, loss = 0.32215400\n",
      "Iteration 47, loss = 0.31475379\n",
      "Iteration 48, loss = 0.31271033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Gridsearch MLP\n",
    "# Inspired by https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(10,10,10),(10,25,10),(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "mlp_base = MLPClassifier(random_state=random_state, max_iter=300, verbose=10)\n",
    "mlp_clf_grid = GridSearchCV(mlp_base, parameter_space, n_jobs=-1, cv=3)\n",
    "mlp_clf_grid.fit(training_df[X_cols],training_df[y_col])\n",
    "\n",
    "mlp_scaled_best_params = mlp_clf_grid.best_params_\n",
    "print(\"MLP Scaled Best Params found through grid search cv are:\")\n",
    "print(mlp_scaled_best_params)\n",
    "\n",
    "# Testing model\n",
    "mlp_acc_grid,mlp_f1_grid = test_model_metrics(mlp_clf_grid,\"MLP-Gridsearch\",testing_df[X_cols],testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    mlp_clf_grid,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"MLP Confusion Matrix-Gridsearch\")\n",
    "\n",
    "print(\"MLP Confusion Matrix-Gridsearch\")\n",
    "print(disp.confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e6fc57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'hidden_layer_sizes': (10, 10, 10),\n",
       " 'learning_rate': 'constant'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_scaled_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c55a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP Scaled Best Params found through grid search cv are:\n",
    "{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f689cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'age_limit',\n",
       " 'view_count',\n",
       " 'like_count',\n",
       " 'view_like_ratio',\n",
       " 'is_comments_enabled',\n",
       " 'is_live_content',\n",
       " 'cat_codes',\n",
       " 'neu',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'compound']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341b6b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ld_score_ohe'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d564e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'age_limit',\n",
       " 'view_count',\n",
       " 'like_count',\n",
       " 'view_like_ratio',\n",
       " 'is_comments_enabled',\n",
       " 'is_live_content',\n",
       " 'cat_codes',\n",
       " 'neu',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'compound',\n",
       " 'ld_score_ohe']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_related_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "5011b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with all required columns\n",
    "tab_df = training_df[all_related_cols].copy()\n",
    "\n",
    "def remap_target(value):\n",
    "    if value == -1:\n",
    "        return \"Negative\"\n",
    "    elif value == 0:\n",
    "        return \"Neutral\"\n",
    "    elif value == 1:\n",
    "        return \"Positive\"\n",
    "\n",
    "tab_df[\"ld_score_ohe\"] = tab_df[\"ld_score_ohe\"].apply(remap_target)\n",
    "\n",
    "# Create tabular object and dataloaders\n",
    "cat_names = ['age_limit','is_comments_enabled','is_live_content','cat_codes']\n",
    "cont_names = ['duration', 'view_count', 'like_count','view_like_ratio','neu','neg','pos','compound']\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "\n",
    "# Creates splits\n",
    "splits = RandomSplitter(valid_pct=0.1)(range_of(tab_df))\n",
    "\n",
    "to = TabularPandas(tab_df, procs=procs,\n",
    "                   cat_names = cat_names,\n",
    "                   cont_names = cont_names,\n",
    "                   y_names=y_col,\n",
    "                   y_block=CategoryBlock,\n",
    "                   splits=splits)\n",
    "\n",
    "dls = to.dataloaders(bs=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ea930b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted f1 score by modifying fastai f1 score\n",
    "# def F1Score(axis=-1, labels=None, pos_label=1, average='weighted', sample_weight=None):\n",
    "#     \"F1 score for single-label classification problems\"\n",
    "#     return skm_to_fastai(skm.f1_score, axis=axis,\n",
    "#                          labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4c735dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss_flat = CrossEntropyLossFlat()\n",
    "f1_score_fai = F1Score()\n",
    "f1_score_multi = F1ScoreMulti(average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a40ba7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tabular learner\n",
    "learn_tab = tabular_learner(dls, layers=[100,200,100], metrics=accuracy,loss_func=cross_entropy_loss_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9a7bda29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.0008317637839354575)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fastai has a useful function to estimate the best learning rate to use.\n",
    "learn_tab.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "444fa81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.537413</td>\n",
       "      <td>0.770312</td>\n",
       "      <td>0.726088</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.387218</td>\n",
       "      <td>1.062577</td>\n",
       "      <td>0.732909</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.352577</td>\n",
       "      <td>1.069417</td>\n",
       "      <td>0.718311</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.318735</td>\n",
       "      <td>1.123301</td>\n",
       "      <td>0.733351</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.308855</td>\n",
       "      <td>0.992836</td>\n",
       "      <td>0.732296</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the data. We are not fine-tuning here. We are learning from our training data alone.\n",
    "learn_tab.fit_one_cycle(5,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c8de9669",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_test_df = testing_df[X_cols].copy()\n",
    "tab_test_dl = learn_tab.dls.test_dl(tab_test_df,ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1702817f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds, _, decoded = learn_tab.get_preds(dl=tab_test_dl, with_decoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "9d4a12ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2,  ..., 2, 2, 2])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "fc43f174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "bb2f6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded[decoded == 0] = -1\n",
    "decoded[decoded == 1] = 0\n",
    "decoded[decoded == 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f633f371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1,  0,  1])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f2d49b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6423349568038629"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(testing_df[y_col],decoded,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c28c1",
   "metadata": {},
   "source": [
    "### HuggingFace Transformer\n",
    "We were interested if formatting the inputs as a string and put through a transformer would work.\n",
    "Based on tutorials by Jeremy Howard: https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "225ef10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 233 ms, sys: 11 ms, total: 244 ms\n",
      "Wall time: 246 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings,transformers,logging,torch\n",
    "from transformers import TrainingArguments,Trainer\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d57428db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dad466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model_name = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c91fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11587375eb024344afd94b8cfa68bd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(dl_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb37f01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep = tokz.sep_token\n",
    "sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2991d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fetch_date</th>\n",
       "      <th>uploader</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>title</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>category</th>\n",
       "      <th>duration</th>\n",
       "      <th>age_limit</th>\n",
       "      <th>view_count</th>\n",
       "      <th>...</th>\n",
       "      <th>desc_neu</th>\n",
       "      <th>desc_neg</th>\n",
       "      <th>desc_pos</th>\n",
       "      <th>desc_compound</th>\n",
       "      <th>video_id</th>\n",
       "      <th>votes</th>\n",
       "      <th>comment_neg</th>\n",
       "      <th>comment_neu</th>\n",
       "      <th>comment_pos</th>\n",
       "      <th>comment_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FyDBw6Cq6-4</td>\n",
       "      <td>2019-02-03 05:06:14</td>\n",
       "      <td>ЛюбительМира</td>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>WARFACE: КЛАН ХэвиРэйн МЫ ВОШЛИ В ТОП 10 СЕРВЕРА АЛЬФА✅</td>\n",
       "      <td>поддержка каналаговорящий донат от 20р заказ музыки от 100р httpdonatepayrudlyubitelmira начни играть в warface с донатом бесплатно httpfasst9ejljd золотой кейс за регистрацию в warface  httpfasstxvuge1 комплект оружия неон и vip ускоритель   httpfasstlm4qzwздравствуй дорогой зритель я рад тебя видеть меня зовут геворг ниже есть много информация про меня и не только добра тебе будь здороваприятного просмотраконтакты авторая вконтакте httpsvkcomid182830840моя группа vk httpsvkcomwarfaceloveworldгруппа моего клана в вк httpsvkcomwhitesharkrgwмой instagram httpswwwinstagramcomgevkaramyanразбл...</td>\n",
       "      <td>Gaming</td>\n",
       "      <td>6852</td>\n",
       "      <td>0</td>\n",
       "      <td>3380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id           fetch_date      uploader upload_date  \\\n",
       "0  FyDBw6Cq6-4  2019-02-03 05:06:14  ЛюбительМира  2018-11-23   \n",
       "\n",
       "                                                     title  \\\n",
       "0  WARFACE: КЛАН ХэвиРэйн МЫ ВОШЛИ В ТОП 10 СЕРВЕРА АЛЬФА✅   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 desc_text  \\\n",
       "0  поддержка каналаговорящий донат от 20р заказ музыки от 100р httpdonatepayrudlyubitelmira начни играть в warface с донатом бесплатно httpfasst9ejljd золотой кейс за регистрацию в warface  httpfasstxvuge1 комплект оружия неон и vip ускоритель   httpfasstlm4qzwздравствуй дорогой зритель я рад тебя видеть меня зовут геворг ниже есть много информация про меня и не только добра тебе будь здороваприятного просмотраконтакты авторая вконтакте httpsvkcomid182830840моя группа vk httpsvkcomwarfaceloveworldгруппа моего клана в вк httpsvkcomwhitesharkrgwмой instagram httpswwwinstagramcomgevkaramyanразбл...   \n",
       "\n",
       "  category  duration  age_limit  view_count  ...  desc_neu  desc_neg  \\\n",
       "0   Gaming      6852          0        3380  ...      0.97      0.00   \n",
       "\n",
       "   desc_pos  desc_compound  video_id  votes  comment_neg  comment_neu  \\\n",
       "0      0.03           0.51         0   0.00         0.00         0.00   \n",
       "\n",
       "   comment_pos  comment_compound  \n",
       "0         0.00              0.00  \n",
       "\n",
       "[1 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43330d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'age_limit',\n",
       " 'view_count',\n",
       " 'like_count',\n",
       " 'view_like_ratio',\n",
       " 'is_comments_enabled',\n",
       " 'is_live_content',\n",
       " 'cat_codes',\n",
       " 'desc_compound',\n",
       " 'comment_compound',\n",
       " 'votes']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84930e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>view_like_ratio</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1611</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    view_like_ratio  view_count  like_count\n",
       "4              0.00          11           0\n",
       "6              0.00         310           0\n",
       "11             0.00          54           0\n",
       "13             0.00        1346           0\n",
       "28             0.00        1611           0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df[training_df[\"view_like_ratio\"].round(2) < 1][[\"view_like_ratio\",\"view_count\",\"like_count\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1df921b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('int64'), dtype('<U5')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m \u001b[38;5;241m+\u001b[39m training_df\u001b[38;5;241m.\u001b[39mage_limit \u001b[38;5;241m+\u001b[39m sep \u001b[38;5;241m+\u001b[39m training_df\u001b[38;5;241m.\u001b[39mview_count \u001b[38;5;241m+\u001b[39m sep \u001b[38;5;241m+\u001b[39m training_df\u001b[38;5;241m.\u001b[39mlike_count \u001b[38;5;241m+\u001b[39m sep \u001b[38;5;241m+\u001b[39m training_df\u001b[38;5;241m.\u001b[39mview_like_ratio \u001b[38;5;241m+\u001b[39m sep \u001b[38;5;241m+\u001b[39m training_df\u001b[38;5;241m.\u001b[39mis_comments_enabled \u001b[38;5;241m+\u001b[39m sep \u001b[38;5;241m+\u001b[39m training_df\u001b[38;5;241m.\u001b[39mis_live_content \u001b[38;5;241m+\u001b[39m sep \u001b[38;5;241m+\u001b[39m training_df\u001b[38;5;241m.\u001b[39mcat_codes \u001b[38;5;241m+\u001b[39m sep \u001b[38;5;241m+\u001b[39m training_df\u001b[38;5;241m.\u001b[39mdesc_compound \u001b[38;5;241m+\u001b[39m sep \u001b[38;5;241m+\u001b[39m training_df\u001b[38;5;241m.\u001b[39mcomment_compound \u001b[38;5;241m+\u001b[39m sep \u001b[38;5;241m+\u001b[39m training_df\u001b[38;5;241m.\u001b[39mvotes\n",
      "File \u001b[0;32m~/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/pandas/core/ops/common.py:70\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     68\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/pandas/core/arraylike.py:100\u001b[0m, in \u001b[0;36mOpsMixin.__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__add__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/pandas/core/series.py:5639\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[1;32m   5638\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39malign_method_SERIES(\u001b[38;5;28mself\u001b[39m, other)\n\u001b[0;32m-> 5639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/pandas/core/base.py:1295\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1292\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(rvalues)\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1295\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:222\u001b[0m, in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# TODO we should handle EAs consistently and move this check before the if/else\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# (https://github.com/pandas-dev/pandas/issues/41165)\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     _bool_arith_check(op, left, right)\n\u001b[0;32m--> 222\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[0;32m~/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:163\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    160\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(expressions\u001b[38;5;241m.\u001b[39mevaluate, op)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (is_object_dtype(left\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m is_object_dtype(right)):\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;66;03m# For object dtype, fallback to a masked operation (only operating\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;66;03m#  on the non-missing values)\u001b[39;00m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('int64'), dtype('<U5')) -> None"
     ]
    }
   ],
   "source": [
    "training_df['inputs'] = training_df.duration.str + sep + training_df.age_limit.str + sep + training_df.view_count + sep + training_df.like_count + sep + training_df.view_like_ratio + sep + training_df.is_comments_enabled + sep + training_df.is_live_content + sep + training_df.cat_codes + sep + training_df.desc_compound + sep + training_df.comment_compound + sep + training_df.votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ea3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
