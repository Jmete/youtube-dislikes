{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983eb787",
   "metadata": {},
   "source": [
    "# Model Training & Testing\n",
    "This notebook is intended to help train and test various models, and expanded from our data_model_prep notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4f1e0a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "random_state = 42\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb1f885",
   "metadata": {},
   "source": [
    "## Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5588aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For python pipeline that will be run from root folder of project\n",
    "# ROOT_DIR = os.path.abspath(os.curdir)\n",
    "\n",
    "# # Pickle save paths\n",
    "# training_df_path = os.path.join(ROOT_DIR,\"data/processed/training_df.pkl\")\n",
    "# testing_df_path = os.path.join(ROOT_DIR,\"data/processed/testing_df.pkl\")\n",
    "\n",
    "# Relative path for notebook\n",
    "training_df_path = \"../data/processed/training_df.pkl\"\n",
    "testing_df_path = \"../data/processed/testing_df.pkl\"\n",
    "\n",
    "# Small df for testing\n",
    "training_df_small_path = \"../data/processed/training_df_small.pkl\"\n",
    "# Big df for testing\n",
    "randompct10_pklpath=r\"/run/user/1000/gvfs/smb-share:server=metebox,share=data/JAMES/datasets/youtube-meta/youtube-02-2019-dump/randompct_df_10.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af712c5",
   "metadata": {},
   "source": [
    "## Load training and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "88940982",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_pickle(training_df_path)\n",
    "testing_df = pd.read_pickle(testing_df_path)\n",
    "\n",
    "# big df\n",
    "# training_df_big = pd.read_pickle(randompct10_pklpath)\n",
    "\n",
    "# small df\n",
    "# training_df = pd.read_pickle(training_df_small_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17829d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat training df and training_df_big for testing if more data helps\n",
    "# training_df = pd.concat([training_df,training_df_big])\n",
    "# training_df = training_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe9d64",
   "metadata": {},
   "source": [
    "## Columns for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "283c877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on what we can get at inference time from the Youtube API or scraping\n",
    "X_cols = [\n",
    "    \"duration\",\n",
    "    \"age_limit\",\n",
    "    \"view_count\",\n",
    "    \"like_count\",\n",
    "    \"view_like_ratio\",\n",
    "    \"is_comments_enabled\",\n",
    "    \"is_live_content\",\n",
    "    \"cat_codes\",\n",
    "    \"desc_neu\",\n",
    "    \"desc_neg\",\n",
    "    \"desc_pos\",\n",
    "    \"desc_compound\",\n",
    "    \"comment_neu\",\n",
    "    \"comment_neg\",\n",
    "    \"comment_pos\",\n",
    "    \"comment_compound\",\n",
    "    \"votes\"\n",
    "]\n",
    "\n",
    "y_col = \"ld_score_ohe\"\n",
    "\n",
    "# Get all related columns - useful for fastai models\n",
    "all_related_cols = X_cols.copy()\n",
    "all_related_cols.append(y_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3bdaedab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing scaled versions\n",
    "scaler = StandardScaler()\n",
    "training_df_scaled_X = scaler.fit_transform(training_df[X_cols])\n",
    "testing_df_scaled_X = scaler.transform(testing_df[X_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2159ff32",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "60cb281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_test_sets(df,X_cols,y_col,random_state=None):\n",
    "    \"\"\"\n",
    "    Takes in a processed dataframe and splits it into appropriate training and test splits.\n",
    "    \"\"\"\n",
    "    X = df[X_cols]\n",
    "    y = df[y_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state=random_state)\n",
    "    print(\"Splits created\")\n",
    "    return X, y, X_train, X_test, y_train,y_test\n",
    "\n",
    "def train_model(clf_object,model_name,X_train,y_train,X_test,y_test):\n",
    "    clf_object.fit(X_train,y_train)\n",
    "    acc, f1 = test_model_metrics(clf=clf_object,model_name=model_name)\n",
    "    return clf_object, acc, f1\n",
    "\n",
    "def test_model_metrics(clf, model_name,X_test,y_test):\n",
    "    testpreds = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test,testpreds)\n",
    "    \n",
    "    if len(y_test.unique()) > 2:\n",
    "        average = \"weighted\"\n",
    "    else:\n",
    "        average = \"binary\"\n",
    "        \n",
    "    f1 = f1_score(y_test,testpreds,average=average)\n",
    "    print(f\"{model_name} metrics:\")\n",
    "    print(f\"Accuracy Score: {acc}\")\n",
    "    print(f\"F1 score: {f1}\")\n",
    "    return acc,f1\n",
    "\n",
    "def cross_val_model(df,clf_object,X_cols,y_col,random_state,scoring,cv=5):\n",
    "    \"\"\"\n",
    "    Takes in a df, processes it, and then outputs a cross-validation f1 score.\n",
    "    Adapted from sklearn docs.\n",
    "    Scoring types available here: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    \"\"\"\n",
    "    X, y, X_train,X_test,y_train,y_test = create_training_test_sets(df,X_cols,y_col,random_state=random_state)\n",
    "    scores = cross_val_score(clf_object, X, y, cv=cv,scoring=scoring,verbose=1,n_jobs=-1)\n",
    "    print(f\"{scores.mean():0.2f} {scoring} with a standard deviation of {scores.std():0.2f}\")\n",
    "    return scores\n",
    "\n",
    "def confusion_matrix_model(df,clf_object,X_cols,y_col,random_state,model_name):\n",
    "    \"\"\"\n",
    "    Takes in a df, processes it, and then outputs a confusion matrix.\n",
    "    Adapted from sklearn docs: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    Scoring types available here: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    \"\"\"\n",
    "    X, y, X_train,X_test,y_train,y_test = create_training_test_sets(df,X_cols,y_col,random_state=random_state)\n",
    "    clf_object.fit(X_train,y_train)\n",
    "    \n",
    "    # Adapted from sklearn docs\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    titles_options = [\n",
    "        (f\"{model_name} Confusion matrix, without normalization\", None),\n",
    "        (f\"{model_name} Normalized confusion matrix\", \"true\"),\n",
    "    ]\n",
    "    \n",
    "    for title, normalize in titles_options:\n",
    "        disp = ConfusionMatrixDisplay.from_estimator(\n",
    "            clf_object,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            cmap=plt.cm.Blues,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "        disp.ax_.set_title(title)\n",
    "\n",
    "        print(title)\n",
    "        print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa59c5",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cb2059",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "038e2ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest metrics:\n",
      "Accuracy Score: 0.6354379070428453\n",
      "F1 score: 0.6568312929586023\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "rf_clf = RandomForestClassifier(n_jobs=-1,random_state=random_state)\n",
    "rf_clf.fit(training_df[X_cols],training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "rf_acc,rf_f1 = test_model_metrics(rf_clf,\"Random Forest\",testing_df[X_cols],testing_df[y_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d9dedc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Confusion Matrix\n",
      "[[0.85347738 0.04456448 0.10195814]\n",
      " [0.40173651 0.15784548 0.44041801]\n",
      " [0.32229689 0.00619658 0.67150653]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    rf_clf,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"RF Confusion Matrix\")\n",
    "\n",
    "print(\"RF Confusion Matrix\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4a934d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest-Scaled metrics:\n",
      "Accuracy Score: 0.6354502650798947\n",
      "F1 score: 0.6571003716766837\n",
      "RF Confusion Matrix-Scaled\n",
      "[[0.85428764 0.04532973 0.10038262]\n",
      " [0.39851305 0.16106894 0.44041801]\n",
      " [0.32248793 0.00665343 0.67085863]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training model - scaled\n",
    "rf_clf = RandomForestClassifier(n_jobs=-1,random_state=random_state)\n",
    "rf_clf.fit(training_df_scaled_X,training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "rf_acc,rf_f1 = test_model_metrics(rf_clf,\"Random Forest-Scaled\",testing_df_scaled_X,testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    rf_clf,\n",
    "    testing_df_scaled_X,\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"RF Confusion Matrix-Scaled\")\n",
    "\n",
    "print(\"RF Confusion Matrix-Scaled\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f467b24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest metrics:\n",
      "Accuracy Score: 0.620756559028164\n",
      "F1 score: 0.6450262347616782\n",
      "RF Confusion Matrix\n",
      "[[0.87711006 0.03799235 0.08489759]\n",
      " [0.45825101 0.15129458 0.3904544 ]\n",
      " [0.32828581 0.02325794 0.64845625]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_weight_dict = {-1:0.5,0:2,1:4}\n",
    "\n",
    "# Training model\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_jobs=-1,\n",
    "    random_state=random_state,\n",
    "    class_weight = class_weight_dict,\n",
    "#     max_depth=64,\n",
    "# The below are good without class weights\n",
    "#     min_samples_split=0.01,\n",
    "#     min_samples_leaf=0.05,\n",
    "#     max_depth=8,\n",
    "    )\n",
    "rf_clf.fit(training_df[X_cols],training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "rf_acc,rf_f1 = test_model_metrics(rf_clf,\"Random Forest\",testing_df[X_cols],testing_df[y_col])\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    rf_clf,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"RF Confusion Matrix\")\n",
    "\n",
    "print(\"RF Confusion Matrix\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cfb58b",
   "metadata": {},
   "source": [
    "### GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8731796e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM metrics:\n",
      "Accuracy Score: 0.6438537302734834\n",
      "F1 score: 0.6637921047694253\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "# Params Based on previous gridsearch cvs\n",
    "gbm_model = lgb.LGBMClassifier(learning_rate=0.05,\n",
    "                               max_depth=20,\n",
    "                               min_child_samples=15,\n",
    "                               num_leaves=100,\n",
    "                               reg_alpha=0.03,\n",
    "                               random_state=random_state)\n",
    "gbm_model.fit(training_df[X_cols],training_df[y_col], verbose=20,eval_metric='logloss')\n",
    "\n",
    "# Testing model\n",
    "gbm_acc,gbm_f1 = test_model_metrics(gbm_model,\"GBM\",testing_df[X_cols],testing_df[y_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4d47cd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM Confusion Matrix\n",
      "[[0.8433941  0.04866081 0.10794508]\n",
      " [0.36435479 0.16689196 0.46875325]\n",
      " [0.31577636 0.00098846 0.68323518]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    gbm_model,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"GBM Confusion Matrix\")\n",
    "\n",
    "print(\"GBM Confusion Matrix\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9ce7b498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM-Scaled metrics:\n",
      "Accuracy Score: 0.6428465502539576\n",
      "F1 score: 0.6624043007780998\n",
      "GBM Confusion Matrix-Scaled\n",
      "[[0.84848076 0.04532973 0.10618951]\n",
      " [0.37516897 0.15961318 0.46521784]\n",
      " [0.31679805 0.00109645 0.68210551]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training model - scaled version\n",
    "# Params Based on previous gridsearch cvs\n",
    "gbm_model = lgb.LGBMClassifier(learning_rate=0.05,\n",
    "                               max_depth=20,\n",
    "                               min_child_samples=15,\n",
    "                               num_leaves=100,\n",
    "                               reg_alpha=0.03,\n",
    "                               random_state=random_state)\n",
    "gbm_model.fit(training_df_scaled_X,training_df[y_col], verbose=20,eval_metric='logloss')\n",
    "\n",
    "# Testing model\n",
    "gbm_acc,gbm_f1 = test_model_metrics(gbm_model,\"GBM-Scaled\",testing_df_scaled_X,testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    gbm_model,\n",
    "    testing_df_scaled_X,\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"GBM Confusion Matrix-Scaled\")\n",
    "\n",
    "print(\"GBM Confusion Matrix-Scaled\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b334408",
   "metadata": {},
   "source": [
    "### Neural Net\n",
    "We will use sklearn mlp and fastai to create a tabular learner neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4c1446d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.70618514\n",
      "Iteration 2, loss = 4.44843135\n",
      "Iteration 3, loss = 3.58985838\n",
      "Iteration 4, loss = 3.98914983\n",
      "Iteration 5, loss = 3.66353547\n",
      "Iteration 6, loss = 4.14276647\n",
      "Iteration 7, loss = 3.70163269\n",
      "Iteration 8, loss = 3.95303072\n",
      "Iteration 9, loss = 3.62451997\n",
      "Iteration 10, loss = 3.61586343\n",
      "Iteration 11, loss = 3.41870720\n",
      "Iteration 12, loss = 3.37889542\n",
      "Iteration 13, loss = 3.43523543\n",
      "Iteration 14, loss = 3.59331089\n",
      "Iteration 15, loss = 3.74510396\n",
      "Iteration 16, loss = 3.81809355\n",
      "Iteration 17, loss = 3.53678291\n",
      "Iteration 18, loss = 3.95099977\n",
      "Iteration 19, loss = 3.40904663\n",
      "Iteration 20, loss = 3.29771042\n",
      "Iteration 21, loss = 3.59571792\n",
      "Iteration 22, loss = 3.65805221\n",
      "Iteration 23, loss = 3.23437949\n",
      "Iteration 24, loss = 3.17895517\n",
      "Iteration 25, loss = 3.34985148\n",
      "Iteration 26, loss = 3.36520237\n",
      "Iteration 27, loss = 2.80226852\n",
      "Iteration 28, loss = 3.07042591\n",
      "Iteration 29, loss = 3.09803314\n",
      "Iteration 30, loss = 3.24287249\n",
      "Iteration 31, loss = 3.02317600\n",
      "Iteration 32, loss = 2.99758577\n",
      "Iteration 33, loss = 2.91007648\n",
      "Iteration 34, loss = 3.18699611\n",
      "Iteration 35, loss = 3.15060781\n",
      "Iteration 36, loss = 3.03311185\n",
      "Iteration 37, loss = 3.18734681\n",
      "Iteration 38, loss = 3.00010253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP metrics:\n",
      "Accuracy Score: 0.6614824701244454\n",
      "F1 score: 0.6740013082752425\n",
      "MLP Confusion Matrix\n",
      "[[0.8806212  0.03992798 0.07945082]\n",
      " [0.53608194 0.09082874 0.37308932]\n",
      " [0.26014005 0.02764372 0.71221623]]\n",
      "CPU times: user 17min 39s, sys: 40.7 s, total: 18min 20s\n",
      "Wall time: 3min 3s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# Starting with Sklearn MLP\n",
    "# Training model\n",
    "mlp_clf = MLPClassifier(random_state=random_state, max_iter=300, verbose=2)\n",
    "mlp_clf.fit(training_df[X_cols],training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "mlp_acc,mlp_f1 = test_model_metrics(mlp_clf,\"MLP\",testing_df[X_cols],testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    mlp_clf,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"MLP Confusion Matrix\")\n",
    "\n",
    "print(\"MLP Confusion Matrix\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05575d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_target(value):\n",
    "    if value == -1:\n",
    "        return \"Negative\"\n",
    "    elif value == 0:\n",
    "        return \"Neutral\"\n",
    "    elif value == 1:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "413db6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'hidden_layer_sizes': (10, 10, 10),\n",
       " 'learning_rate': 'constant'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_scaled_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db6549",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ca93d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.98394474\n",
      "Iteration 2, loss = 0.60136631\n",
      "Iteration 3, loss = 0.48013982\n",
      "Iteration 4, loss = 0.42686003\n",
      "Iteration 5, loss = 0.40203085\n",
      "Iteration 6, loss = 0.35834747\n",
      "Iteration 7, loss = 0.37245060\n",
      "Iteration 8, loss = 0.36442998\n",
      "Iteration 9, loss = 0.35226515\n",
      "Iteration 10, loss = 0.34701592\n",
      "Iteration 11, loss = 0.34784189\n",
      "Iteration 12, loss = 0.35255896\n",
      "Iteration 13, loss = 0.35846632\n",
      "Iteration 14, loss = 0.33597485\n",
      "Iteration 15, loss = 0.34080617\n",
      "Iteration 16, loss = 0.36363319\n",
      "Iteration 17, loss = 0.35360933\n",
      "Iteration 18, loss = 0.33881292\n",
      "Iteration 19, loss = 0.35300106\n",
      "Iteration 20, loss = 0.33102447\n",
      "Iteration 21, loss = 0.33878282\n",
      "Iteration 22, loss = 0.33162479\n",
      "Iteration 23, loss = 0.34219974\n",
      "Iteration 24, loss = 0.33647438\n",
      "Iteration 25, loss = 0.32818195\n",
      "Iteration 26, loss = 0.33174123\n",
      "Iteration 27, loss = 0.32838274\n",
      "Iteration 28, loss = 0.33957324\n",
      "Iteration 29, loss = 0.34177609\n",
      "Iteration 30, loss = 0.33483412\n",
      "Iteration 31, loss = 0.34416708\n",
      "Iteration 32, loss = 0.34183918\n",
      "Iteration 33, loss = 0.32187309\n",
      "Iteration 34, loss = 0.31894404\n",
      "Iteration 35, loss = 0.33053451\n",
      "Iteration 36, loss = 0.33621877\n",
      "Iteration 37, loss = 0.32471525\n",
      "Iteration 38, loss = 0.33104618\n",
      "Iteration 39, loss = 0.34592787\n",
      "Iteration 40, loss = 0.32740385\n",
      "Iteration 41, loss = 0.32552364\n",
      "Iteration 42, loss = 0.32936861\n",
      "Iteration 43, loss = 0.33186093\n",
      "Iteration 44, loss = 0.34116406\n",
      "Iteration 45, loss = 0.32939756\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP-BestParams metrics:\n",
      "Accuracy Score: 0.823984478305466\n",
      "F1 score: 0.7772226295935762\n",
      "MLP Confusion Matrix-BestParams\n",
      "[[0.87436417 0.         0.12563583]\n",
      " [0.39825309 0.         0.60174691]\n",
      " [0.05366769 0.         0.94633231]]\n",
      "CPU times: user 1min 44s, sys: 133 ms, total: 1min 44s\n",
      "Wall time: 1min 42s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# Starting with Sklearn MLP\n",
    "# Training model\n",
    "mlp_clf = MLPClassifier(random_state=random_state, \n",
    "                        max_iter=300, \n",
    "                        verbose=2,\n",
    "                        activation=\"relu\",\n",
    "                        hidden_layer_sizes=(5,3,),\n",
    "                        learning_rate=\"adaptive\",\n",
    "                        alpha=0.0001)\n",
    "\n",
    "mlp_clf.fit(training_df[X_cols],training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "mlp_acc,mlp_f1 = test_model_metrics(mlp_clf,\"MLP-BestParams\",testing_df[X_cols],testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    mlp_clf,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"MLP Confusion Matrix-BestParams\")\n",
    "\n",
    "print(\"MLP Confusion Matrix-BestParams\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e4f7a",
   "metadata": {},
   "source": [
    "#### MLP with Scaled X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1007ebb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.58122594\n",
      "Iteration 2, loss = 0.47999104\n",
      "Iteration 3, loss = 0.46225984\n",
      "Iteration 4, loss = 0.45358690\n",
      "Iteration 5, loss = 0.44726086\n",
      "Iteration 6, loss = 0.43841673\n",
      "Iteration 7, loss = 0.42764830\n",
      "Iteration 8, loss = 0.42251626\n",
      "Iteration 9, loss = 0.41629883\n",
      "Iteration 10, loss = 0.40817802\n",
      "Iteration 11, loss = 0.39966268\n",
      "Iteration 12, loss = 0.39181279\n",
      "Iteration 13, loss = 0.38570109\n",
      "Iteration 14, loss = 0.38130141\n",
      "Iteration 15, loss = 0.37747817\n",
      "Iteration 16, loss = 0.37418093\n",
      "Iteration 17, loss = 0.37088841\n",
      "Iteration 18, loss = 0.36722427\n",
      "Iteration 19, loss = 0.36402013\n",
      "Iteration 20, loss = 0.36077295\n",
      "Iteration 21, loss = 0.35742218\n",
      "Iteration 22, loss = 0.35323337\n",
      "Iteration 23, loss = 0.35099195\n",
      "Iteration 24, loss = 0.34915468\n",
      "Iteration 25, loss = 0.34728271\n",
      "Iteration 26, loss = 0.34641326\n",
      "Iteration 27, loss = 0.34460953\n",
      "Iteration 28, loss = 0.34307582\n",
      "Iteration 29, loss = 0.34301996\n",
      "Iteration 30, loss = 0.34108510\n",
      "Iteration 31, loss = 0.33956773\n",
      "Iteration 32, loss = 0.33984120\n",
      "Iteration 33, loss = 0.33767803\n",
      "Iteration 34, loss = 0.33637332\n",
      "Iteration 35, loss = 0.33508571\n",
      "Iteration 36, loss = 0.33386875\n",
      "Iteration 37, loss = 0.33350224\n",
      "Iteration 38, loss = 0.33169924\n",
      "Iteration 39, loss = 0.33112600\n",
      "Iteration 40, loss = 0.33001043\n",
      "Iteration 41, loss = 0.32818630\n",
      "Iteration 42, loss = 0.32886329\n",
      "Iteration 43, loss = 0.32659306\n",
      "Iteration 44, loss = 0.32666103\n",
      "Iteration 45, loss = 0.32696779\n",
      "Iteration 46, loss = 0.32625741\n",
      "Iteration 47, loss = 0.32484428\n",
      "Iteration 48, loss = 0.32502605\n",
      "Iteration 49, loss = 0.32641416\n",
      "Iteration 50, loss = 0.32353307\n",
      "Iteration 51, loss = 0.32351044\n",
      "Iteration 52, loss = 0.32387346\n",
      "Iteration 53, loss = 0.32395187\n",
      "Iteration 54, loss = 0.32252973\n",
      "Iteration 55, loss = 0.32060273\n",
      "Iteration 56, loss = 0.32196902\n",
      "Iteration 57, loss = 0.32150970\n",
      "Iteration 58, loss = 0.31971146\n",
      "Iteration 59, loss = 0.32000740\n",
      "Iteration 60, loss = 0.32027955\n",
      "Iteration 61, loss = 0.32056681\n",
      "Iteration 62, loss = 0.31898262\n",
      "Iteration 63, loss = 0.31933776\n",
      "Iteration 64, loss = 0.31927800\n",
      "Iteration 65, loss = 0.32009310\n",
      "Iteration 66, loss = 0.31927849\n",
      "Iteration 67, loss = 0.31986852\n",
      "Iteration 68, loss = 0.31767235\n",
      "Iteration 69, loss = 0.31962895\n",
      "Iteration 70, loss = 0.31900928\n",
      "Iteration 71, loss = 0.31967869\n",
      "Iteration 72, loss = 0.31720191\n",
      "Iteration 73, loss = 0.31854126\n",
      "Iteration 74, loss = 0.31808832\n",
      "Iteration 75, loss = 0.31807955\n",
      "Iteration 76, loss = 0.31833251\n",
      "Iteration 77, loss = 0.31855079\n",
      "Iteration 78, loss = 0.31681986\n",
      "Iteration 79, loss = 0.31839560\n",
      "Iteration 80, loss = 0.31745213\n",
      "Iteration 81, loss = 0.31769416\n",
      "Iteration 82, loss = 0.31858354\n",
      "Iteration 83, loss = 0.31819926\n",
      "Iteration 84, loss = 0.31680325\n",
      "Iteration 85, loss = 0.31827931\n",
      "Iteration 86, loss = 0.31593085\n",
      "Iteration 87, loss = 0.31608116\n",
      "Iteration 88, loss = 0.31510215\n",
      "Iteration 89, loss = 0.31488565\n",
      "Iteration 90, loss = 0.31528065\n",
      "Iteration 91, loss = 0.31442312\n",
      "Iteration 92, loss = 0.31103005\n",
      "Iteration 93, loss = 0.31035601\n",
      "Iteration 94, loss = 0.31058461\n",
      "Iteration 95, loss = 0.30875218\n",
      "Iteration 96, loss = 0.30957883\n",
      "Iteration 97, loss = 0.30862642\n",
      "Iteration 98, loss = 0.30928569\n",
      "Iteration 99, loss = 0.31018770\n",
      "Iteration 100, loss = 0.30750615\n",
      "Iteration 101, loss = 0.30823542\n",
      "Iteration 102, loss = 0.30744204\n",
      "Iteration 103, loss = 0.30822252\n",
      "Iteration 104, loss = 0.30769598\n",
      "Iteration 105, loss = 0.30665420\n",
      "Iteration 106, loss = 0.30669712\n",
      "Iteration 107, loss = 0.30703386\n",
      "Iteration 108, loss = 0.30748392\n",
      "Iteration 109, loss = 0.30701385\n",
      "Iteration 110, loss = 0.30705066\n",
      "Iteration 111, loss = 0.30617901\n",
      "Iteration 112, loss = 0.30666778\n",
      "Iteration 113, loss = 0.30583025\n",
      "Iteration 114, loss = 0.30574787\n",
      "Iteration 115, loss = 0.30603491\n",
      "Iteration 116, loss = 0.30586065\n",
      "Iteration 117, loss = 0.30557681\n",
      "Iteration 118, loss = 0.30544535\n",
      "Iteration 119, loss = 0.30511474\n",
      "Iteration 120, loss = 0.30651702\n",
      "Iteration 121, loss = 0.30436220\n",
      "Iteration 122, loss = 0.30535416\n",
      "Iteration 123, loss = 0.30550413\n",
      "Iteration 124, loss = 0.30493584\n",
      "Iteration 125, loss = 0.30519462\n",
      "Iteration 126, loss = 0.30394618\n",
      "Iteration 127, loss = 0.30513828\n",
      "Iteration 128, loss = 0.30328243\n",
      "Iteration 129, loss = 0.30475422\n",
      "Iteration 130, loss = 0.30346441\n",
      "Iteration 131, loss = 0.30513641\n",
      "Iteration 132, loss = 0.30326562\n",
      "Iteration 133, loss = 0.30335130\n",
      "Iteration 134, loss = 0.30380836\n",
      "Iteration 135, loss = 0.30530089\n",
      "Iteration 136, loss = 0.30328279\n",
      "Iteration 137, loss = 0.30259740\n",
      "Iteration 138, loss = 0.30287442\n",
      "Iteration 139, loss = 0.30139100\n",
      "Iteration 140, loss = 0.30296066\n",
      "Iteration 141, loss = 0.30255840\n",
      "Iteration 142, loss = 0.30329935\n",
      "Iteration 143, loss = 0.30261800\n",
      "Iteration 144, loss = 0.30159439\n",
      "Iteration 145, loss = 0.30254870\n",
      "Iteration 146, loss = 0.30225065\n",
      "Iteration 147, loss = 0.30250145\n",
      "Iteration 148, loss = 0.30348550\n",
      "Iteration 149, loss = 0.30119282\n",
      "Iteration 150, loss = 0.30264814\n",
      "Iteration 151, loss = 0.30106864\n",
      "Iteration 152, loss = 0.30142316\n",
      "Iteration 153, loss = 0.30232438\n",
      "Iteration 154, loss = 0.30227519\n",
      "Iteration 155, loss = 0.30114371\n",
      "Iteration 156, loss = 0.30187401\n",
      "Iteration 157, loss = 0.30174245\n",
      "Iteration 158, loss = 0.30153224\n",
      "Iteration 159, loss = 0.30171177\n",
      "Iteration 160, loss = 0.30129484\n",
      "Iteration 161, loss = 0.30054106\n",
      "Iteration 162, loss = 0.30183795\n",
      "Iteration 163, loss = 0.30195677\n",
      "Iteration 164, loss = 0.30116273\n",
      "Iteration 165, loss = 0.30102756\n",
      "Iteration 166, loss = 0.30192354\n",
      "Iteration 167, loss = 0.30097963\n",
      "Iteration 168, loss = 0.30135132\n",
      "Iteration 169, loss = 0.30064222\n",
      "Iteration 170, loss = 0.30063607\n",
      "Iteration 171, loss = 0.30056947\n",
      "Iteration 172, loss = 0.30198251\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP-BestParams-Scaled metrics:\n",
      "Accuracy Score: 0.7717594137347223\n",
      "F1 score: 0.7324512205490494\n",
      "MLP Confusion Matrix-BestParams-Scaled\n",
      "[[0.73810488 0.         0.26189512]\n",
      " [0.38234377 0.         0.61765623]\n",
      " [0.09872995 0.         0.90127005]]\n",
      "CPU times: user 5min 1s, sys: 245 ms, total: 5min 1s\n",
      "Wall time: 5min 1s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# Starting with Sklearn MLP\n",
    "# Training model\n",
    "mlp_clf = MLPClassifier(random_state=random_state, \n",
    "                        max_iter=300, \n",
    "                        verbose=2,\n",
    "                        activation=\"relu\",\n",
    "                        hidden_layer_sizes=(5,3,),\n",
    "                        learning_rate=\"adaptive\",\n",
    "                        alpha=0.0001)\n",
    "\n",
    "mlp_clf.fit(training_df_scaled_X,training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "mlp_acc,mlp_f1 = test_model_metrics(mlp_clf,\"MLP-BestParams-Scaled\",testing_df_scaled_X,testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    mlp_clf,\n",
    "    testing_df_scaled_X,\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"MLP Confusion Matrix-BestParams-Scaled\")\n",
    "\n",
    "print(\"MLP Confusion Matrix-BestParams-Scaled\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6382c653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.52888149\n",
      "Iteration 2, loss = 0.45452839\n",
      "Iteration 3, loss = 0.43503824\n",
      "Iteration 4, loss = 0.42217512\n",
      "Iteration 5, loss = 0.41280090\n",
      "Iteration 6, loss = 0.40462974\n",
      "Iteration 7, loss = 0.39753161\n",
      "Iteration 8, loss = 0.38915297\n",
      "Iteration 9, loss = 0.38123902\n",
      "Iteration 10, loss = 0.37458984\n",
      "Iteration 11, loss = 0.36895252\n",
      "Iteration 12, loss = 0.36397631\n",
      "Iteration 13, loss = 0.35963411\n",
      "Iteration 14, loss = 0.35531498\n",
      "Iteration 15, loss = 0.35205756\n",
      "Iteration 16, loss = 0.34842535\n",
      "Iteration 17, loss = 0.34576676\n",
      "Iteration 18, loss = 0.34285251\n",
      "Iteration 19, loss = 0.34089546\n",
      "Iteration 20, loss = 0.33869156\n",
      "Iteration 21, loss = 0.33695216\n",
      "Iteration 22, loss = 0.33499959\n",
      "Iteration 23, loss = 0.33353915\n",
      "Iteration 24, loss = 0.33181305\n",
      "Iteration 25, loss = 0.32977335\n",
      "Iteration 26, loss = 0.32894643\n",
      "Iteration 27, loss = 0.32776113\n",
      "Iteration 28, loss = 0.32671537\n",
      "Iteration 29, loss = 0.32560771\n",
      "Iteration 30, loss = 0.32408620\n",
      "Iteration 31, loss = 0.32290429\n",
      "Iteration 32, loss = 0.32237581\n",
      "Iteration 33, loss = 0.32139370\n",
      "Iteration 34, loss = 0.32002606\n",
      "Iteration 35, loss = 0.31975339\n",
      "Iteration 36, loss = 0.31849274\n",
      "Iteration 37, loss = 0.31799886\n",
      "Iteration 38, loss = 0.31742785\n",
      "Iteration 39, loss = 0.31651666\n",
      "Iteration 40, loss = 0.31590620\n",
      "Iteration 41, loss = 0.31566462\n",
      "Iteration 42, loss = 0.31489282\n",
      "Iteration 43, loss = 0.31461642\n",
      "Iteration 44, loss = 0.31426660\n",
      "Iteration 45, loss = 0.31381502\n",
      "Iteration 46, loss = 0.31357520\n",
      "Iteration 47, loss = 0.31307796\n",
      "Iteration 48, loss = 0.31247214\n",
      "Iteration 49, loss = 0.31161227\n",
      "Iteration 50, loss = 0.31174415\n",
      "Iteration 51, loss = 0.31091615\n",
      "Iteration 52, loss = 0.31094916\n",
      "Iteration 53, loss = 0.31149827\n",
      "Iteration 54, loss = 0.31033933\n",
      "Iteration 55, loss = 0.31045071\n",
      "Iteration 56, loss = 0.31008013\n",
      "Iteration 57, loss = 0.30991602\n",
      "Iteration 58, loss = 0.30934253\n",
      "Iteration 59, loss = 0.30880318\n",
      "Iteration 60, loss = 0.30868098\n",
      "Iteration 61, loss = 0.30869055\n",
      "Iteration 62, loss = 0.30836068\n",
      "Iteration 63, loss = 0.30800431\n",
      "Iteration 64, loss = 0.30690231\n",
      "Iteration 65, loss = 0.30814661\n",
      "Iteration 66, loss = 0.30705528\n",
      "Iteration 67, loss = 0.30722602\n",
      "Iteration 68, loss = 0.30670784\n",
      "Iteration 69, loss = 0.30649609\n",
      "Iteration 70, loss = 0.30644363\n",
      "Iteration 71, loss = 0.30606570\n",
      "Iteration 72, loss = 0.30640294\n",
      "Iteration 73, loss = 0.30611454\n",
      "Iteration 74, loss = 0.30554937\n",
      "Iteration 75, loss = 0.30536798\n",
      "Iteration 76, loss = 0.30475647\n",
      "Iteration 77, loss = 0.30507828\n",
      "Iteration 78, loss = 0.30507615\n",
      "Iteration 79, loss = 0.30546442\n",
      "Iteration 80, loss = 0.30551854\n",
      "Iteration 81, loss = 0.30464310\n",
      "Iteration 82, loss = 0.30407571\n",
      "Iteration 83, loss = 0.30420274\n",
      "Iteration 84, loss = 0.30487480\n",
      "Iteration 85, loss = 0.30414891\n",
      "Iteration 86, loss = 0.30413849\n",
      "Iteration 87, loss = 0.30384598\n",
      "Iteration 88, loss = 0.30384827\n",
      "Iteration 89, loss = 0.30363723\n",
      "Iteration 90, loss = 0.30303959\n",
      "Iteration 91, loss = 0.30295677\n",
      "Iteration 92, loss = 0.30321859\n",
      "Iteration 93, loss = 0.30302828\n",
      "Iteration 94, loss = 0.30289775\n",
      "Iteration 95, loss = 0.30236895\n",
      "Iteration 96, loss = 0.30288141\n",
      "Iteration 97, loss = 0.30202122\n",
      "Iteration 98, loss = 0.30245049\n",
      "Iteration 99, loss = 0.30183711\n",
      "Iteration 100, loss = 0.30183750\n",
      "Iteration 101, loss = 0.30182555\n",
      "Iteration 102, loss = 0.30176326\n",
      "Iteration 103, loss = 0.30231682\n",
      "Iteration 104, loss = 0.30103099\n",
      "Iteration 105, loss = 0.30175281\n",
      "Iteration 106, loss = 0.30130672\n",
      "Iteration 107, loss = 0.30136934\n",
      "Iteration 108, loss = 0.30125336\n",
      "Iteration 109, loss = 0.30150064\n",
      "Iteration 110, loss = 0.30088800\n",
      "Iteration 111, loss = 0.30104147\n",
      "Iteration 112, loss = 0.30096487\n",
      "Iteration 113, loss = 0.30051331\n",
      "Iteration 114, loss = 0.30091037\n",
      "Iteration 115, loss = 0.30041917\n",
      "Iteration 116, loss = 0.30034547\n",
      "Iteration 117, loss = 0.30059148\n",
      "Iteration 118, loss = 0.30083908\n",
      "Iteration 119, loss = 0.29979175\n",
      "Iteration 120, loss = 0.29982565\n",
      "Iteration 121, loss = 0.29975100\n",
      "Iteration 122, loss = 0.29924385\n",
      "Iteration 123, loss = 0.29947310\n",
      "Iteration 124, loss = 0.29966863\n",
      "Iteration 125, loss = 0.29933657\n",
      "Iteration 126, loss = 0.29974274\n",
      "Iteration 127, loss = 0.29856663\n",
      "Iteration 128, loss = 0.29959955\n",
      "Iteration 129, loss = 0.29887989\n",
      "Iteration 130, loss = 0.29861118\n",
      "Iteration 131, loss = 0.29920393\n",
      "Iteration 132, loss = 0.29825620\n",
      "Iteration 133, loss = 0.29919310\n",
      "Iteration 134, loss = 0.29821654\n",
      "Iteration 135, loss = 0.29818867\n",
      "Iteration 136, loss = 0.29826318\n",
      "Iteration 137, loss = 0.29867544\n",
      "Iteration 138, loss = 0.29779693\n",
      "Iteration 139, loss = 0.29847433\n",
      "Iteration 140, loss = 0.29818635\n",
      "Iteration 141, loss = 0.29766970\n",
      "Iteration 142, loss = 0.29778160\n",
      "Iteration 143, loss = 0.29789199\n",
      "Iteration 144, loss = 0.29690438\n",
      "Iteration 145, loss = 0.29728909\n",
      "Iteration 146, loss = 0.29774769\n",
      "Iteration 147, loss = 0.29736654\n",
      "Iteration 148, loss = 0.29717979\n",
      "Iteration 149, loss = 0.29761501\n",
      "Iteration 150, loss = 0.29728422\n",
      "Iteration 151, loss = 0.29687847\n",
      "Iteration 152, loss = 0.29655063\n",
      "Iteration 153, loss = 0.29685347\n",
      "Iteration 154, loss = 0.29711520\n",
      "Iteration 155, loss = 0.29701968\n",
      "Iteration 156, loss = 0.29612678\n",
      "Iteration 157, loss = 0.29711503\n",
      "Iteration 158, loss = 0.29630919\n",
      "Iteration 159, loss = 0.29660917\n",
      "Iteration 160, loss = 0.29694571\n",
      "Iteration 161, loss = 0.29600743\n",
      "Iteration 162, loss = 0.29653635\n",
      "Iteration 163, loss = 0.29612042\n",
      "Iteration 164, loss = 0.29577699\n",
      "Iteration 165, loss = 0.29552632\n",
      "Iteration 166, loss = 0.29567548\n",
      "Iteration 167, loss = 0.29589733\n",
      "Iteration 168, loss = 0.29546622\n",
      "Iteration 169, loss = 0.29599204\n",
      "Iteration 170, loss = 0.29541652\n",
      "Iteration 171, loss = 0.29553605\n",
      "Iteration 172, loss = 0.29498046\n",
      "Iteration 173, loss = 0.29524613\n",
      "Iteration 174, loss = 0.29589048\n",
      "Iteration 175, loss = 0.29537446\n",
      "Iteration 176, loss = 0.29534253\n",
      "Iteration 177, loss = 0.29505773\n",
      "Iteration 178, loss = 0.29474055\n",
      "Iteration 179, loss = 0.29547905\n",
      "Iteration 180, loss = 0.29504412\n",
      "Iteration 181, loss = 0.29529162\n",
      "Iteration 182, loss = 0.29493744\n",
      "Iteration 183, loss = 0.29457853\n",
      "Iteration 184, loss = 0.29504029\n",
      "Iteration 185, loss = 0.29490183\n",
      "Iteration 186, loss = 0.29459652\n",
      "Iteration 187, loss = 0.29388614\n",
      "Iteration 188, loss = 0.29516332\n",
      "Iteration 189, loss = 0.29467582\n",
      "Iteration 190, loss = 0.29469713\n",
      "Iteration 191, loss = 0.29387004\n",
      "Iteration 192, loss = 0.29492151\n",
      "Iteration 193, loss = 0.29410039\n",
      "Iteration 194, loss = 0.29419426\n",
      "Iteration 195, loss = 0.29438839\n",
      "Iteration 196, loss = 0.29359345\n",
      "Iteration 197, loss = 0.29369781\n",
      "Iteration 198, loss = 0.29393363\n",
      "Iteration 199, loss = 0.29452686\n",
      "Iteration 200, loss = 0.29387441\n",
      "Iteration 201, loss = 0.29374552\n",
      "Iteration 202, loss = 0.29384298\n",
      "Iteration 203, loss = 0.29335357\n",
      "Iteration 204, loss = 0.29423296\n",
      "Iteration 205, loss = 0.29371302\n",
      "Iteration 206, loss = 0.29332048\n",
      "Iteration 207, loss = 0.29371154\n",
      "Iteration 208, loss = 0.29338339\n",
      "Iteration 209, loss = 0.29325125\n",
      "Iteration 210, loss = 0.29326695\n",
      "Iteration 211, loss = 0.29279153\n",
      "Iteration 212, loss = 0.29330649\n",
      "Iteration 213, loss = 0.29281097\n",
      "Iteration 214, loss = 0.29361046\n",
      "Iteration 215, loss = 0.29343059\n",
      "Iteration 216, loss = 0.29292150\n",
      "Iteration 217, loss = 0.29281753\n",
      "Iteration 218, loss = 0.29323715\n",
      "Iteration 219, loss = 0.29317442\n",
      "Iteration 220, loss = 0.29248131\n",
      "Iteration 221, loss = 0.29296128\n",
      "Iteration 222, loss = 0.29313455\n",
      "Iteration 223, loss = 0.29251139\n",
      "Iteration 224, loss = 0.29234400\n",
      "Iteration 225, loss = 0.29212789\n",
      "Iteration 226, loss = 0.29222372\n",
      "Iteration 227, loss = 0.29212244\n",
      "Iteration 228, loss = 0.29234730\n",
      "Iteration 229, loss = 0.29219140\n",
      "Iteration 230, loss = 0.29147185\n",
      "Iteration 231, loss = 0.29243487\n",
      "Iteration 232, loss = 0.29200217\n",
      "Iteration 233, loss = 0.29196741\n",
      "Iteration 234, loss = 0.29178357\n",
      "Iteration 235, loss = 0.29197687\n",
      "Iteration 236, loss = 0.29143964\n",
      "Iteration 237, loss = 0.29111337\n",
      "Iteration 238, loss = 0.29199148\n",
      "Iteration 239, loss = 0.29163764\n",
      "Iteration 240, loss = 0.29158829\n",
      "Iteration 241, loss = 0.29132336\n",
      "Iteration 242, loss = 0.29109332\n",
      "Iteration 243, loss = 0.29122760\n",
      "Iteration 244, loss = 0.29118173\n",
      "Iteration 245, loss = 0.29134089\n",
      "Iteration 246, loss = 0.29155140\n",
      "Iteration 247, loss = 0.29111938\n",
      "Iteration 248, loss = 0.29068767\n",
      "Iteration 249, loss = 0.29164970\n",
      "Iteration 250, loss = 0.29125775\n",
      "Iteration 251, loss = 0.29105743\n",
      "Iteration 252, loss = 0.29058608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.29106422\n",
      "Iteration 254, loss = 0.29083931\n",
      "Iteration 255, loss = 0.29093415\n",
      "Iteration 256, loss = 0.29077275\n",
      "Iteration 257, loss = 0.29095828\n",
      "Iteration 258, loss = 0.29068945\n",
      "Iteration 259, loss = 0.29043570\n",
      "Iteration 260, loss = 0.29064498\n",
      "Iteration 261, loss = 0.29039395\n",
      "Iteration 262, loss = 0.29096491\n",
      "Iteration 263, loss = 0.29039398\n",
      "Iteration 264, loss = 0.29034399\n",
      "Iteration 265, loss = 0.29036935\n",
      "Iteration 266, loss = 0.28994328\n",
      "Iteration 267, loss = 0.28975751\n",
      "Iteration 268, loss = 0.29016122\n",
      "Iteration 269, loss = 0.29005193\n",
      "Iteration 270, loss = 0.28985348\n",
      "Iteration 271, loss = 0.29003092\n",
      "Iteration 272, loss = 0.28978113\n",
      "Iteration 273, loss = 0.29058611\n",
      "Iteration 274, loss = 0.28987994\n",
      "Iteration 275, loss = 0.29013931\n",
      "Iteration 276, loss = 0.29024714\n",
      "Iteration 277, loss = 0.28943902\n",
      "Iteration 278, loss = 0.28980867\n",
      "Iteration 279, loss = 0.28947305\n",
      "Iteration 280, loss = 0.28994736\n",
      "Iteration 281, loss = 0.28951315\n",
      "Iteration 282, loss = 0.29001049\n",
      "Iteration 283, loss = 0.28951405\n",
      "Iteration 284, loss = 0.28970222\n",
      "Iteration 285, loss = 0.28965472\n",
      "Iteration 286, loss = 0.28975397\n",
      "Iteration 287, loss = 0.28955687\n",
      "Iteration 288, loss = 0.28923531\n",
      "Iteration 289, loss = 0.28925289\n",
      "Iteration 290, loss = 0.28919380\n",
      "Iteration 291, loss = 0.28940797\n",
      "Iteration 292, loss = 0.28895637\n",
      "Iteration 293, loss = 0.28910091\n",
      "Iteration 294, loss = 0.28969830\n",
      "Iteration 295, loss = 0.28882778\n",
      "Iteration 296, loss = 0.28928294\n",
      "Iteration 297, loss = 0.28933241\n",
      "Iteration 298, loss = 0.28921114\n",
      "Iteration 299, loss = 0.28922337\n",
      "Iteration 300, loss = 0.28946019\n",
      "MLP-scaled metrics:\n",
      "Accuracy Score: 0.7711476909007773\n",
      "F1 score: 0.7368985108906481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Confusion Matrix-scaled\n",
      "[[8.08327707e-01 1.12536574e-03 1.90546928e-01]\n",
      " [4.47436831e-01 5.45908287e-03 5.47104087e-01]\n",
      " [1.12585037e-01 7.97415046e-04 8.86617548e-01]]\n",
      "CPU times: user 1h 51min 17s, sys: 4min 38s, total: 1h 55min 56s\n",
      "Wall time: 19min 20s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time \n",
    "# Starting with Sklearn MLP\n",
    "# Training model\n",
    "mlp_clf = MLPClassifier(random_state=random_state, max_iter=300, verbose=10)\n",
    "mlp_clf.fit(training_df_scaled_X,training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "mlp_acc,mlp_f1 = test_model_metrics(mlp_clf,\"MLP-scaled\",testing_df_scaled_X,testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    mlp_clf,\n",
    "    testing_df_scaled_X,\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"MLP Confusion Matrix-scaled\")\n",
    "\n",
    "print(\"MLP Confusion Matrix-scaled\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9abebde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.40701400\n",
      "Iteration 2, loss = 0.31252940\n",
      "Iteration 3, loss = 0.30277185\n",
      "Iteration 4, loss = 0.30906611\n",
      "Iteration 5, loss = 0.29275641\n",
      "Iteration 6, loss = 0.28824150\n",
      "Iteration 7, loss = 0.28439195\n",
      "Iteration 8, loss = 0.29689165\n",
      "Iteration 9, loss = 0.27265937\n",
      "Iteration 10, loss = 0.28276546\n",
      "Iteration 11, loss = 0.28344491\n",
      "Iteration 12, loss = 0.30125225\n",
      "Iteration 13, loss = 0.28142215\n",
      "Iteration 14, loss = 0.27446613\n",
      "Iteration 15, loss = 0.27726245\n",
      "Iteration 16, loss = 0.27750526\n",
      "Iteration 17, loss = 0.27472576\n",
      "Iteration 18, loss = 0.29798478\n",
      "Iteration 19, loss = 0.27344115\n",
      "Iteration 20, loss = 0.27723733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33151311\n",
      "Iteration 2, loss = 0.28947453\n",
      "Iteration 3, loss = 0.27905612\n",
      "Iteration 4, loss = 0.27487679\n",
      "Iteration 5, loss = 0.27349553\n",
      "Iteration 6, loss = 0.27349734\n",
      "Iteration 7, loss = 0.26545096\n",
      "Iteration 8, loss = 0.26780056\n",
      "Iteration 9, loss = 0.27019535\n",
      "Iteration 10, loss = 0.26352828\n",
      "Iteration 11, loss = 0.26251522\n",
      "Iteration 12, loss = 0.26131800\n",
      "Iteration 13, loss = 0.26319210\n",
      "Iteration 14, loss = 0.26076037\n",
      "Iteration 15, loss = 0.26463225\n",
      "Iteration 16, loss = 0.26420030\n",
      "Iteration 17, loss = 0.26245610\n",
      "Iteration 18, loss = 0.25809431\n",
      "Iteration 19, loss = 0.25912869\n",
      "Iteration 20, loss = 0.25764032\n",
      "Iteration 21, loss = 0.25622206\n",
      "Iteration 22, loss = 0.26324904\n",
      "Iteration 23, loss = 0.25670276\n",
      "Iteration 24, loss = 0.25505707\n",
      "Iteration 25, loss = 0.25654553\n",
      "Iteration 26, loss = 0.25630235\n",
      "Iteration 27, loss = 0.25776975\n",
      "Iteration 28, loss = 0.25645046\n",
      "Iteration 29, loss = 0.25730915\n",
      "Iteration 30, loss = 0.25621657\n",
      "Iteration 31, loss = 0.25539488\n",
      "Iteration 32, loss = 0.25624228\n",
      "Iteration 33, loss = 0.25360930\n",
      "Iteration 34, loss = 0.25557961\n",
      "Iteration 35, loss = 0.25553889\n",
      "Iteration 36, loss = 0.25778810\n",
      "Iteration 37, loss = 0.25508887\n",
      "Iteration 38, loss = 0.25414387\n",
      "Iteration 39, loss = 0.25342958\n",
      "Iteration 40, loss = 0.26147934\n",
      "Iteration 41, loss = 0.25905275\n",
      "Iteration 42, loss = 0.25461603\n",
      "Iteration 43, loss = 0.25411535\n",
      "Iteration 44, loss = 0.25944967\n",
      "Iteration 45, loss = 0.25393417\n",
      "Iteration 46, loss = 0.25428147\n",
      "Iteration 47, loss = 0.25308785\n",
      "Iteration 48, loss = 0.25401726\n",
      "Iteration 49, loss = 0.25564002\n",
      "Iteration 50, loss = 0.25591395\n",
      "Iteration 51, loss = 0.25577974\n",
      "Iteration 52, loss = 0.25217546\n",
      "Iteration 53, loss = 0.25247073\n",
      "Iteration 54, loss = 0.25170026\n",
      "Iteration 55, loss = 0.25297521\n",
      "Iteration 56, loss = 0.25436681\n",
      "Iteration 57, loss = 0.25361548\n",
      "Iteration 58, loss = 0.25426128\n",
      "Iteration 59, loss = 0.25122683\n",
      "Iteration 60, loss = 0.25344580\n",
      "Iteration 61, loss = 0.25222897\n",
      "Iteration 62, loss = 0.25551096\n",
      "Iteration 63, loss = 0.25327813\n",
      "Iteration 64, loss = 0.25419450\n",
      "Iteration 65, loss = 0.25240072\n",
      "Iteration 66, loss = 0.25018247\n",
      "Iteration 67, loss = 0.25251785\n",
      "Iteration 68, loss = 0.25390283\n",
      "Iteration 69, loss = 0.25459611\n",
      "Iteration 70, loss = 0.25112403\n",
      "Iteration 71, loss = 0.25181009\n",
      "Iteration 72, loss = 0.25375163\n",
      "Iteration 73, loss = 0.25159532\n",
      "Iteration 74, loss = 0.25060620\n",
      "Iteration 75, loss = 0.25239002\n",
      "Iteration 76, loss = 0.25120615\n",
      "Iteration 77, loss = 0.25238745\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33997125\n",
      "Iteration 2, loss = 0.29539368\n",
      "Iteration 3, loss = 0.29507243\n",
      "Iteration 4, loss = 0.28532762\n",
      "Iteration 5, loss = 0.28135136\n",
      "Iteration 6, loss = 0.28502956\n",
      "Iteration 7, loss = 0.27266012\n",
      "Iteration 8, loss = 0.27748809\n",
      "Iteration 9, loss = 0.27056093\n",
      "Iteration 10, loss = 0.27045629\n",
      "Iteration 11, loss = 0.27345408\n",
      "Iteration 12, loss = 0.27578223\n",
      "Iteration 13, loss = 0.26338679\n",
      "Iteration 14, loss = 0.27297334\n",
      "Iteration 15, loss = 0.26640932\n",
      "Iteration 16, loss = 0.26764288\n",
      "Iteration 17, loss = 0.26415566\n",
      "Iteration 18, loss = 0.26213627\n",
      "Iteration 19, loss = 0.26431007\n",
      "Iteration 20, loss = 0.26140680\n",
      "Iteration 21, loss = 0.26196546\n",
      "Iteration 22, loss = 0.26047105\n",
      "Iteration 23, loss = 0.25917369\n",
      "Iteration 24, loss = 0.26111586\n",
      "Iteration 25, loss = 0.25987146\n",
      "Iteration 26, loss = 0.25732424\n",
      "Iteration 27, loss = 0.25997891\n",
      "Iteration 28, loss = 0.25839253\n",
      "Iteration 29, loss = 0.25611018\n",
      "Iteration 30, loss = 0.25857453\n",
      "Iteration 31, loss = 0.26175838\n",
      "Iteration 32, loss = 0.25661687\n",
      "Iteration 33, loss = 0.26037248\n",
      "Iteration 34, loss = 0.26065735\n",
      "Iteration 35, loss = 0.26076017\n",
      "Iteration 36, loss = 0.25959812\n",
      "Iteration 37, loss = 0.25866199\n",
      "Iteration 38, loss = 0.25746707\n",
      "Iteration 39, loss = 0.26004436\n",
      "Iteration 40, loss = 0.26084979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45275526\n",
      "Iteration 2, loss = 0.32560499\n",
      "Iteration 3, loss = 0.31452355\n",
      "Iteration 4, loss = 0.32580662\n",
      "Iteration 5, loss = 0.29341889\n",
      "Iteration 6, loss = 0.28483689\n",
      "Iteration 7, loss = 0.29165098\n",
      "Iteration 8, loss = 0.28636074\n",
      "Iteration 9, loss = 0.28689821\n",
      "Iteration 10, loss = 0.29008393\n",
      "Iteration 11, loss = 0.27521144\n",
      "Iteration 12, loss = 0.27387334\n",
      "Iteration 13, loss = 0.27391479\n",
      "Iteration 14, loss = 0.28197902\n",
      "Iteration 15, loss = 0.27639178\n",
      "Iteration 16, loss = 0.28908648\n",
      "Iteration 17, loss = 0.29012385\n",
      "Iteration 18, loss = 0.28440596\n",
      "Iteration 19, loss = 0.27585607\n",
      "Iteration 20, loss = 0.28933934\n",
      "Iteration 21, loss = 0.27958911\n",
      "Iteration 22, loss = 0.28063368\n",
      "Iteration 23, loss = 0.27714014\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42228354\n",
      "Iteration 2, loss = 0.32215967\n",
      "Iteration 3, loss = 0.33908476\n",
      "Iteration 4, loss = 0.32521411\n",
      "Iteration 5, loss = 0.32044450\n",
      "Iteration 6, loss = 0.31179280\n",
      "Iteration 7, loss = 0.29381041\n",
      "Iteration 8, loss = 0.28349443\n",
      "Iteration 9, loss = 0.31964936\n",
      "Iteration 10, loss = 0.29322928\n",
      "Iteration 11, loss = 0.30093576\n",
      "Iteration 12, loss = 0.28619338\n",
      "Iteration 13, loss = 0.29023536\n",
      "Iteration 14, loss = 0.28329078\n",
      "Iteration 15, loss = 0.28776916\n",
      "Iteration 16, loss = 0.30342951\n",
      "Iteration 17, loss = 0.29841244\n",
      "Iteration 18, loss = 0.28476726\n",
      "Iteration 19, loss = 0.29218428\n",
      "Iteration 20, loss = 0.30346762\n",
      "Iteration 21, loss = 0.27419065\n",
      "Iteration 22, loss = 0.28124028\n",
      "Iteration 23, loss = 0.28219874\n",
      "Iteration 24, loss = 0.28055917\n",
      "Iteration 25, loss = 0.29387752\n",
      "Iteration 26, loss = 0.27792768\n",
      "Iteration 27, loss = 0.28441130\n",
      "Iteration 28, loss = 0.28188538\n",
      "Iteration 29, loss = 0.27375106\n",
      "Iteration 30, loss = 0.28203961\n",
      "Iteration 31, loss = 0.27606503\n",
      "Iteration 32, loss = 0.28179319\n",
      "Iteration 33, loss = 0.28937709\n",
      "Iteration 34, loss = 0.28695145\n",
      "Iteration 35, loss = 0.28219006\n",
      "Iteration 36, loss = 0.28241085\n",
      "Iteration 37, loss = 0.29089766\n",
      "Iteration 38, loss = 0.28637184\n",
      "Iteration 39, loss = 0.27293665\n",
      "Iteration 40, loss = 0.28057402\n",
      "Iteration 41, loss = 0.28730272\n",
      "Iteration 42, loss = 0.27935682\n",
      "Iteration 43, loss = 0.27516534\n",
      "Iteration 44, loss = 0.28736978\n",
      "Iteration 45, loss = 0.27591741\n",
      "Iteration 46, loss = 0.28472459\n",
      "Iteration 47, loss = 0.27635595\n",
      "Iteration 48, loss = 0.27355454\n",
      "Iteration 49, loss = 0.28148498\n",
      "Iteration 50, loss = 0.27211471\n",
      "Iteration 51, loss = 0.28507277\n",
      "Iteration 52, loss = 0.27622686\n",
      "Iteration 53, loss = 0.27450023\n",
      "Iteration 54, loss = 0.28213408\n",
      "Iteration 55, loss = 0.28091597\n",
      "Iteration 56, loss = 0.28580932\n",
      "Iteration 57, loss = 0.27667131\n",
      "Iteration 58, loss = 0.28859642\n",
      "Iteration 59, loss = 0.28628586\n",
      "Iteration 60, loss = 0.28737237\n",
      "Iteration 61, loss = 0.27819459\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33886880\n",
      "Iteration 2, loss = 0.30227519\n",
      "Iteration 3, loss = 0.30093644\n",
      "Iteration 4, loss = 0.29721878\n",
      "Iteration 5, loss = 0.29810112\n",
      "Iteration 6, loss = 0.28372478\n",
      "Iteration 7, loss = 0.29411973\n",
      "Iteration 8, loss = 0.28725877\n",
      "Iteration 9, loss = 0.29052292\n",
      "Iteration 10, loss = 0.28122062\n",
      "Iteration 11, loss = 0.28404076\n",
      "Iteration 12, loss = 0.28360170\n",
      "Iteration 13, loss = 0.28201041\n",
      "Iteration 14, loss = 0.28509947\n",
      "Iteration 15, loss = 0.28321803\n",
      "Iteration 16, loss = 0.28851742\n",
      "Iteration 17, loss = 0.28646807\n",
      "Iteration 18, loss = 0.29269542\n",
      "Iteration 19, loss = 0.29207284\n",
      "Iteration 20, loss = 0.27558027\n",
      "Iteration 21, loss = 0.27571184\n",
      "Iteration 22, loss = 0.28152000\n",
      "Iteration 1, loss = 0.40701400\n",
      "Iteration 2, loss = 0.31252940\n",
      "Iteration 3, loss = 0.30277185\n",
      "Iteration 4, loss = 0.30906611\n",
      "Iteration 5, loss = 0.29275641\n",
      "Iteration 6, loss = 0.28824150\n",
      "Iteration 7, loss = 0.28439195\n",
      "Iteration 8, loss = 0.29689165\n",
      "Iteration 9, loss = 0.27265937\n",
      "Iteration 10, loss = 0.28276546\n",
      "Iteration 11, loss = 0.28344491\n",
      "Iteration 12, loss = 0.30125225\n",
      "Iteration 13, loss = 0.28142215\n",
      "Iteration 14, loss = 0.27446613\n",
      "Iteration 15, loss = 0.27726245\n",
      "Iteration 16, loss = 0.27750526\n",
      "Iteration 17, loss = 0.27472576\n",
      "Iteration 18, loss = 0.29798478\n",
      "Iteration 19, loss = 0.27344115\n",
      "Iteration 20, loss = 0.27723733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35310690\n",
      "Iteration 2, loss = 0.29126031\n",
      "Iteration 3, loss = 0.27900377\n",
      "Iteration 4, loss = 0.28017537\n",
      "Iteration 5, loss = 0.27582387\n",
      "Iteration 6, loss = 0.27587155\n",
      "Iteration 7, loss = 0.27904152\n",
      "Iteration 8, loss = 0.27379281\n",
      "Iteration 9, loss = 0.26701148\n",
      "Iteration 10, loss = 0.26656521\n",
      "Iteration 11, loss = 0.27045159\n",
      "Iteration 12, loss = 0.26603306\n",
      "Iteration 13, loss = 0.26635417\n",
      "Iteration 14, loss = 0.26360474\n",
      "Iteration 15, loss = 0.25977120\n",
      "Iteration 16, loss = 0.26397038\n",
      "Iteration 17, loss = 0.26226392\n",
      "Iteration 18, loss = 0.26406126\n",
      "Iteration 19, loss = 0.26059179\n",
      "Iteration 20, loss = 0.25974975\n",
      "Iteration 21, loss = 0.26264193\n",
      "Iteration 22, loss = 0.26257541\n",
      "Iteration 23, loss = 0.26154436\n",
      "Iteration 24, loss = 0.25674708\n",
      "Iteration 25, loss = 0.26095626\n",
      "Iteration 26, loss = 0.25592188\n",
      "Iteration 27, loss = 0.25928222\n",
      "Iteration 28, loss = 0.26028780\n",
      "Iteration 29, loss = 0.25755722\n",
      "Iteration 30, loss = 0.25395901\n",
      "Iteration 31, loss = 0.25503988\n",
      "Iteration 32, loss = 0.25724433\n",
      "Iteration 33, loss = 0.25611049\n",
      "Iteration 34, loss = 0.25579115\n",
      "Iteration 35, loss = 0.25507405\n",
      "Iteration 36, loss = 0.25810615\n",
      "Iteration 37, loss = 0.26202333\n",
      "Iteration 38, loss = 0.25692064\n",
      "Iteration 39, loss = 0.25561773\n",
      "Iteration 40, loss = 0.25325582\n",
      "Iteration 41, loss = 0.25732719\n",
      "Iteration 42, loss = 0.26141346\n",
      "Iteration 43, loss = 0.25365774\n",
      "Iteration 44, loss = 0.25505255\n",
      "Iteration 45, loss = 0.25330053\n",
      "Iteration 46, loss = 0.25549687\n",
      "Iteration 47, loss = 0.25372307\n",
      "Iteration 48, loss = 0.25542309\n",
      "Iteration 49, loss = 0.25167797\n",
      "Iteration 50, loss = 0.25172891\n",
      "Iteration 51, loss = 0.25211969\n",
      "Iteration 52, loss = 0.25466409\n",
      "Iteration 53, loss = 0.25272482\n",
      "Iteration 54, loss = 0.25550707\n",
      "Iteration 55, loss = 0.25083508\n",
      "Iteration 56, loss = 0.25303383\n",
      "Iteration 57, loss = 0.25156925\n",
      "Iteration 58, loss = 0.25434770\n",
      "Iteration 59, loss = 0.25390362\n",
      "Iteration 60, loss = 0.25116677\n",
      "Iteration 61, loss = 0.25445935\n",
      "Iteration 62, loss = 0.25365540\n",
      "Iteration 63, loss = 0.25375427\n",
      "Iteration 64, loss = 0.25168098\n",
      "Iteration 65, loss = 0.25327376\n",
      "Iteration 66, loss = 0.25462668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34412152\n",
      "Iteration 2, loss = 0.30507362\n",
      "Iteration 3, loss = 0.28688225\n",
      "Iteration 4, loss = 0.28465214\n",
      "Iteration 5, loss = 0.28346516\n",
      "Iteration 6, loss = 0.27826548\n",
      "Iteration 7, loss = 0.27111969\n",
      "Iteration 8, loss = 0.26859874\n",
      "Iteration 9, loss = 0.27663955\n",
      "Iteration 10, loss = 0.26935929\n",
      "Iteration 11, loss = 0.27348801\n",
      "Iteration 12, loss = 0.26735122\n",
      "Iteration 13, loss = 0.26954567\n",
      "Iteration 14, loss = 0.26080474\n",
      "Iteration 15, loss = 0.26695176\n",
      "Iteration 16, loss = 0.26470107\n",
      "Iteration 17, loss = 0.26914987\n",
      "Iteration 18, loss = 0.26520454\n",
      "Iteration 19, loss = 0.26286107\n",
      "Iteration 20, loss = 0.25930528\n",
      "Iteration 21, loss = 0.26202986\n",
      "Iteration 22, loss = 0.25915062\n",
      "Iteration 23, loss = 0.26093488\n",
      "Iteration 24, loss = 0.25880682\n",
      "Iteration 25, loss = 0.25974888\n",
      "Iteration 26, loss = 0.25753064\n",
      "Iteration 27, loss = 0.25910204\n",
      "Iteration 28, loss = 0.25803839\n",
      "Iteration 29, loss = 0.26096888\n",
      "Iteration 30, loss = 0.25744794\n",
      "Iteration 31, loss = 0.26151137\n",
      "Iteration 32, loss = 0.25990849\n",
      "Iteration 33, loss = 0.25746378\n",
      "Iteration 34, loss = 0.25697236\n",
      "Iteration 35, loss = 0.25544654\n",
      "Iteration 36, loss = 0.25817216\n",
      "Iteration 37, loss = 0.25586631\n",
      "Iteration 38, loss = 0.25636061\n",
      "Iteration 39, loss = 0.25574711\n",
      "Iteration 40, loss = 0.25627766\n",
      "Iteration 41, loss = 0.25819090\n",
      "Iteration 42, loss = 0.25808895\n",
      "Iteration 43, loss = 0.25391420\n",
      "Iteration 44, loss = 0.25472954\n",
      "Iteration 45, loss = 0.25111495\n",
      "Iteration 46, loss = 0.25806814\n",
      "Iteration 47, loss = 0.26367074\n",
      "Iteration 48, loss = 0.26384067\n",
      "Iteration 49, loss = 0.25948340\n",
      "Iteration 50, loss = 0.25567969\n",
      "Iteration 51, loss = 0.25502550\n",
      "Iteration 52, loss = 0.25164974\n",
      "Iteration 53, loss = 0.25167926\n",
      "Iteration 54, loss = 0.25414922\n",
      "Iteration 55, loss = 0.25283118\n",
      "Iteration 56, loss = 0.25541345\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43983587\n",
      "Iteration 2, loss = 0.31729028\n",
      "Iteration 3, loss = 0.31520438\n",
      "Iteration 4, loss = 0.32229455\n",
      "Iteration 5, loss = 0.31791594\n",
      "Iteration 6, loss = 0.31137275\n",
      "Iteration 7, loss = 0.30226321\n",
      "Iteration 8, loss = 0.29729596\n",
      "Iteration 9, loss = 0.28987493\n",
      "Iteration 10, loss = 0.29233973\n",
      "Iteration 11, loss = 0.30081873\n",
      "Iteration 12, loss = 0.28684776\n",
      "Iteration 13, loss = 0.29986260\n",
      "Iteration 14, loss = 0.29266166\n",
      "Iteration 15, loss = 0.28097592\n",
      "Iteration 16, loss = 0.28441976\n",
      "Iteration 17, loss = 0.28720398\n",
      "Iteration 18, loss = 0.28669895\n",
      "Iteration 19, loss = 0.27988257\n",
      "Iteration 20, loss = 0.28899673\n",
      "Iteration 21, loss = 0.27408737\n",
      "Iteration 22, loss = 0.29983929\n",
      "Iteration 23, loss = 0.28418638\n",
      "Iteration 24, loss = 0.29932057\n",
      "Iteration 25, loss = 0.27737012\n",
      "Iteration 26, loss = 0.30741849\n",
      "Iteration 27, loss = 0.29578433\n",
      "Iteration 28, loss = 0.28405771\n",
      "Iteration 29, loss = 0.28218164\n",
      "Iteration 30, loss = 0.28136491\n",
      "Iteration 31, loss = 0.28889972\n",
      "Iteration 32, loss = 0.28979961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40000571\n",
      "Iteration 2, loss = 0.37220302\n",
      "Iteration 3, loss = 0.32016539\n",
      "Iteration 4, loss = 0.30737767\n",
      "Iteration 5, loss = 0.30925138\n",
      "Iteration 6, loss = 0.29328554\n",
      "Iteration 7, loss = 0.30448788\n",
      "Iteration 8, loss = 0.29331085\n",
      "Iteration 9, loss = 0.29151593\n",
      "Iteration 10, loss = 0.29175163\n",
      "Iteration 11, loss = 0.29018645\n",
      "Iteration 12, loss = 0.27903685\n",
      "Iteration 13, loss = 0.28337694\n",
      "Iteration 14, loss = 0.28741029\n",
      "Iteration 15, loss = 0.28446943\n",
      "Iteration 16, loss = 0.28255836\n",
      "Iteration 17, loss = 0.28286942\n",
      "Iteration 18, loss = 0.28012104\n",
      "Iteration 19, loss = 0.28975265\n",
      "Iteration 20, loss = 0.28423648\n",
      "Iteration 21, loss = 0.28825179\n",
      "Iteration 22, loss = 0.28541930\n",
      "Iteration 23, loss = 0.28465797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33969944\n",
      "Iteration 2, loss = 0.31447001\n",
      "Iteration 3, loss = 0.30077515\n",
      "Iteration 4, loss = 0.29296112\n",
      "Iteration 5, loss = 0.29624165\n",
      "Iteration 6, loss = 0.28895337\n",
      "Iteration 7, loss = 0.28212130\n",
      "Iteration 8, loss = 0.29246379\n",
      "Iteration 9, loss = 0.28455260\n",
      "Iteration 10, loss = 0.28570243\n",
      "Iteration 11, loss = 0.27872090\n",
      "Iteration 12, loss = 0.27897943\n",
      "Iteration 13, loss = 0.27810370\n",
      "Iteration 14, loss = 0.28219594\n",
      "Iteration 15, loss = 0.27571478\n",
      "Iteration 16, loss = 0.27969054\n",
      "Iteration 17, loss = 0.28249519\n",
      "Iteration 18, loss = 0.28444932\n",
      "Iteration 19, loss = 0.28018518\n",
      "Iteration 20, loss = 0.27543873\n",
      "Iteration 21, loss = 0.28336504\n",
      "Iteration 22, loss = 0.28011209\n",
      "Iteration 23, loss = 0.28260725\n",
      "Iteration 24, loss = 0.28103323\n",
      "Iteration 25, loss = 0.27666036\n",
      "Iteration 26, loss = 0.27555071\n",
      "Iteration 27, loss = 0.27562886\n",
      "Iteration 28, loss = 0.27443534\n",
      "Iteration 29, loss = 0.27585572\n",
      "Iteration 30, loss = 0.28362881\n",
      "Iteration 31, loss = 0.28319067\n",
      "Iteration 32, loss = 0.27869514\n",
      "Iteration 33, loss = 0.27837626\n",
      "Iteration 34, loss = 0.27506372\n",
      "Iteration 35, loss = 0.28144086\n",
      "Iteration 36, loss = 0.28676845\n",
      "Iteration 37, loss = 0.27660267\n",
      "Iteration 38, loss = 0.28035506\n",
      "Iteration 39, loss = 0.27865599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34238184\n",
      "Iteration 2, loss = 0.31384285\n",
      "Iteration 3, loss = 0.30361478\n",
      "Iteration 4, loss = 0.30548195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.29190243Iteration 1, loss = 0.45841544\n",
      "Iteration 2, loss = 0.34654656\n",
      "Iteration 3, loss = 0.31394718\n",
      "Iteration 4, loss = 0.28977850\n",
      "Iteration 5, loss = 0.29568180\n",
      "Iteration 6, loss = 0.31243977\n",
      "Iteration 7, loss = 0.30642095\n",
      "Iteration 8, loss = 0.29913575\n",
      "Iteration 9, loss = 0.29182255\n",
      "Iteration 10, loss = 0.29886972\n",
      "Iteration 11, loss = 0.29874969\n",
      "Iteration 12, loss = 0.28401646\n",
      "Iteration 13, loss = 0.28735091\n",
      "Iteration 14, loss = 0.29277096\n",
      "Iteration 15, loss = 0.28090091\n",
      "Iteration 16, loss = 0.28376037\n",
      "Iteration 17, loss = 0.29561624\n",
      "Iteration 18, loss = 0.27745587\n",
      "Iteration 19, loss = 0.27463096\n",
      "Iteration 20, loss = 0.27954703\n",
      "Iteration 21, loss = 0.28071557\n",
      "Iteration 22, loss = 0.27581267\n",
      "Iteration 23, loss = 0.28211803\n",
      "Iteration 24, loss = 0.27460174\n",
      "Iteration 25, loss = 0.27419507\n",
      "Iteration 26, loss = 0.28064788\n",
      "Iteration 27, loss = 0.27651367\n",
      "Iteration 28, loss = 0.27514478\n",
      "Iteration 29, loss = 0.27130439\n",
      "Iteration 30, loss = 0.26952141\n",
      "Iteration 31, loss = 0.27813779\n",
      "Iteration 32, loss = 0.26822979\n",
      "Iteration 33, loss = 0.26409080\n",
      "Iteration 34, loss = 0.26880538\n",
      "Iteration 35, loss = 0.27146014\n",
      "Iteration 36, loss = 0.26879669\n",
      "Iteration 37, loss = 0.27306933\n",
      "Iteration 38, loss = 0.27849066\n",
      "Iteration 39, loss = 0.27951046\n",
      "Iteration 40, loss = 0.27310617\n",
      "Iteration 41, loss = 0.27022008\n",
      "Iteration 42, loss = 0.26748070\n",
      "Iteration 43, loss = 0.26589211\n",
      "Iteration 44, loss = 0.27344177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35310690\n",
      "Iteration 2, loss = 0.29126031\n",
      "Iteration 3, loss = 0.27900377\n",
      "Iteration 4, loss = 0.28017537\n",
      "Iteration 5, loss = 0.27582387\n",
      "Iteration 6, loss = 0.27587155\n",
      "Iteration 7, loss = 0.27904152\n",
      "Iteration 8, loss = 0.27379281\n",
      "Iteration 9, loss = 0.26701148\n",
      "Iteration 10, loss = 0.26656521\n",
      "Iteration 11, loss = 0.27045159\n",
      "Iteration 12, loss = 0.26603306\n",
      "Iteration 13, loss = 0.26635417\n",
      "Iteration 14, loss = 0.26360474\n",
      "Iteration 15, loss = 0.25977120\n",
      "Iteration 16, loss = 0.26397038\n",
      "Iteration 17, loss = 0.26226392\n",
      "Iteration 18, loss = 0.26406126\n",
      "Iteration 19, loss = 0.26059179\n",
      "Iteration 20, loss = 0.25974975\n",
      "Iteration 21, loss = 0.26264193\n",
      "Iteration 22, loss = 0.26257541\n",
      "Iteration 23, loss = 0.26154436\n",
      "Iteration 24, loss = 0.25674708\n",
      "Iteration 25, loss = 0.26095626\n",
      "Iteration 26, loss = 0.25592188\n",
      "Iteration 27, loss = 0.25928222\n",
      "Iteration 28, loss = 0.26028780\n",
      "Iteration 29, loss = 0.25755722\n",
      "Iteration 30, loss = 0.25395901\n",
      "Iteration 31, loss = 0.25503988\n",
      "Iteration 32, loss = 0.25724433\n",
      "Iteration 33, loss = 0.25611049\n",
      "Iteration 34, loss = 0.25579115\n",
      "Iteration 35, loss = 0.25507405\n",
      "Iteration 36, loss = 0.25810615\n",
      "Iteration 37, loss = 0.26202333\n",
      "Iteration 38, loss = 0.25692064\n",
      "Iteration 39, loss = 0.25561773\n",
      "Iteration 40, loss = 0.25325582\n",
      "Iteration 41, loss = 0.25732719\n",
      "Iteration 42, loss = 0.26141346\n",
      "Iteration 43, loss = 0.25365774\n",
      "Iteration 44, loss = 0.25505255\n",
      "Iteration 45, loss = 0.25330053\n",
      "Iteration 46, loss = 0.25549687\n",
      "Iteration 47, loss = 0.25372307\n",
      "Iteration 48, loss = 0.25542309\n",
      "Iteration 49, loss = 0.25167797\n",
      "Iteration 50, loss = 0.25172891\n",
      "Iteration 51, loss = 0.25211969\n",
      "Iteration 52, loss = 0.25466409\n",
      "Iteration 53, loss = 0.25272482\n",
      "Iteration 54, loss = 0.25550707\n",
      "Iteration 55, loss = 0.25083508\n",
      "Iteration 56, loss = 0.25303383\n",
      "Iteration 57, loss = 0.25156925\n",
      "Iteration 58, loss = 0.25434770\n",
      "Iteration 59, loss = 0.25390362\n",
      "Iteration 60, loss = 0.25116677\n",
      "Iteration 61, loss = 0.25445935\n",
      "Iteration 62, loss = 0.25365540\n",
      "Iteration 63, loss = 0.25375427\n",
      "Iteration 64, loss = 0.25168098\n",
      "Iteration 65, loss = 0.25327376\n",
      "Iteration 66, loss = 0.25462668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34412152\n",
      "Iteration 2, loss = 0.30507362\n",
      "Iteration 3, loss = 0.28688225\n",
      "Iteration 4, loss = 0.28465214\n",
      "Iteration 5, loss = 0.28346516\n",
      "Iteration 6, loss = 0.27826548\n",
      "Iteration 7, loss = 0.27111969\n",
      "Iteration 8, loss = 0.26859874\n",
      "Iteration 9, loss = 0.27663955\n",
      "Iteration 10, loss = 0.26935929\n",
      "Iteration 11, loss = 0.27348801\n",
      "Iteration 12, loss = 0.26735122\n",
      "Iteration 13, loss = 0.26954567\n",
      "Iteration 14, loss = 0.26080474\n",
      "Iteration 15, loss = 0.26695176\n",
      "Iteration 16, loss = 0.26470107\n",
      "Iteration 17, loss = 0.26914987\n",
      "Iteration 18, loss = 0.26520454\n",
      "Iteration 19, loss = 0.26286107\n",
      "Iteration 20, loss = 0.25930528\n",
      "Iteration 21, loss = 0.26202986\n",
      "Iteration 22, loss = 0.25915062\n",
      "Iteration 23, loss = 0.26093488\n",
      "Iteration 24, loss = 0.25880682\n",
      "Iteration 25, loss = 0.25974888\n",
      "Iteration 26, loss = 0.25753064\n",
      "Iteration 27, loss = 0.25910204\n",
      "Iteration 28, loss = 0.25803839\n",
      "Iteration 29, loss = 0.26096888\n",
      "Iteration 30, loss = 0.25744794\n",
      "Iteration 31, loss = 0.26151137\n",
      "Iteration 32, loss = 0.25990849\n",
      "Iteration 33, loss = 0.25746378\n",
      "Iteration 34, loss = 0.25697236\n",
      "Iteration 35, loss = 0.25544654\n",
      "Iteration 36, loss = 0.25817216\n",
      "Iteration 37, loss = 0.25586631\n",
      "Iteration 38, loss = 0.25636061\n",
      "Iteration 39, loss = 0.25574711\n",
      "Iteration 40, loss = 0.25627766\n",
      "Iteration 41, loss = 0.25819090\n",
      "Iteration 42, loss = 0.25808895\n",
      "Iteration 43, loss = 0.25391420\n",
      "Iteration 44, loss = 0.25472954\n",
      "Iteration 45, loss = 0.25111495\n",
      "Iteration 46, loss = 0.25806814\n",
      "Iteration 47, loss = 0.26367074\n",
      "Iteration 48, loss = 0.26384067\n",
      "Iteration 49, loss = 0.25948340\n",
      "Iteration 50, loss = 0.25567969\n",
      "Iteration 51, loss = 0.25502550\n",
      "Iteration 52, loss = 0.25164974\n",
      "Iteration 53, loss = 0.25167926\n",
      "Iteration 54, loss = 0.25414922\n",
      "Iteration 55, loss = 0.25283118\n",
      "Iteration 56, loss = 0.25541345\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42228354\n",
      "Iteration 2, loss = 0.32215967\n",
      "Iteration 3, loss = 0.33908476\n",
      "Iteration 4, loss = 0.32521411\n",
      "Iteration 5, loss = 0.32044450\n",
      "Iteration 6, loss = 0.31179280\n",
      "Iteration 7, loss = 0.29381041\n",
      "Iteration 8, loss = 0.28349443\n",
      "Iteration 9, loss = 0.31964936\n",
      "Iteration 10, loss = 0.29322928\n",
      "Iteration 11, loss = 0.30093576\n",
      "Iteration 12, loss = 0.28619338\n",
      "Iteration 13, loss = 0.29023536\n",
      "Iteration 14, loss = 0.28329078\n",
      "Iteration 15, loss = 0.28776916\n",
      "Iteration 16, loss = 0.30342951\n",
      "Iteration 17, loss = 0.29841244\n",
      "Iteration 18, loss = 0.28476726\n",
      "Iteration 19, loss = 0.29218428\n",
      "Iteration 20, loss = 0.30346762\n",
      "Iteration 21, loss = 0.27419065\n",
      "Iteration 22, loss = 0.28124028\n",
      "Iteration 23, loss = 0.28219874\n",
      "Iteration 24, loss = 0.28055917\n",
      "Iteration 25, loss = 0.29387752\n",
      "Iteration 26, loss = 0.27792768\n",
      "Iteration 27, loss = 0.28441130\n",
      "Iteration 28, loss = 0.28188538\n",
      "Iteration 29, loss = 0.27375106\n",
      "Iteration 30, loss = 0.28203961\n",
      "Iteration 31, loss = 0.27606503\n",
      "Iteration 32, loss = 0.28179319\n",
      "Iteration 33, loss = 0.28937709\n",
      "Iteration 34, loss = 0.28695145\n",
      "Iteration 35, loss = 0.28219006\n",
      "Iteration 36, loss = 0.28241085\n",
      "Iteration 37, loss = 0.29089766\n",
      "Iteration 38, loss = 0.28637184\n",
      "Iteration 39, loss = 0.27293665\n",
      "Iteration 40, loss = 0.28057402\n",
      "Iteration 41, loss = 0.28730272\n",
      "Iteration 42, loss = 0.27935682\n",
      "Iteration 43, loss = 0.27516534\n",
      "Iteration 44, loss = 0.28736978\n",
      "Iteration 45, loss = 0.27591741\n",
      "Iteration 46, loss = 0.28472459\n",
      "Iteration 47, loss = 0.27635595\n",
      "Iteration 48, loss = 0.27355454\n",
      "Iteration 49, loss = 0.28148498\n",
      "Iteration 50, loss = 0.27211471\n",
      "Iteration 51, loss = 0.28507277\n",
      "Iteration 52, loss = 0.27622686\n",
      "Iteration 53, loss = 0.27450023\n",
      "Iteration 54, loss = 0.28213408\n",
      "Iteration 55, loss = 0.28091597\n",
      "Iteration 56, loss = 0.28580932\n",
      "Iteration 57, loss = 0.27667131\n",
      "Iteration 58, loss = 0.28859642\n",
      "Iteration 59, loss = 0.28628586\n",
      "Iteration 60, loss = 0.28737237\n",
      "Iteration 61, loss = 0.27819459\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35012753\n",
      "Iteration 2, loss = 0.31187009\n",
      "Iteration 3, loss = 0.30081213\n",
      "Iteration 4, loss = 0.30203804\n",
      "Iteration 5, loss = 0.29962846\n",
      "Iteration 6, loss = 0.29144054\n",
      "Iteration 7, loss = 0.30352538\n",
      "Iteration 8, loss = 0.28212710\n",
      "Iteration 9, loss = 0.27611418\n",
      "Iteration 10, loss = 0.28331971\n",
      "Iteration 11, loss = 0.27936639\n",
      "Iteration 12, loss = 0.28649040\n",
      "Iteration 13, loss = 0.28354700\n",
      "Iteration 14, loss = 0.28269315\n",
      "Iteration 15, loss = 0.27968561\n",
      "Iteration 16, loss = 0.27436379\n",
      "Iteration 17, loss = 0.27809853\n",
      "Iteration 18, loss = 0.27708354\n",
      "Iteration 19, loss = 0.27994934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.45841544\n",
      "Iteration 2, loss = 0.34654656\n",
      "Iteration 3, loss = 0.31394718\n",
      "Iteration 4, loss = 0.28977850\n",
      "Iteration 5, loss = 0.29568180\n",
      "Iteration 6, loss = 0.31243977\n",
      "Iteration 7, loss = 0.30642095\n",
      "Iteration 8, loss = 0.29913575\n",
      "Iteration 9, loss = 0.29182255\n",
      "Iteration 10, loss = 0.29886972\n",
      "Iteration 11, loss = 0.29874969\n",
      "Iteration 12, loss = 0.28401646\n",
      "Iteration 13, loss = 0.28735091\n",
      "Iteration 14, loss = 0.29277096\n",
      "Iteration 15, loss = 0.28090091\n",
      "Iteration 16, loss = 0.28376037\n",
      "Iteration 17, loss = 0.29561624\n",
      "Iteration 18, loss = 0.27745587\n",
      "Iteration 19, loss = 0.27463096\n",
      "Iteration 20, loss = 0.27954703\n",
      "Iteration 21, loss = 0.28071557\n",
      "Iteration 22, loss = 0.27581267\n",
      "Iteration 23, loss = 0.28211803\n",
      "Iteration 24, loss = 0.27460174\n",
      "Iteration 25, loss = 0.27419507\n",
      "Iteration 26, loss = 0.28064788\n",
      "Iteration 27, loss = 0.27651367\n",
      "Iteration 28, loss = 0.27514478\n",
      "Iteration 29, loss = 0.27130439\n",
      "Iteration 30, loss = 0.26952141\n",
      "Iteration 31, loss = 0.27813779\n",
      "Iteration 32, loss = 0.26822979\n",
      "Iteration 33, loss = 0.26409080\n",
      "Iteration 34, loss = 0.26880538\n",
      "Iteration 35, loss = 0.27146014\n",
      "Iteration 36, loss = 0.26879669\n",
      "Iteration 37, loss = 0.27306933\n",
      "Iteration 38, loss = 0.27849066\n",
      "Iteration 39, loss = 0.27951046\n",
      "Iteration 40, loss = 0.27310617\n",
      "Iteration 41, loss = 0.27022008\n",
      "Iteration 42, loss = 0.26748070\n",
      "Iteration 43, loss = 0.26589211\n",
      "Iteration 44, loss = 0.27344177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34291501\n",
      "Iteration 2, loss = 0.29500607\n",
      "Iteration 3, loss = 0.28058830\n",
      "Iteration 4, loss = 0.27178575\n",
      "Iteration 5, loss = 0.27609873\n",
      "Iteration 6, loss = 0.27801016\n",
      "Iteration 7, loss = 0.27528812\n",
      "Iteration 8, loss = 0.27252364\n",
      "Iteration 9, loss = 0.27038149\n",
      "Iteration 10, loss = 0.26240489\n",
      "Iteration 11, loss = 0.26627241\n",
      "Iteration 12, loss = 0.26774125\n",
      "Iteration 13, loss = 0.26427118\n",
      "Iteration 14, loss = 0.26522747\n",
      "Iteration 15, loss = 0.26211975\n",
      "Iteration 16, loss = 0.26155282\n",
      "Iteration 17, loss = 0.26259581\n",
      "Iteration 18, loss = 0.26852240\n",
      "Iteration 19, loss = 0.26405049\n",
      "Iteration 20, loss = 0.25933372\n",
      "Iteration 21, loss = 0.26023630\n",
      "Iteration 22, loss = 0.26001604\n",
      "Iteration 23, loss = 0.26179863\n",
      "Iteration 24, loss = 0.25826912\n",
      "Iteration 25, loss = 0.26377833\n",
      "Iteration 26, loss = 0.25846869\n",
      "Iteration 27, loss = 0.26156723\n",
      "Iteration 28, loss = 0.25755943\n",
      "Iteration 29, loss = 0.25935396\n",
      "Iteration 30, loss = 0.26148714\n",
      "Iteration 31, loss = 0.26253635\n",
      "Iteration 32, loss = 0.26294067\n",
      "Iteration 33, loss = 0.26538209\n",
      "Iteration 34, loss = 0.25929151\n",
      "Iteration 35, loss = 0.25784545\n",
      "Iteration 36, loss = 0.26695703\n",
      "Iteration 37, loss = 0.25982008\n",
      "Iteration 38, loss = 0.26353839\n",
      "Iteration 39, loss = 0.25585629\n",
      "Iteration 40, loss = 0.25655704\n",
      "Iteration 41, loss = 0.26105007\n",
      "Iteration 42, loss = 0.25925358\n",
      "Iteration 43, loss = 0.25577151\n",
      "Iteration 44, loss = 0.25501973\n",
      "Iteration 45, loss = 0.25483249\n",
      "Iteration 46, loss = 0.25843106\n",
      "Iteration 47, loss = 0.26708715\n",
      "Iteration 48, loss = 0.26090175\n",
      "Iteration 49, loss = 0.25412638\n",
      "Iteration 50, loss = 0.25729877\n",
      "Iteration 51, loss = 0.25798209\n",
      "Iteration 52, loss = 0.25305552\n",
      "Iteration 53, loss = 0.25570322\n",
      "Iteration 54, loss = 0.25517822\n",
      "Iteration 55, loss = 0.25446731\n",
      "Iteration 56, loss = 0.25493160\n",
      "Iteration 57, loss = 0.25423001\n",
      "Iteration 58, loss = 0.25343570\n",
      "Iteration 59, loss = 0.25394544\n",
      "Iteration 60, loss = 0.25531175\n",
      "Iteration 61, loss = 0.25594552\n",
      "Iteration 62, loss = 0.25496612\n",
      "Iteration 63, loss = 0.25478571\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34046450\n",
      "Iteration 2, loss = 0.29284170\n",
      "Iteration 3, loss = 0.29636928\n",
      "Iteration 4, loss = 0.29038097\n",
      "Iteration 5, loss = 0.28046988\n",
      "Iteration 6, loss = 0.28027077\n",
      "Iteration 7, loss = 0.27241974\n",
      "Iteration 8, loss = 0.26979722\n",
      "Iteration 9, loss = 0.27498592\n",
      "Iteration 10, loss = 0.27103554\n",
      "Iteration 11, loss = 0.27342860\n",
      "Iteration 12, loss = 0.26846742\n",
      "Iteration 13, loss = 0.27085779\n",
      "Iteration 14, loss = 0.27121645\n",
      "Iteration 15, loss = 0.26490563\n",
      "Iteration 16, loss = 0.26872473\n",
      "Iteration 17, loss = 0.26643400\n",
      "Iteration 18, loss = 0.26899433\n",
      "Iteration 19, loss = 0.26454798\n",
      "Iteration 20, loss = 0.27697933\n",
      "Iteration 21, loss = 0.26856130\n",
      "Iteration 22, loss = 0.26171339\n",
      "Iteration 23, loss = 0.25994784\n",
      "Iteration 24, loss = 0.25915033\n",
      "Iteration 25, loss = 0.26295082\n",
      "Iteration 26, loss = 0.26389108\n",
      "Iteration 27, loss = 0.26106324\n",
      "Iteration 28, loss = 0.26101430\n",
      "Iteration 29, loss = 0.26235098\n",
      "Iteration 30, loss = 0.26197620\n",
      "Iteration 31, loss = 0.26112588\n",
      "Iteration 32, loss = 0.26748913\n",
      "Iteration 33, loss = 0.26332203\n",
      "Iteration 34, loss = 0.25861028\n",
      "Iteration 35, loss = 0.25973752\n",
      "Iteration 36, loss = 0.25778477\n",
      "Iteration 37, loss = 0.25819609\n",
      "Iteration 38, loss = 0.25727670\n",
      "Iteration 39, loss = 0.25645869\n",
      "Iteration 40, loss = 0.25629221\n",
      "Iteration 41, loss = 0.26045788\n",
      "Iteration 42, loss = 0.25970154\n",
      "Iteration 43, loss = 0.25997620\n",
      "Iteration 44, loss = 0.25779954\n",
      "Iteration 45, loss = 0.25869989\n",
      "Iteration 46, loss = 0.25951138\n",
      "Iteration 47, loss = 0.25679167\n",
      "Iteration 48, loss = 0.25538106\n",
      "Iteration 49, loss = 0.25529408\n",
      "Iteration 50, loss = 0.25724414\n",
      "Iteration 51, loss = 0.25846311\n",
      "Iteration 52, loss = 0.25817688\n",
      "Iteration 53, loss = 0.25264486\n",
      "Iteration 54, loss = 0.25580812\n",
      "Iteration 55, loss = 0.26435318\n",
      "Iteration 56, loss = 0.25749681\n",
      "Iteration 57, loss = 0.25702260\n",
      "Iteration 58, loss = 0.25832108\n",
      "Iteration 59, loss = 0.25497284\n",
      "Iteration 60, loss = 0.25501213\n",
      "Iteration 61, loss = 0.25485345\n",
      "Iteration 62, loss = 0.25547245\n",
      "Iteration 63, loss = 0.25398175\n",
      "Iteration 64, loss = 0.25532931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.39443838\n",
      "Iteration 2, loss = 0.33292099\n",
      "Iteration 3, loss = 0.32900879\n",
      "Iteration 4, loss = 0.29889072\n",
      "Iteration 5, loss = 0.29166310\n",
      "Iteration 6, loss = 0.29106813\n",
      "Iteration 7, loss = 0.28847920\n",
      "Iteration 8, loss = 0.30751759\n",
      "Iteration 9, loss = 0.30405878\n",
      "Iteration 10, loss = 0.30945946\n",
      "Iteration 11, loss = 0.29037193\n",
      "Iteration 12, loss = 0.28887894\n",
      "Iteration 13, loss = 0.27965071\n",
      "Iteration 14, loss = 0.30863038\n",
      "Iteration 15, loss = 0.28092835\n",
      "Iteration 16, loss = 0.28911840\n",
      "Iteration 17, loss = 0.27822269\n",
      "Iteration 18, loss = 0.27948408\n",
      "Iteration 19, loss = 0.28948319\n",
      "Iteration 20, loss = 0.29806050\n",
      "Iteration 21, loss = 0.28314987\n",
      "Iteration 22, loss = 0.27505109\n",
      "Iteration 23, loss = 0.27916823\n",
      "Iteration 24, loss = 0.27733262\n",
      "Iteration 25, loss = 0.27643289\n",
      "Iteration 26, loss = 0.29130423\n",
      "Iteration 27, loss = 0.28522523\n",
      "Iteration 28, loss = 0.28498711\n",
      "Iteration 29, loss = 0.29282798\n",
      "Iteration 30, loss = 0.28597295\n",
      "Iteration 31, loss = 0.27376143\n",
      "Iteration 32, loss = 0.27422798\n",
      "Iteration 33, loss = 0.27979881\n",
      "Iteration 34, loss = 0.28598303\n",
      "Iteration 35, loss = 0.28640313\n",
      "Iteration 36, loss = 0.28398905\n",
      "Iteration 37, loss = 0.28586615\n",
      "Iteration 38, loss = 0.28168766\n",
      "Iteration 39, loss = 0.28092529\n",
      "Iteration 40, loss = 0.28327101\n",
      "Iteration 41, loss = 0.28525217\n",
      "Iteration 42, loss = 0.28102315\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35012753\n",
      "Iteration 2, loss = 0.31187009\n",
      "Iteration 3, loss = 0.30081213\n",
      "Iteration 4, loss = 0.30203804\n",
      "Iteration 5, loss = 0.29962846\n",
      "Iteration 6, loss = 0.29144054\n",
      "Iteration 7, loss = 0.30352538\n",
      "Iteration 8, loss = 0.28212710\n",
      "Iteration 9, loss = 0.27611418\n",
      "Iteration 10, loss = 0.28331971\n",
      "Iteration 11, loss = 0.27936639\n",
      "Iteration 12, loss = 0.28649040\n",
      "Iteration 13, loss = 0.28354700\n",
      "Iteration 14, loss = 0.28269315\n",
      "Iteration 15, loss = 0.27968561\n",
      "Iteration 16, loss = 0.27436379\n",
      "Iteration 17, loss = 0.27809853\n",
      "Iteration 18, loss = 0.27708354\n",
      "Iteration 19, loss = 0.27994934\n",
      "Iteration 20, loss = 0.28042078\n",
      "Iteration 21, loss = 0.27569442\n",
      "Iteration 22, loss = 0.27933167\n",
      "Iteration 23, loss = 0.27792424\n",
      "Iteration 24, loss = 0.27968770\n",
      "Iteration 25, loss = 0.27708390\n",
      "Iteration 26, loss = 0.28308456\n",
      "Iteration 27, loss = 0.28120179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03730475\n",
      "Iteration 2, loss = 0.52652208\n",
      "Iteration 3, loss = 0.53688611\n",
      "Iteration 1, loss = 0.44875388\n",
      "Iteration 2, loss = 0.32803265\n",
      "Iteration 3, loss = 0.30295411\n",
      "Iteration 4, loss = 0.30525981\n",
      "Iteration 5, loss = 0.28385153\n",
      "Iteration 6, loss = 0.28093854\n",
      "Iteration 7, loss = 0.28211341\n",
      "Iteration 8, loss = 0.27435152\n",
      "Iteration 9, loss = 0.28466128\n",
      "Iteration 10, loss = 0.27983947\n",
      "Iteration 11, loss = 0.27222857\n",
      "Iteration 12, loss = 0.27685946\n",
      "Iteration 13, loss = 0.27565362\n",
      "Iteration 14, loss = 0.27264528\n",
      "Iteration 15, loss = 0.27710601\n",
      "Iteration 16, loss = 0.27124760\n",
      "Iteration 17, loss = 0.26814518\n",
      "Iteration 18, loss = 0.27494164\n",
      "Iteration 19, loss = 0.27865161\n",
      "Iteration 20, loss = 0.27068859\n",
      "Iteration 21, loss = 0.26518025\n",
      "Iteration 22, loss = 0.26931815\n",
      "Iteration 23, loss = 0.26274321\n",
      "Iteration 24, loss = 0.26806973\n",
      "Iteration 25, loss = 0.26914704\n",
      "Iteration 26, loss = 0.26481809\n",
      "Iteration 27, loss = 0.26363173\n",
      "Iteration 28, loss = 0.25824601\n",
      "Iteration 29, loss = 0.26876245\n",
      "Iteration 30, loss = 0.26608196\n",
      "Iteration 31, loss = 0.26632558\n",
      "Iteration 32, loss = 0.26291151\n",
      "Iteration 33, loss = 0.26366030\n",
      "Iteration 34, loss = 0.26617174\n",
      "Iteration 35, loss = 0.26174809\n",
      "Iteration 36, loss = 0.26166652\n",
      "Iteration 37, loss = 0.25816570\n",
      "Iteration 38, loss = 0.25820141\n",
      "Iteration 39, loss = 0.26088375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34291501\n",
      "Iteration 2, loss = 0.29500607\n",
      "Iteration 3, loss = 0.28058830\n",
      "Iteration 4, loss = 0.27178575\n",
      "Iteration 5, loss = 0.27609873\n",
      "Iteration 6, loss = 0.27801016\n",
      "Iteration 7, loss = 0.27528812\n",
      "Iteration 8, loss = 0.27252364\n",
      "Iteration 9, loss = 0.27038149\n",
      "Iteration 10, loss = 0.26240489\n",
      "Iteration 11, loss = 0.26627241\n",
      "Iteration 12, loss = 0.26774125\n",
      "Iteration 13, loss = 0.26427118\n",
      "Iteration 14, loss = 0.26522747\n",
      "Iteration 15, loss = 0.26211975\n",
      "Iteration 16, loss = 0.26155282\n",
      "Iteration 17, loss = 0.26259581\n",
      "Iteration 18, loss = 0.26852240\n",
      "Iteration 19, loss = 0.26405049\n",
      "Iteration 20, loss = 0.25933372\n",
      "Iteration 21, loss = 0.26023630\n",
      "Iteration 22, loss = 0.26001604\n",
      "Iteration 23, loss = 0.26179863\n",
      "Iteration 24, loss = 0.25826912\n",
      "Iteration 25, loss = 0.26377833\n",
      "Iteration 26, loss = 0.25846869\n",
      "Iteration 27, loss = 0.26156723\n",
      "Iteration 28, loss = 0.25755943\n",
      "Iteration 29, loss = 0.25935396\n",
      "Iteration 30, loss = 0.26148714\n",
      "Iteration 31, loss = 0.26253635\n",
      "Iteration 32, loss = 0.26294067\n",
      "Iteration 33, loss = 0.26538209\n",
      "Iteration 34, loss = 0.25929151\n",
      "Iteration 35, loss = 0.25784545\n",
      "Iteration 36, loss = 0.26695703\n",
      "Iteration 37, loss = 0.25982008\n",
      "Iteration 38, loss = 0.26353839\n",
      "Iteration 39, loss = 0.25585629\n",
      "Iteration 40, loss = 0.25655704\n",
      "Iteration 41, loss = 0.26105007\n",
      "Iteration 42, loss = 0.25925358\n",
      "Iteration 43, loss = 0.25577151\n",
      "Iteration 44, loss = 0.25501973\n",
      "Iteration 45, loss = 0.25483249\n",
      "Iteration 46, loss = 0.25843106\n",
      "Iteration 47, loss = 0.26708715\n",
      "Iteration 48, loss = 0.26090175\n",
      "Iteration 49, loss = 0.25412638\n",
      "Iteration 50, loss = 0.25729877\n",
      "Iteration 51, loss = 0.25798209\n",
      "Iteration 52, loss = 0.25305552\n",
      "Iteration 53, loss = 0.25570322\n",
      "Iteration 54, loss = 0.25517822\n",
      "Iteration 55, loss = 0.25446731\n",
      "Iteration 56, loss = 0.25493160\n",
      "Iteration 57, loss = 0.25423001\n",
      "Iteration 58, loss = 0.25343570\n",
      "Iteration 59, loss = 0.25394544\n",
      "Iteration 60, loss = 0.25531175\n",
      "Iteration 61, loss = 0.25594552\n",
      "Iteration 62, loss = 0.25496612\n",
      "Iteration 63, loss = 0.25478571\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33997125\n",
      "Iteration 2, loss = 0.29539368\n",
      "Iteration 3, loss = 0.29507243\n",
      "Iteration 4, loss = 0.28532762\n",
      "Iteration 5, loss = 0.28135136\n",
      "Iteration 6, loss = 0.28502956\n",
      "Iteration 7, loss = 0.27266012\n",
      "Iteration 8, loss = 0.27748809\n",
      "Iteration 9, loss = 0.27056093\n",
      "Iteration 10, loss = 0.27045629\n",
      "Iteration 11, loss = 0.27345408\n",
      "Iteration 12, loss = 0.27578223\n",
      "Iteration 13, loss = 0.26338679\n",
      "Iteration 14, loss = 0.27297334\n",
      "Iteration 15, loss = 0.26640932\n",
      "Iteration 16, loss = 0.26764288\n",
      "Iteration 17, loss = 0.26415566\n",
      "Iteration 18, loss = 0.26213627\n",
      "Iteration 19, loss = 0.26431007\n",
      "Iteration 20, loss = 0.26140680\n",
      "Iteration 21, loss = 0.26196546\n",
      "Iteration 22, loss = 0.26047105\n",
      "Iteration 23, loss = 0.25917369\n",
      "Iteration 24, loss = 0.26111586\n",
      "Iteration 25, loss = 0.25987146\n",
      "Iteration 26, loss = 0.25732424\n",
      "Iteration 27, loss = 0.25997891\n",
      "Iteration 28, loss = 0.25839253\n",
      "Iteration 29, loss = 0.25611018\n",
      "Iteration 30, loss = 0.25857453\n",
      "Iteration 31, loss = 0.26175838\n",
      "Iteration 32, loss = 0.25661687\n",
      "Iteration 33, loss = 0.26037248\n",
      "Iteration 34, loss = 0.26065735\n",
      "Iteration 35, loss = 0.26076017\n",
      "Iteration 36, loss = 0.25959812\n",
      "Iteration 37, loss = 0.25866199\n",
      "Iteration 38, loss = 0.25746707\n",
      "Iteration 39, loss = 0.26004436\n",
      "Iteration 40, loss = 0.26084979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45275526\n",
      "Iteration 2, loss = 0.32560499\n",
      "Iteration 3, loss = 0.31452355\n",
      "Iteration 4, loss = 0.32580662\n",
      "Iteration 5, loss = 0.29341889\n",
      "Iteration 6, loss = 0.28483689\n",
      "Iteration 7, loss = 0.29165098\n",
      "Iteration 8, loss = 0.28636074\n",
      "Iteration 9, loss = 0.28689821\n",
      "Iteration 10, loss = 0.29008393\n",
      "Iteration 11, loss = 0.27521144\n",
      "Iteration 12, loss = 0.27387334\n",
      "Iteration 13, loss = 0.27391479\n",
      "Iteration 14, loss = 0.28197902\n",
      "Iteration 15, loss = 0.27639178\n",
      "Iteration 16, loss = 0.28908648\n",
      "Iteration 17, loss = 0.29012385\n",
      "Iteration 18, loss = 0.28440596\n",
      "Iteration 19, loss = 0.27585607\n",
      "Iteration 20, loss = 0.28933934\n",
      "Iteration 21, loss = 0.27958911\n",
      "Iteration 22, loss = 0.28063368\n",
      "Iteration 23, loss = 0.27714014\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43983587\n",
      "Iteration 2, loss = 0.31729028\n",
      "Iteration 3, loss = 0.31520438\n",
      "Iteration 4, loss = 0.32229455\n",
      "Iteration 5, loss = 0.31791594\n",
      "Iteration 6, loss = 0.31137275\n",
      "Iteration 7, loss = 0.30226321\n",
      "Iteration 8, loss = 0.29729596\n",
      "Iteration 9, loss = 0.28987493\n",
      "Iteration 10, loss = 0.29233973\n",
      "Iteration 11, loss = 0.30081873\n",
      "Iteration 12, loss = 0.28684776\n",
      "Iteration 13, loss = 0.29986260\n",
      "Iteration 14, loss = 0.29266166\n",
      "Iteration 15, loss = 0.28097592\n",
      "Iteration 16, loss = 0.28441976\n",
      "Iteration 17, loss = 0.28720398\n",
      "Iteration 18, loss = 0.28669895\n",
      "Iteration 19, loss = 0.27988257\n",
      "Iteration 20, loss = 0.28899673\n",
      "Iteration 21, loss = 0.27408737\n",
      "Iteration 22, loss = 0.29983929\n",
      "Iteration 23, loss = 0.28418638\n",
      "Iteration 24, loss = 0.29932057\n",
      "Iteration 25, loss = 0.27737012\n",
      "Iteration 26, loss = 0.30741849\n",
      "Iteration 27, loss = 0.29578433\n",
      "Iteration 28, loss = 0.28405771\n",
      "Iteration 29, loss = 0.28218164\n",
      "Iteration 30, loss = 0.28136491\n",
      "Iteration 31, loss = 0.28889972\n",
      "Iteration 32, loss = 0.28979961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34249469\n",
      "Iteration 2, loss = 0.30939975\n",
      "Iteration 3, loss = 0.30104668\n",
      "Iteration 4, loss = 0.30917268\n",
      "Iteration 5, loss = 0.29554753\n",
      "Iteration 6, loss = 0.28846199\n",
      "Iteration 7, loss = 0.28578333\n",
      "Iteration 8, loss = 0.28190714\n",
      "Iteration 9, loss = 0.29203196\n",
      "Iteration 10, loss = 0.29031537\n",
      "Iteration 11, loss = 0.27794009\n",
      "Iteration 12, loss = 0.28747130\n",
      "Iteration 13, loss = 0.29081796\n",
      "Iteration 14, loss = 0.28625215\n",
      "Iteration 15, loss = 0.28232886\n",
      "Iteration 16, loss = 0.28493678\n",
      "Iteration 17, loss = 0.28377445\n",
      "Iteration 18, loss = 0.28456187\n",
      "Iteration 19, loss = 0.28147283\n",
      "Iteration 20, loss = 0.29084189\n",
      "Iteration 21, loss = 0.27825771\n",
      "Iteration 22, loss = 0.27814402\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33858635\n",
      "Iteration 2, loss = 0.30778193\n",
      "Iteration 3, loss = 0.30471595\n",
      "Iteration 4, loss = 0.29040910\n",
      "Iteration 5, loss = 0.30071873\n",
      "Iteration 6, loss = 0.28754639\n",
      "Iteration 7, loss = 0.29352935\n",
      "Iteration 8, loss = 0.28722879\n",
      "Iteration 9, loss = 0.29000302\n",
      "Iteration 10, loss = 0.28573283\n",
      "Iteration 11, loss = 0.29185717\n",
      "Iteration 12, loss = 0.29160123\n",
      "Iteration 13, loss = 0.28589554\n",
      "Iteration 14, loss = 0.28194657\n",
      "Iteration 15, loss = 0.28180095\n",
      "Iteration 16, loss = 0.27576904\n",
      "Iteration 17, loss = 0.28626491\n",
      "Iteration 18, loss = 0.28025919\n",
      "Iteration 19, loss = 0.27900945\n",
      "Iteration 20, loss = 0.27767558\n",
      "Iteration 21, loss = 0.28208661\n",
      "Iteration 22, loss = 0.28280583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39242055\n",
      "Iteration 2, loss = 0.31204405\n",
      "Iteration 3, loss = 0.28957022\n",
      "Iteration 4, loss = 0.29374337\n",
      "Iteration 5, loss = 0.29989757\n",
      "Iteration 6, loss = 0.29258826\n",
      "Iteration 7, loss = 0.28457235\n",
      "Iteration 8, loss = 0.29022549\n",
      "Iteration 9, loss = 0.28188179\n",
      "Iteration 10, loss = 0.26999265\n",
      "Iteration 11, loss = 0.27272760\n",
      "Iteration 12, loss = 0.27325180\n",
      "Iteration 13, loss = 0.27196838\n",
      "Iteration 14, loss = 0.28348588\n",
      "Iteration 15, loss = 0.27541411\n",
      "Iteration 16, loss = 0.27938415\n",
      "Iteration 17, loss = 0.26783011\n",
      "Iteration 18, loss = 0.26672062\n",
      "Iteration 19, loss = 0.27240025\n",
      "Iteration 20, loss = 0.27135273\n",
      "Iteration 21, loss = 0.27228484\n",
      "Iteration 22, loss = 0.27035860\n",
      "Iteration 23, loss = 0.27653132\n",
      "Iteration 24, loss = 0.26120957\n",
      "Iteration 25, loss = 0.27231464\n",
      "Iteration 26, loss = 0.26824774\n",
      "Iteration 27, loss = 0.26252530\n",
      "Iteration 28, loss = 0.27605703\n",
      "Iteration 29, loss = 0.27084002\n",
      "Iteration 30, loss = 0.26141037\n",
      "Iteration 31, loss = 0.27804010\n",
      "Iteration 32, loss = 0.26967168\n",
      "Iteration 33, loss = 0.26427760\n",
      "Iteration 34, loss = 0.26486691\n",
      "Iteration 35, loss = 0.25939272\n",
      "Iteration 36, loss = 0.26296934\n",
      "Iteration 37, loss = 0.26197350\n",
      "Iteration 38, loss = 0.26352083\n",
      "Iteration 39, loss = 0.26417965\n",
      "Iteration 40, loss = 0.25778622\n",
      "Iteration 41, loss = 0.27372531\n",
      "Iteration 42, loss = 0.27316681\n",
      "Iteration 43, loss = 0.26286818\n",
      "Iteration 44, loss = 0.26230359\n",
      "Iteration 45, loss = 0.26592481\n",
      "Iteration 46, loss = 0.26076650\n",
      "Iteration 47, loss = 0.27010382\n",
      "Iteration 48, loss = 0.26818141\n",
      "Iteration 49, loss = 0.26268712\n",
      "Iteration 50, loss = 0.26497388\n",
      "Iteration 51, loss = 0.25999259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32436504\n",
      "Iteration 2, loss = 0.29267955\n",
      "Iteration 3, loss = 0.27552604\n",
      "Iteration 4, loss = 0.28160806\n",
      "Iteration 5, loss = 0.28362334\n",
      "Iteration 6, loss = 0.26966019\n",
      "Iteration 7, loss = 0.27732815\n",
      "Iteration 8, loss = 0.27254651\n",
      "Iteration 9, loss = 0.27190064\n",
      "Iteration 10, loss = 0.26929618\n",
      "Iteration 11, loss = 0.27055575\n",
      "Iteration 12, loss = 0.26526046\n",
      "Iteration 13, loss = 0.27091367\n",
      "Iteration 14, loss = 0.27082958\n",
      "Iteration 15, loss = 0.26368384\n",
      "Iteration 16, loss = 0.26567026\n",
      "Iteration 17, loss = 0.26492746\n",
      "Iteration 18, loss = 0.25943577\n",
      "Iteration 19, loss = 0.26267235\n",
      "Iteration 20, loss = 0.26173274\n",
      "Iteration 21, loss = 0.25806392\n",
      "Iteration 22, loss = 0.25815437\n",
      "Iteration 23, loss = 0.25797938\n",
      "Iteration 24, loss = 0.25945586\n",
      "Iteration 25, loss = 0.26078264\n",
      "Iteration 26, loss = 0.25992750\n",
      "Iteration 27, loss = 0.25892138\n",
      "Iteration 28, loss = 0.26008763\n",
      "Iteration 29, loss = 0.25817315\n",
      "Iteration 30, loss = 0.25928301\n",
      "Iteration 31, loss = 0.25902017\n",
      "Iteration 32, loss = 0.25716729\n",
      "Iteration 33, loss = 0.26002691\n",
      "Iteration 34, loss = 0.25748724\n",
      "Iteration 35, loss = 0.25575005\n",
      "Iteration 36, loss = 0.26041141\n",
      "Iteration 37, loss = 0.25982077\n",
      "Iteration 38, loss = 0.25577404\n",
      "Iteration 39, loss = 0.26273307\n",
      "Iteration 40, loss = 0.25585238\n",
      "Iteration 41, loss = 0.25743137\n",
      "Iteration 42, loss = 0.25781635\n",
      "Iteration 43, loss = 0.25887336\n",
      "Iteration 44, loss = 0.25434334\n",
      "Iteration 45, loss = 0.25761018\n",
      "Iteration 46, loss = 0.25685808\n",
      "Iteration 47, loss = 0.25616351\n",
      "Iteration 48, loss = 0.25479267\n",
      "Iteration 49, loss = 0.25459775\n",
      "Iteration 50, loss = 0.25516402\n",
      "Iteration 51, loss = 0.25597100\n",
      "Iteration 52, loss = 0.25666565\n",
      "Iteration 53, loss = 0.25745901\n",
      "Iteration 54, loss = 0.25780891\n",
      "Iteration 55, loss = 0.25857528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45854977\n",
      "Iteration 2, loss = 0.32705334\n",
      "Iteration 3, loss = 0.30647440\n",
      "Iteration 4, loss = 0.31381893\n",
      "Iteration 5, loss = 0.32287646\n",
      "Iteration 6, loss = 0.30008669\n",
      "Iteration 7, loss = 0.30398073\n",
      "Iteration 8, loss = 0.30161651\n",
      "Iteration 9, loss = 0.29553628\n",
      "Iteration 10, loss = 0.29668539\n",
      "Iteration 11, loss = 0.30049975\n",
      "Iteration 12, loss = 0.29837256\n",
      "Iteration 13, loss = 0.29187693\n",
      "Iteration 14, loss = 0.29996969\n",
      "Iteration 15, loss = 0.29214676\n",
      "Iteration 16, loss = 0.28715203\n",
      "Iteration 17, loss = 0.28777686\n",
      "Iteration 18, loss = 0.29096674\n",
      "Iteration 19, loss = 0.28196483\n",
      "Iteration 20, loss = 0.29316554\n",
      "Iteration 21, loss = 0.29719384\n",
      "Iteration 22, loss = 0.28797157\n",
      "Iteration 23, loss = 0.28802062\n",
      "Iteration 24, loss = 0.30485876\n",
      "Iteration 25, loss = 0.29089130\n",
      "Iteration 26, loss = 0.28562553\n",
      "Iteration 27, loss = 0.27803426\n",
      "Iteration 28, loss = 0.28427327\n",
      "Iteration 29, loss = 0.27953030\n",
      "Iteration 30, loss = 0.28595144\n",
      "Iteration 31, loss = 0.28354627\n",
      "Iteration 32, loss = 0.28929023\n",
      "Iteration 33, loss = 0.28718427\n",
      "Iteration 34, loss = 0.28500774\n",
      "Iteration 35, loss = 0.27457554\n",
      "Iteration 36, loss = 0.28960681\n",
      "Iteration 37, loss = 0.28523049\n",
      "Iteration 38, loss = 0.27958499\n",
      "Iteration 39, loss = 0.27601912\n",
      "Iteration 40, loss = 0.28530363\n",
      "Iteration 41, loss = 0.27911645\n",
      "Iteration 42, loss = 0.27421995\n",
      "Iteration 43, loss = 0.27580120\n",
      "Iteration 44, loss = 0.28816286\n",
      "Iteration 45, loss = 0.27642962\n",
      "Iteration 46, loss = 0.27929834\n",
      "Iteration 47, loss = 0.27471052\n",
      "Iteration 48, loss = 0.28028755\n",
      "Iteration 49, loss = 0.28300192\n",
      "Iteration 50, loss = 0.28902122\n",
      "Iteration 51, loss = 0.27593822\n",
      "Iteration 52, loss = 0.28232583\n",
      "Iteration 53, loss = 0.28158612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34871220\n",
      "Iteration 2, loss = 0.29981910\n",
      "Iteration 3, loss = 0.29185756\n",
      "Iteration 4, loss = 0.30017587\n",
      "Iteration 5, loss = 0.29834249\n",
      "Iteration 6, loss = 0.29422057\n",
      "Iteration 7, loss = 0.30070252\n",
      "Iteration 8, loss = 0.28407037\n",
      "Iteration 9, loss = 0.27963147\n",
      "Iteration 10, loss = 0.28080668\n",
      "Iteration 11, loss = 0.28355050\n",
      "Iteration 12, loss = 0.28023564\n",
      "Iteration 13, loss = 0.28010103\n",
      "Iteration 14, loss = 0.27455026\n",
      "Iteration 15, loss = 0.28526906\n",
      "Iteration 16, loss = 0.27852308\n",
      "Iteration 17, loss = 0.27462149\n",
      "Iteration 18, loss = 0.27743606\n",
      "Iteration 19, loss = 0.28895725\n",
      "Iteration 20, loss = 0.28475098\n",
      "Iteration 21, loss = 0.28036930\n",
      "Iteration 22, loss = 0.27169199\n",
      "Iteration 23, loss = 0.28344635\n",
      "Iteration 24, loss = 0.28039466\n",
      "Iteration 25, loss = 0.28113810\n",
      "Iteration 26, loss = 0.27874194\n",
      "Iteration 27, loss = 0.28485145\n",
      "Iteration 28, loss = 0.27753205\n",
      "Iteration 29, loss = 0.28464029\n",
      "Iteration 30, loss = 0.27865688\n",
      "Iteration 31, loss = 0.27479750\n",
      "Iteration 32, loss = 0.28201473\n",
      "Iteration 33, loss = 0.27963003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33886880\n",
      "Iteration 2, loss = 0.30227519\n",
      "Iteration 3, loss = 0.30093644\n",
      "Iteration 4, loss = 0.29721878\n",
      "Iteration 5, loss = 0.29810112\n",
      "Iteration 6, loss = 0.28372478\n",
      "Iteration 7, loss = 0.29411973\n",
      "Iteration 8, loss = 0.28725877\n",
      "Iteration 9, loss = 0.29052292\n",
      "Iteration 10, loss = 0.28122062\n",
      "Iteration 11, loss = 0.28404076\n",
      "Iteration 12, loss = 0.28360170\n",
      "Iteration 13, loss = 0.28201041\n",
      "Iteration 14, loss = 0.28509947\n",
      "Iteration 15, loss = 0.28321803\n",
      "Iteration 16, loss = 0.28851742\n",
      "Iteration 17, loss = 0.28646807\n",
      "Iteration 18, loss = 0.29269542\n",
      "Iteration 19, loss = 0.29207284\n",
      "Iteration 20, loss = 0.27558027\n",
      "Iteration 21, loss = 0.27571184\n",
      "Iteration 22, loss = 0.28152000\n",
      "Iteration 23, loss = 0.27988077\n",
      "Iteration 24, loss = 0.28031619\n",
      "Iteration 25, loss = 0.29849990\n",
      "Iteration 26, loss = 0.28053530\n",
      "Iteration 27, loss = 0.28798878\n",
      "Iteration 28, loss = 0.28110858\n",
      "Iteration 29, loss = 0.27508688\n",
      "Iteration 30, loss = 0.28056612\n",
      "Iteration 31, loss = 0.28385828\n",
      "Iteration 32, loss = 0.28053998\n",
      "Iteration 33, loss = 0.28835195\n",
      "Iteration 34, loss = 0.27869765\n",
      "Iteration 35, loss = 0.28081795\n",
      "Iteration 36, loss = 0.28706433\n",
      "Iteration 37, loss = 0.28518791\n",
      "Iteration 38, loss = 0.28930397\n",
      "Iteration 39, loss = 0.28047430\n",
      "Iteration 40, loss = 0.27632530\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.01521708\n",
      "Iteration 2, loss = 1.64897489\n",
      "Iteration 3, loss = 1.81415397\n",
      "Iteration 4, loss = 1.47177078\n",
      "Iteration 5, loss = 0.54865839\n",
      "Iteration 6, loss = 0.43726069\n",
      "Iteration 7, loss = 0.38119775\n",
      "Iteration 8, loss = 0.34830063\n",
      "Iteration 9, loss = 0.36633994\n",
      "Iteration 10, loss = 0.34233760\n",
      "Iteration 11, loss = 0.35932068\n",
      "Iteration 1, loss = 0.43228643\n",
      "Iteration 2, loss = 0.32308270\n",
      "Iteration 3, loss = 0.31103518\n",
      "Iteration 4, loss = 0.31969680\n",
      "Iteration 5, loss = 0.31817753\n",
      "Iteration 6, loss = 0.29652956\n",
      "Iteration 7, loss = 0.28340677\n",
      "Iteration 8, loss = 0.28506687\n",
      "Iteration 9, loss = 0.29999942\n",
      "Iteration 10, loss = 0.29030464\n",
      "Iteration 11, loss = 0.28989360\n",
      "Iteration 12, loss = 0.28486162\n",
      "Iteration 13, loss = 0.28209297\n",
      "Iteration 14, loss = 0.28262956\n",
      "Iteration 15, loss = 0.27798160\n",
      "Iteration 16, loss = 0.27096726\n",
      "Iteration 17, loss = 0.26611379\n",
      "Iteration 18, loss = 0.28328057\n",
      "Iteration 19, loss = 0.26881195\n",
      "Iteration 20, loss = 0.27464903\n",
      "Iteration 21, loss = 0.27106633\n",
      "Iteration 22, loss = 0.26892023\n",
      "Iteration 23, loss = 0.27653037\n",
      "Iteration 24, loss = 0.27241722\n",
      "Iteration 25, loss = 0.26666767\n",
      "Iteration 26, loss = 0.26336568\n",
      "Iteration 27, loss = 0.27374789\n",
      "Iteration 28, loss = 0.27803383\n",
      "Iteration 29, loss = 0.27297842\n",
      "Iteration 30, loss = 0.27108207\n",
      "Iteration 31, loss = 0.26726562\n",
      "Iteration 32, loss = 0.26461053\n",
      "Iteration 33, loss = 0.28226236\n",
      "Iteration 34, loss = 0.27006554\n",
      "Iteration 35, loss = 0.26455900\n",
      "Iteration 36, loss = 0.26480816\n",
      "Iteration 37, loss = 0.26141438\n",
      "Iteration 38, loss = 0.26329052\n",
      "Iteration 39, loss = 0.26592469\n",
      "Iteration 40, loss = 0.26645773\n",
      "Iteration 41, loss = 0.28111673\n",
      "Iteration 42, loss = 0.26063417\n",
      "Iteration 43, loss = 0.26738898\n",
      "Iteration 44, loss = 0.25807088\n",
      "Iteration 45, loss = 0.26315012\n",
      "Iteration 46, loss = 0.26131449\n",
      "Iteration 47, loss = 0.26146796\n",
      "Iteration 48, loss = 0.26159466\n",
      "Iteration 49, loss = 0.26666708\n",
      "Iteration 50, loss = 0.26552470\n",
      "Iteration 51, loss = 0.26132345\n",
      "Iteration 52, loss = 0.26157995\n",
      "Iteration 53, loss = 0.25665452\n",
      "Iteration 54, loss = 0.25968546\n",
      "Iteration 55, loss = 0.26635515\n",
      "Iteration 56, loss = 0.26715518\n",
      "Iteration 57, loss = 0.26078040\n",
      "Iteration 58, loss = 0.26302141\n",
      "Iteration 59, loss = 0.25644241\n",
      "Iteration 60, loss = 0.25855108\n",
      "Iteration 61, loss = 0.26141055\n",
      "Iteration 62, loss = 0.26257111\n",
      "Iteration 63, loss = 0.26101854\n",
      "Iteration 64, loss = 0.25992800\n",
      "Iteration 65, loss = 0.26119649\n",
      "Iteration 66, loss = 0.25860744\n",
      "Iteration 67, loss = 0.25541273\n",
      "Iteration 68, loss = 0.25680503\n",
      "Iteration 69, loss = 0.25493068\n",
      "Iteration 70, loss = 0.25879034\n",
      "Iteration 71, loss = 0.25776234\n",
      "Iteration 72, loss = 0.25824165\n",
      "Iteration 73, loss = 0.25984357\n",
      "Iteration 74, loss = 0.26115468\n",
      "Iteration 75, loss = 0.26865868\n",
      "Iteration 76, loss = 0.26604475\n",
      "Iteration 77, loss = 0.26567838\n",
      "Iteration 78, loss = 0.26436652\n",
      "Iteration 79, loss = 0.26626573\n",
      "Iteration 80, loss = 0.26363499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32436504\n",
      "Iteration 2, loss = 0.29267955\n",
      "Iteration 3, loss = 0.27552604\n",
      "Iteration 4, loss = 0.28160806\n",
      "Iteration 5, loss = 0.28362334\n",
      "Iteration 6, loss = 0.26966019\n",
      "Iteration 7, loss = 0.27732815\n",
      "Iteration 8, loss = 0.27254651\n",
      "Iteration 9, loss = 0.27190064\n",
      "Iteration 10, loss = 0.26929618\n",
      "Iteration 11, loss = 0.27055575\n",
      "Iteration 12, loss = 0.26526046\n",
      "Iteration 13, loss = 0.27091367\n",
      "Iteration 14, loss = 0.27082958\n",
      "Iteration 15, loss = 0.26368384\n",
      "Iteration 16, loss = 0.26567026\n",
      "Iteration 17, loss = 0.26492746\n",
      "Iteration 18, loss = 0.25943577\n",
      "Iteration 19, loss = 0.26267235\n",
      "Iteration 20, loss = 0.26173274\n",
      "Iteration 21, loss = 0.25806392\n",
      "Iteration 22, loss = 0.25815437\n",
      "Iteration 23, loss = 0.25797938\n",
      "Iteration 24, loss = 0.25945586\n",
      "Iteration 25, loss = 0.26078264\n",
      "Iteration 26, loss = 0.25992750\n",
      "Iteration 27, loss = 0.25892138\n",
      "Iteration 28, loss = 0.26008763\n",
      "Iteration 29, loss = 0.25817315\n",
      "Iteration 30, loss = 0.25928301\n",
      "Iteration 31, loss = 0.25902017\n",
      "Iteration 32, loss = 0.25716729\n",
      "Iteration 33, loss = 0.26002691\n",
      "Iteration 34, loss = 0.25748724\n",
      "Iteration 35, loss = 0.25575005\n",
      "Iteration 36, loss = 0.26041141\n",
      "Iteration 37, loss = 0.25982077\n",
      "Iteration 38, loss = 0.25577404\n",
      "Iteration 39, loss = 0.26273307\n",
      "Iteration 40, loss = 0.25585238\n",
      "Iteration 41, loss = 0.25743137\n",
      "Iteration 42, loss = 0.25781635\n",
      "Iteration 43, loss = 0.25887336\n",
      "Iteration 44, loss = 0.25434334\n",
      "Iteration 45, loss = 0.25761018\n",
      "Iteration 46, loss = 0.25685808\n",
      "Iteration 47, loss = 0.25616351\n",
      "Iteration 48, loss = 0.25479267\n",
      "Iteration 49, loss = 0.25459775\n",
      "Iteration 50, loss = 0.25516402\n",
      "Iteration 51, loss = 0.25597100\n",
      "Iteration 52, loss = 0.25666565\n",
      "Iteration 53, loss = 0.25745901\n",
      "Iteration 54, loss = 0.25780891\n",
      "Iteration 55, loss = 0.25857528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45854977\n",
      "Iteration 2, loss = 0.32705334\n",
      "Iteration 3, loss = 0.30647440\n",
      "Iteration 4, loss = 0.31381893\n",
      "Iteration 5, loss = 0.32287646\n",
      "Iteration 6, loss = 0.30008669\n",
      "Iteration 7, loss = 0.30398073\n",
      "Iteration 8, loss = 0.30161651\n",
      "Iteration 9, loss = 0.29553628\n",
      "Iteration 10, loss = 0.29668539\n",
      "Iteration 11, loss = 0.30049975\n",
      "Iteration 12, loss = 0.29837256\n",
      "Iteration 13, loss = 0.29187693\n",
      "Iteration 14, loss = 0.29996969\n",
      "Iteration 15, loss = 0.29214676\n",
      "Iteration 16, loss = 0.28715203\n",
      "Iteration 17, loss = 0.28777686\n",
      "Iteration 18, loss = 0.29096674\n",
      "Iteration 19, loss = 0.28196483\n",
      "Iteration 20, loss = 0.29316554\n",
      "Iteration 21, loss = 0.29719384\n",
      "Iteration 22, loss = 0.28797157\n",
      "Iteration 23, loss = 0.28802062\n",
      "Iteration 24, loss = 0.30485876\n",
      "Iteration 25, loss = 0.29089130\n",
      "Iteration 26, loss = 0.28562553\n",
      "Iteration 27, loss = 0.27803426\n",
      "Iteration 28, loss = 0.28427327\n",
      "Iteration 29, loss = 0.27953030\n",
      "Iteration 30, loss = 0.28595144\n",
      "Iteration 31, loss = 0.28354627\n",
      "Iteration 32, loss = 0.28929023\n",
      "Iteration 33, loss = 0.28718427\n",
      "Iteration 34, loss = 0.28500774\n",
      "Iteration 35, loss = 0.27457554\n",
      "Iteration 36, loss = 0.28960681\n",
      "Iteration 37, loss = 0.28523049\n",
      "Iteration 38, loss = 0.27958499\n",
      "Iteration 39, loss = 0.27601912\n",
      "Iteration 40, loss = 0.28530363\n",
      "Iteration 41, loss = 0.27911645\n",
      "Iteration 42, loss = 0.27421995\n",
      "Iteration 43, loss = 0.27580120\n",
      "Iteration 44, loss = 0.28816286\n",
      "Iteration 45, loss = 0.27642962\n",
      "Iteration 46, loss = 0.27929834\n",
      "Iteration 47, loss = 0.27471052\n",
      "Iteration 48, loss = 0.28028755\n",
      "Iteration 49, loss = 0.28300192\n",
      "Iteration 50, loss = 0.28902122\n",
      "Iteration 51, loss = 0.27593822\n",
      "Iteration 52, loss = 0.28232583\n",
      "Iteration 53, loss = 0.28158612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33858635\n",
      "Iteration 2, loss = 0.30778193\n",
      "Iteration 3, loss = 0.30471595\n",
      "Iteration 4, loss = 0.29040910\n",
      "Iteration 5, loss = 0.30071873\n",
      "Iteration 6, loss = 0.28754639\n",
      "Iteration 7, loss = 0.29352935\n",
      "Iteration 8, loss = 0.28722879\n",
      "Iteration 9, loss = 0.29000302\n",
      "Iteration 10, loss = 0.28573283\n",
      "Iteration 11, loss = 0.29185717\n",
      "Iteration 12, loss = 0.29160123\n",
      "Iteration 13, loss = 0.28589554\n",
      "Iteration 14, loss = 0.28194657\n",
      "Iteration 15, loss = 0.28180095\n",
      "Iteration 16, loss = 0.27576904\n",
      "Iteration 17, loss = 0.28626491\n",
      "Iteration 18, loss = 0.28025919\n",
      "Iteration 19, loss = 0.27900945\n",
      "Iteration 20, loss = 0.27767558\n",
      "Iteration 21, loss = 0.28208661\n",
      "Iteration 22, loss = 0.28280583\n",
      "Iteration 23, loss = 0.28182723\n",
      "Iteration 24, loss = 0.27821578\n",
      "Iteration 25, loss = 0.28814125\n",
      "Iteration 26, loss = 0.28696574\n",
      "Iteration 27, loss = 0.27912478\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.62706357\n",
      "Iteration 2, loss = 0.55629896\n",
      "Iteration 3, loss = 0.41722030\n",
      "Iteration 4, loss = 0.40907008\n",
      "Iteration 5, loss = 0.41138787\n",
      "Iteration 6, loss = 0.39820108\n",
      "Iteration 7, loss = 0.36213394\n",
      "Iteration 8, loss = 0.36245695\n",
      "Iteration 9, loss = 0.37257134\n",
      "Iteration 10, loss = 0.38364261\n",
      "Iteration 11, loss = 0.33257169\n",
      "Iteration 12, loss = 0.34460911\n",
      "Iteration 13, loss = 0.34227834\n",
      "Iteration 14, loss = 0.33524577\n",
      "Iteration 15, loss = 0.33397612\n",
      "Iteration 16, loss = 0.33797543\n",
      "Iteration 17, loss = 0.32768054\n",
      "Iteration 18, loss = 0.31329356\n",
      "Iteration 19, loss = 0.30191182\n",
      "Iteration 20, loss = 0.30947770\n",
      "Iteration 21, loss = 0.30191768\n",
      "Iteration 22, loss = 0.29710415\n",
      "Iteration 23, loss = 0.28373661\n",
      "Iteration 24, loss = 0.28913701\n",
      "Iteration 25, loss = 0.27867094\n",
      "Iteration 26, loss = 0.28857820\n",
      "Iteration 27, loss = 0.26955110\n",
      "Iteration 28, loss = 0.27159739\n",
      "Iteration 29, loss = 0.26655253\n",
      "Iteration 30, loss = 0.29026483\n",
      "Iteration 31, loss = 0.28287326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.43228643\n",
      "Iteration 2, loss = 0.32308270\n",
      "Iteration 3, loss = 0.31103518\n",
      "Iteration 4, loss = 0.31969680\n",
      "Iteration 5, loss = 0.31817753\n",
      "Iteration 6, loss = 0.29652956\n",
      "Iteration 7, loss = 0.28340677\n",
      "Iteration 8, loss = 0.28506687\n",
      "Iteration 9, loss = 0.29999942\n",
      "Iteration 10, loss = 0.29030464\n",
      "Iteration 11, loss = 0.28989360\n",
      "Iteration 12, loss = 0.28486162\n",
      "Iteration 13, loss = 0.28209297\n",
      "Iteration 14, loss = 0.28262956\n",
      "Iteration 15, loss = 0.27798160\n",
      "Iteration 16, loss = 0.27096726\n",
      "Iteration 17, loss = 0.26611379\n",
      "Iteration 18, loss = 0.28328057\n",
      "Iteration 19, loss = 0.26881195\n",
      "Iteration 20, loss = 0.27464903\n",
      "Iteration 21, loss = 0.27106633\n",
      "Iteration 22, loss = 0.26892023\n",
      "Iteration 23, loss = 0.27653037\n",
      "Iteration 24, loss = 0.27241722\n",
      "Iteration 25, loss = 0.26666767\n",
      "Iteration 26, loss = 0.26336568\n",
      "Iteration 27, loss = 0.27374789\n",
      "Iteration 28, loss = 0.27803383\n",
      "Iteration 29, loss = 0.27297842\n",
      "Iteration 30, loss = 0.27108207\n",
      "Iteration 31, loss = 0.26726562\n",
      "Iteration 32, loss = 0.26461053\n",
      "Iteration 33, loss = 0.28226236\n",
      "Iteration 34, loss = 0.27006554\n",
      "Iteration 35, loss = 0.26455900\n",
      "Iteration 36, loss = 0.26480816\n",
      "Iteration 37, loss = 0.26141438\n",
      "Iteration 38, loss = 0.26329052\n",
      "Iteration 39, loss = 0.26592469\n",
      "Iteration 40, loss = 0.26645773\n",
      "Iteration 41, loss = 0.28111673\n",
      "Iteration 42, loss = 0.26063417\n",
      "Iteration 43, loss = 0.26738898\n",
      "Iteration 44, loss = 0.25807088\n",
      "Iteration 45, loss = 0.26315012\n",
      "Iteration 46, loss = 0.26131449\n",
      "Iteration 47, loss = 0.26146796\n",
      "Iteration 48, loss = 0.26159466\n",
      "Iteration 49, loss = 0.26666708\n",
      "Iteration 50, loss = 0.26552470\n",
      "Iteration 51, loss = 0.26132345\n",
      "Iteration 52, loss = 0.26157995\n",
      "Iteration 53, loss = 0.25665452\n",
      "Iteration 54, loss = 0.25968546\n",
      "Iteration 55, loss = 0.26635515\n",
      "Iteration 56, loss = 0.26715518\n",
      "Iteration 57, loss = 0.26078040\n",
      "Iteration 58, loss = 0.26302141\n",
      "Iteration 59, loss = 0.25644241\n",
      "Iteration 60, loss = 0.25855108\n",
      "Iteration 61, loss = 0.26141055\n",
      "Iteration 62, loss = 0.26257111\n",
      "Iteration 63, loss = 0.26101854\n",
      "Iteration 64, loss = 0.25992800\n",
      "Iteration 65, loss = 0.26119649\n",
      "Iteration 66, loss = 0.25860744\n",
      "Iteration 67, loss = 0.25541273\n",
      "Iteration 68, loss = 0.25680503\n",
      "Iteration 69, loss = 0.25493068\n",
      "Iteration 70, loss = 0.25879034\n",
      "Iteration 71, loss = 0.25776234\n",
      "Iteration 72, loss = 0.25824165\n",
      "Iteration 73, loss = 0.25984357\n",
      "Iteration 74, loss = 0.26115468\n",
      "Iteration 75, loss = 0.26865868\n",
      "Iteration 76, loss = 0.26604475\n",
      "Iteration 77, loss = 0.26567838\n",
      "Iteration 78, loss = 0.26436652\n",
      "Iteration 79, loss = 0.26626573\n",
      "Iteration 80, loss = 0.26363499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32434942\n",
      "Iteration 2, loss = 0.29985038\n",
      "Iteration 3, loss = 0.29090230\n",
      "Iteration 4, loss = 0.28850457\n",
      "Iteration 5, loss = 0.27272719\n",
      "Iteration 6, loss = 0.27055070\n",
      "Iteration 7, loss = 0.27346628\n",
      "Iteration 8, loss = 0.27117008\n",
      "Iteration 9, loss = 0.26830509\n",
      "Iteration 10, loss = 0.26956159\n",
      "Iteration 11, loss = 0.26846116\n",
      "Iteration 12, loss = 0.27014146\n",
      "Iteration 13, loss = 0.27058057\n",
      "Iteration 14, loss = 0.27079866\n",
      "Iteration 15, loss = 0.26668562\n",
      "Iteration 16, loss = 0.26012376\n",
      "Iteration 17, loss = 0.25996659\n",
      "Iteration 18, loss = 0.26242659\n",
      "Iteration 19, loss = 0.26204022\n",
      "Iteration 20, loss = 0.26099314\n",
      "Iteration 21, loss = 0.26173423\n",
      "Iteration 22, loss = 0.25898780\n",
      "Iteration 23, loss = 0.25884459\n",
      "Iteration 24, loss = 0.25740253\n",
      "Iteration 25, loss = 0.25961267\n",
      "Iteration 26, loss = 0.25906162\n",
      "Iteration 27, loss = 0.25823804\n",
      "Iteration 28, loss = 0.25670945\n",
      "Iteration 29, loss = 0.25745858\n",
      "Iteration 30, loss = 0.25844053\n",
      "Iteration 31, loss = 0.25661233\n",
      "Iteration 32, loss = 0.25714210\n",
      "Iteration 33, loss = 0.25988589\n",
      "Iteration 34, loss = 0.26075137\n",
      "Iteration 35, loss = 0.26219823\n",
      "Iteration 36, loss = 0.25861344\n",
      "Iteration 37, loss = 0.25743115\n",
      "Iteration 38, loss = 0.26019598\n",
      "Iteration 39, loss = 0.25649596\n",
      "Iteration 40, loss = 0.25749612\n",
      "Iteration 41, loss = 0.26454367\n",
      "Iteration 42, loss = 0.26070327\n",
      "Iteration 43, loss = 0.25566664\n",
      "Iteration 44, loss = 0.25738582\n",
      "Iteration 45, loss = 0.25571767\n",
      "Iteration 46, loss = 0.25669669\n",
      "Iteration 47, loss = 0.25949312\n",
      "Iteration 48, loss = 0.25554268\n",
      "Iteration 49, loss = 0.25494504\n",
      "Iteration 50, loss = 0.25725210\n",
      "Iteration 51, loss = 0.25634936\n",
      "Iteration 52, loss = 0.25539986\n",
      "Iteration 53, loss = 0.25883489\n",
      "Iteration 54, loss = 0.25959323\n",
      "Iteration 55, loss = 0.26029152\n",
      "Iteration 56, loss = 0.25601634\n",
      "Iteration 57, loss = 0.25934361\n",
      "Iteration 58, loss = 0.25375830\n",
      "Iteration 59, loss = 0.25648503\n",
      "Iteration 60, loss = 0.25246916\n",
      "Iteration 61, loss = 0.25528138\n",
      "Iteration 62, loss = 0.26105891\n",
      "Iteration 63, loss = 0.25807591\n",
      "Iteration 64, loss = 0.25602675\n",
      "Iteration 65, loss = 0.25378010\n",
      "Iteration 66, loss = 0.25463779\n",
      "Iteration 67, loss = 0.25567661\n",
      "Iteration 68, loss = 0.25425159\n",
      "Iteration 69, loss = 0.25357130\n",
      "Iteration 70, loss = 0.25369497\n",
      "Iteration 71, loss = 0.25404609\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34249469\n",
      "Iteration 2, loss = 0.30939975\n",
      "Iteration 3, loss = 0.30104668\n",
      "Iteration 4, loss = 0.30917268\n",
      "Iteration 5, loss = 0.29554753\n",
      "Iteration 6, loss = 0.28846199\n",
      "Iteration 7, loss = 0.28578333\n",
      "Iteration 8, loss = 0.28190714\n",
      "Iteration 9, loss = 0.29203196\n",
      "Iteration 10, loss = 0.29031537\n",
      "Iteration 11, loss = 0.27794009\n",
      "Iteration 12, loss = 0.28747130\n",
      "Iteration 13, loss = 0.29081796\n",
      "Iteration 14, loss = 0.28625215\n",
      "Iteration 15, loss = 0.28232886\n",
      "Iteration 16, loss = 0.28493678\n",
      "Iteration 17, loss = 0.28377445\n",
      "Iteration 18, loss = 0.28456187\n",
      "Iteration 19, loss = 0.28147283\n",
      "Iteration 20, loss = 0.29084189\n",
      "Iteration 21, loss = 0.27825771\n",
      "Iteration 22, loss = 0.27814402\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34772470\n",
      "Iteration 2, loss = 0.30868981\n",
      "Iteration 3, loss = 0.30100217\n",
      "Iteration 4, loss = 0.29633502\n",
      "Iteration 5, loss = 0.28880930\n",
      "Iteration 6, loss = 0.29520778\n",
      "Iteration 7, loss = 0.29105342\n",
      "Iteration 8, loss = 0.28922895\n",
      "Iteration 9, loss = 0.28774358\n",
      "Iteration 10, loss = 0.29083045\n",
      "Iteration 11, loss = 0.29504800\n",
      "Iteration 12, loss = 0.28148057\n",
      "Iteration 13, loss = 0.28895930\n",
      "Iteration 14, loss = 0.28373150\n",
      "Iteration 15, loss = 0.28125956\n",
      "Iteration 16, loss = 0.28586745\n",
      "Iteration 17, loss = 0.28441730\n",
      "Iteration 18, loss = 0.28368598\n",
      "Iteration 19, loss = 0.29072603\n",
      "Iteration 20, loss = 0.28072587\n",
      "Iteration 21, loss = 0.28169707\n",
      "Iteration 22, loss = 0.28171458\n",
      "Iteration 23, loss = 0.27588566\n",
      "Iteration 24, loss = 0.28317490\n",
      "Iteration 25, loss = 0.28260859\n",
      "Iteration 26, loss = 0.27978162\n",
      "Iteration 27, loss = 0.28348853\n",
      "Iteration 28, loss = 0.27940686\n",
      "Iteration 29, loss = 0.30550173\n",
      "Iteration 30, loss = 0.28348657\n",
      "Iteration 31, loss = 0.28131422\n",
      "Iteration 32, loss = 0.29738083\n",
      "Iteration 33, loss = 0.28740456\n",
      "Iteration 34, loss = 0.28238663\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.01521708\n",
      "Iteration 2, loss = 1.64897489\n",
      "Iteration 3, loss = 1.81415397\n",
      "Iteration 4, loss = 1.47177078\n",
      "Iteration 5, loss = 0.54865839\n",
      "Iteration 6, loss = 0.43726069\n",
      "Iteration 7, loss = 0.38119775\n",
      "Iteration 8, loss = 0.34830063\n",
      "Iteration 9, loss = 0.36633994\n",
      "Iteration 10, loss = 0.34233760\n",
      "Iteration 11, loss = 0.35932068\n",
      "Iteration 12, loss = 0.33843686\n",
      "Iteration 13, loss = 0.33073238\n",
      "Iteration 14, loss = 0.34857813\n",
      "Iteration 15, loss = 0.32466532\n",
      "Iteration 16, loss = 0.31929802\n",
      "Iteration 17, loss = 0.30489573\n",
      "Iteration 18, loss = 0.30278815\n",
      "Iteration 19, loss = 0.30386948\n",
      "Iteration 20, loss = 0.31009412\n",
      "Iteration 21, loss = 0.29126980\n",
      "Iteration 22, loss = 0.29962275\n",
      "Iteration 23, loss = 0.29676968\n",
      "Iteration 24, loss = 0.28835326\n",
      "Iteration 25, loss = 0.29539973\n",
      "Iteration 26, loss = 0.30785738\n",
      "Iteration 27, loss = 0.29848506\n",
      "Iteration 28, loss = 0.28663958\n",
      "Iteration 29, loss = 0.29018834\n",
      "Iteration 30, loss = 0.28354246\n",
      "Iteration 31, loss = 0.28385747\n",
      "Iteration 32, loss = 0.28169338\n",
      "Iteration 33, loss = 0.27131886\n",
      "Iteration 34, loss = 0.27636691\n",
      "Iteration 35, loss = 0.29458031\n",
      "Iteration 36, loss = 0.28171488\n",
      "Iteration 37, loss = 0.27392482\n",
      "Iteration 38, loss = 0.27618959\n",
      "Iteration 39, loss = 0.26012561\n",
      "Iteration 1, loss = 0.44875388\n",
      "Iteration 2, loss = 0.32803265\n",
      "Iteration 3, loss = 0.30295411\n",
      "Iteration 4, loss = 0.30525981\n",
      "Iteration 5, loss = 0.28385153\n",
      "Iteration 6, loss = 0.28093854\n",
      "Iteration 7, loss = 0.28211341\n",
      "Iteration 8, loss = 0.27435152\n",
      "Iteration 9, loss = 0.28466128\n",
      "Iteration 10, loss = 0.27983947\n",
      "Iteration 11, loss = 0.27222857\n",
      "Iteration 12, loss = 0.27685946\n",
      "Iteration 13, loss = 0.27565362\n",
      "Iteration 14, loss = 0.27264528\n",
      "Iteration 15, loss = 0.27710601\n",
      "Iteration 16, loss = 0.27124760\n",
      "Iteration 17, loss = 0.26814518\n",
      "Iteration 18, loss = 0.27494164\n",
      "Iteration 19, loss = 0.27865161\n",
      "Iteration 20, loss = 0.27068859\n",
      "Iteration 21, loss = 0.26518025\n",
      "Iteration 22, loss = 0.26931815\n",
      "Iteration 23, loss = 0.26274321\n",
      "Iteration 24, loss = 0.26806973\n",
      "Iteration 25, loss = 0.26914704\n",
      "Iteration 26, loss = 0.26481809\n",
      "Iteration 27, loss = 0.26363173\n",
      "Iteration 28, loss = 0.25824601\n",
      "Iteration 29, loss = 0.26876245\n",
      "Iteration 30, loss = 0.26608196\n",
      "Iteration 31, loss = 0.26632558\n",
      "Iteration 32, loss = 0.26291151\n",
      "Iteration 33, loss = 0.26366030\n",
      "Iteration 34, loss = 0.26617174\n",
      "Iteration 35, loss = 0.26174809\n",
      "Iteration 36, loss = 0.26166652\n",
      "Iteration 37, loss = 0.25816570\n",
      "Iteration 38, loss = 0.25820141\n",
      "Iteration 39, loss = 0.26088375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33151311\n",
      "Iteration 2, loss = 0.28947453\n",
      "Iteration 3, loss = 0.27905612\n",
      "Iteration 4, loss = 0.27487679\n",
      "Iteration 5, loss = 0.27349553\n",
      "Iteration 6, loss = 0.27349734\n",
      "Iteration 7, loss = 0.26545096\n",
      "Iteration 8, loss = 0.26780056\n",
      "Iteration 9, loss = 0.27019535\n",
      "Iteration 10, loss = 0.26352828\n",
      "Iteration 11, loss = 0.26251522\n",
      "Iteration 12, loss = 0.26131800\n",
      "Iteration 13, loss = 0.26319210\n",
      "Iteration 14, loss = 0.26076037\n",
      "Iteration 15, loss = 0.26463225\n",
      "Iteration 16, loss = 0.26420030\n",
      "Iteration 17, loss = 0.26245610\n",
      "Iteration 18, loss = 0.25809431\n",
      "Iteration 19, loss = 0.25912869\n",
      "Iteration 20, loss = 0.25764032\n",
      "Iteration 21, loss = 0.25622206\n",
      "Iteration 22, loss = 0.26324904\n",
      "Iteration 23, loss = 0.25670276\n",
      "Iteration 24, loss = 0.25505707\n",
      "Iteration 25, loss = 0.25654553\n",
      "Iteration 26, loss = 0.25630235\n",
      "Iteration 27, loss = 0.25776975\n",
      "Iteration 28, loss = 0.25645046\n",
      "Iteration 29, loss = 0.25730915\n",
      "Iteration 30, loss = 0.25621657\n",
      "Iteration 31, loss = 0.25539488\n",
      "Iteration 32, loss = 0.25624228\n",
      "Iteration 33, loss = 0.25360930\n",
      "Iteration 34, loss = 0.25557961\n",
      "Iteration 35, loss = 0.25553889\n",
      "Iteration 36, loss = 0.25778810\n",
      "Iteration 37, loss = 0.25508887\n",
      "Iteration 38, loss = 0.25414387\n",
      "Iteration 39, loss = 0.25342958\n",
      "Iteration 40, loss = 0.26147934\n",
      "Iteration 41, loss = 0.25905275\n",
      "Iteration 42, loss = 0.25461603\n",
      "Iteration 43, loss = 0.25411535\n",
      "Iteration 44, loss = 0.25944967\n",
      "Iteration 45, loss = 0.25393417\n",
      "Iteration 46, loss = 0.25428147\n",
      "Iteration 47, loss = 0.25308785\n",
      "Iteration 48, loss = 0.25401726\n",
      "Iteration 49, loss = 0.25564002\n",
      "Iteration 50, loss = 0.25591395\n",
      "Iteration 51, loss = 0.25577974\n",
      "Iteration 52, loss = 0.25217546\n",
      "Iteration 53, loss = 0.25247073\n",
      "Iteration 54, loss = 0.25170026\n",
      "Iteration 55, loss = 0.25297521\n",
      "Iteration 56, loss = 0.25436681\n",
      "Iteration 57, loss = 0.25361548\n",
      "Iteration 58, loss = 0.25426128\n",
      "Iteration 59, loss = 0.25122683\n",
      "Iteration 60, loss = 0.25344580\n",
      "Iteration 61, loss = 0.25222897\n",
      "Iteration 62, loss = 0.25551096\n",
      "Iteration 63, loss = 0.25327813\n",
      "Iteration 64, loss = 0.25419450\n",
      "Iteration 65, loss = 0.25240072\n",
      "Iteration 66, loss = 0.25018247\n",
      "Iteration 67, loss = 0.25251785\n",
      "Iteration 68, loss = 0.25390283\n",
      "Iteration 69, loss = 0.25459611\n",
      "Iteration 70, loss = 0.25112403\n",
      "Iteration 71, loss = 0.25181009\n",
      "Iteration 72, loss = 0.25375163\n",
      "Iteration 73, loss = 0.25159532\n",
      "Iteration 74, loss = 0.25060620\n",
      "Iteration 75, loss = 0.25239002\n",
      "Iteration 76, loss = 0.25120615\n",
      "Iteration 77, loss = 0.25238745\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34046450\n",
      "Iteration 2, loss = 0.29284170\n",
      "Iteration 3, loss = 0.29636928\n",
      "Iteration 4, loss = 0.29038097\n",
      "Iteration 5, loss = 0.28046988\n",
      "Iteration 6, loss = 0.28027077\n",
      "Iteration 7, loss = 0.27241974\n",
      "Iteration 8, loss = 0.26979722\n",
      "Iteration 9, loss = 0.27498592\n",
      "Iteration 10, loss = 0.27103554\n",
      "Iteration 11, loss = 0.27342860\n",
      "Iteration 12, loss = 0.26846742\n",
      "Iteration 13, loss = 0.27085779\n",
      "Iteration 14, loss = 0.27121645\n",
      "Iteration 15, loss = 0.26490563\n",
      "Iteration 16, loss = 0.26872473\n",
      "Iteration 17, loss = 0.26643400\n",
      "Iteration 18, loss = 0.26899433\n",
      "Iteration 19, loss = 0.26454798\n",
      "Iteration 20, loss = 0.27697933\n",
      "Iteration 21, loss = 0.26856130\n",
      "Iteration 22, loss = 0.26171339\n",
      "Iteration 23, loss = 0.25994784\n",
      "Iteration 24, loss = 0.25915033\n",
      "Iteration 25, loss = 0.26295082\n",
      "Iteration 26, loss = 0.26389108\n",
      "Iteration 27, loss = 0.26106324\n",
      "Iteration 28, loss = 0.26101430\n",
      "Iteration 29, loss = 0.26235098\n",
      "Iteration 30, loss = 0.26197620\n",
      "Iteration 31, loss = 0.26112588\n",
      "Iteration 32, loss = 0.26748913\n",
      "Iteration 33, loss = 0.26332203\n",
      "Iteration 34, loss = 0.25861028\n",
      "Iteration 35, loss = 0.25973752\n",
      "Iteration 36, loss = 0.25778477\n",
      "Iteration 37, loss = 0.25819609\n",
      "Iteration 38, loss = 0.25727670\n",
      "Iteration 39, loss = 0.25645869\n",
      "Iteration 40, loss = 0.25629221\n",
      "Iteration 41, loss = 0.26045788\n",
      "Iteration 42, loss = 0.25970154\n",
      "Iteration 43, loss = 0.25997620\n",
      "Iteration 44, loss = 0.25779954\n",
      "Iteration 45, loss = 0.25869989\n",
      "Iteration 46, loss = 0.25951138\n",
      "Iteration 47, loss = 0.25679167\n",
      "Iteration 48, loss = 0.25538106\n",
      "Iteration 49, loss = 0.25529408\n",
      "Iteration 50, loss = 0.25724414\n",
      "Iteration 51, loss = 0.25846311\n",
      "Iteration 52, loss = 0.25817688\n",
      "Iteration 53, loss = 0.25264486\n",
      "Iteration 54, loss = 0.25580812\n",
      "Iteration 55, loss = 0.26435318\n",
      "Iteration 56, loss = 0.25749681\n",
      "Iteration 57, loss = 0.25702260\n",
      "Iteration 58, loss = 0.25832108\n",
      "Iteration 59, loss = 0.25497284\n",
      "Iteration 60, loss = 0.25501213\n",
      "Iteration 61, loss = 0.25485345\n",
      "Iteration 62, loss = 0.25547245\n",
      "Iteration 63, loss = 0.25398175\n",
      "Iteration 64, loss = 0.25532931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40000571\n",
      "Iteration 2, loss = 0.37220302\n",
      "Iteration 3, loss = 0.32016539\n",
      "Iteration 4, loss = 0.30737767\n",
      "Iteration 5, loss = 0.30925138\n",
      "Iteration 6, loss = 0.29328554\n",
      "Iteration 7, loss = 0.30448788\n",
      "Iteration 8, loss = 0.29331085\n",
      "Iteration 9, loss = 0.29151593\n",
      "Iteration 10, loss = 0.29175163\n",
      "Iteration 11, loss = 0.29018645\n",
      "Iteration 12, loss = 0.27903685\n",
      "Iteration 13, loss = 0.28337694\n",
      "Iteration 14, loss = 0.28741029\n",
      "Iteration 15, loss = 0.28446943\n",
      "Iteration 16, loss = 0.28255836\n",
      "Iteration 17, loss = 0.28286942\n",
      "Iteration 18, loss = 0.28012104\n",
      "Iteration 19, loss = 0.28975265\n",
      "Iteration 20, loss = 0.28423648\n",
      "Iteration 21, loss = 0.28825179\n",
      "Iteration 22, loss = 0.28541930\n",
      "Iteration 23, loss = 0.28465797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33242854\n",
      "Iteration 2, loss = 0.31074654\n",
      "Iteration 3, loss = 0.29896558\n",
      "Iteration 4, loss = 0.28379778\n",
      "Iteration 5, loss = 0.28541444\n",
      "Iteration 6, loss = 0.28944530\n",
      "Iteration 7, loss = 0.29107797\n",
      "Iteration 8, loss = 0.28482574\n",
      "Iteration 9, loss = 0.27932808\n",
      "Iteration 10, loss = 0.28020947\n",
      "Iteration 11, loss = 0.27995085\n",
      "Iteration 12, loss = 0.28132452\n",
      "Iteration 13, loss = 0.28209399\n",
      "Iteration 14, loss = 0.29129478\n",
      "Iteration 15, loss = 0.28026812\n",
      "Iteration 16, loss = 0.27967520\n",
      "Iteration 17, loss = 0.27887279\n",
      "Iteration 18, loss = 0.27645326\n",
      "Iteration 19, loss = 0.27746107\n",
      "Iteration 20, loss = 0.28138703\n",
      "Iteration 21, loss = 0.27854840\n",
      "Iteration 22, loss = 0.28120016\n",
      "Iteration 23, loss = 0.28509941\n",
      "Iteration 24, loss = 0.28299080\n",
      "Iteration 25, loss = 0.27729659\n",
      "Iteration 26, loss = 0.27421597\n",
      "Iteration 27, loss = 0.27801730\n",
      "Iteration 28, loss = 0.28217224\n",
      "Iteration 29, loss = 0.27550744\n",
      "Iteration 30, loss = 0.27389538\n",
      "Iteration 31, loss = 0.27250447\n",
      "Iteration 32, loss = 0.27927032\n",
      "Iteration 33, loss = 0.28050985\n",
      "Iteration 34, loss = 0.27376671\n",
      "Iteration 35, loss = 0.27917285\n",
      "Iteration 36, loss = 0.28395756\n",
      "Iteration 37, loss = 0.27420055\n",
      "Iteration 38, loss = 0.27695567\n",
      "Iteration 39, loss = 0.28316998\n",
      "Iteration 40, loss = 0.27682415\n",
      "Iteration 41, loss = 0.27439887\n",
      "Iteration 42, loss = 0.28077775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.42151894\n",
      "Iteration 2, loss = 0.34186727\n",
      "Iteration 3, loss = 0.31679228\n",
      "Iteration 4, loss = 0.30287305\n",
      "Iteration 5, loss = 0.29309950\n",
      "Iteration 6, loss = 0.28576152\n",
      "Iteration 7, loss = 0.28978015\n",
      "Iteration 8, loss = 0.28198268\n",
      "Iteration 9, loss = 0.28089363\n",
      "Iteration 10, loss = 0.27712700\n",
      "Iteration 11, loss = 0.27628600\n",
      "Iteration 12, loss = 0.29583465\n",
      "Iteration 13, loss = 0.28339407\n",
      "Iteration 14, loss = 0.28101336\n",
      "Iteration 15, loss = 0.27754933\n",
      "Iteration 16, loss = 0.27840929\n",
      "Iteration 17, loss = 0.28164941\n",
      "Iteration 18, loss = 0.28252430\n",
      "Iteration 19, loss = 0.27841760\n",
      "Iteration 20, loss = 0.28397716\n",
      "Iteration 21, loss = 0.27713089\n",
      "Iteration 22, loss = 0.27606080\n",
      "Iteration 23, loss = 0.27220012\n",
      "Iteration 24, loss = 0.26823327\n",
      "Iteration 25, loss = 0.27431739\n",
      "Iteration 26, loss = 0.26965281\n",
      "Iteration 27, loss = 0.26360360\n",
      "Iteration 28, loss = 0.26928454\n",
      "Iteration 29, loss = 0.26748154\n",
      "Iteration 30, loss = 0.26909094\n",
      "Iteration 31, loss = 0.26898226\n",
      "Iteration 32, loss = 0.26927090\n",
      "Iteration 33, loss = 0.26961169\n",
      "Iteration 34, loss = 0.27268473\n",
      "Iteration 35, loss = 0.26548275\n",
      "Iteration 36, loss = 0.26328813\n",
      "Iteration 37, loss = 0.26824222\n",
      "Iteration 38, loss = 0.26555288\n",
      "Iteration 39, loss = 0.26849856\n",
      "Iteration 40, loss = 0.26605283\n",
      "Iteration 41, loss = 0.26944256\n",
      "Iteration 42, loss = 0.26130335\n",
      "Iteration 43, loss = 0.26077314\n",
      "Iteration 44, loss = 0.25786360\n",
      "Iteration 45, loss = 0.26107840\n",
      "Iteration 46, loss = 0.26205652\n",
      "Iteration 47, loss = 0.26292893\n",
      "Iteration 48, loss = 0.26386465\n",
      "Iteration 49, loss = 0.26263117\n",
      "Iteration 50, loss = 0.26438390\n",
      "Iteration 51, loss = 0.26133263\n",
      "Iteration 52, loss = 0.26193811\n",
      "Iteration 53, loss = 0.26047349\n",
      "Iteration 54, loss = 0.26515849\n",
      "Iteration 55, loss = 0.26781119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32434942\n",
      "Iteration 2, loss = 0.29985038\n",
      "Iteration 3, loss = 0.29090230\n",
      "Iteration 4, loss = 0.28850457\n",
      "Iteration 5, loss = 0.27272719\n",
      "Iteration 6, loss = 0.27055070\n",
      "Iteration 7, loss = 0.27346628\n",
      "Iteration 8, loss = 0.27117008\n",
      "Iteration 9, loss = 0.26830509\n",
      "Iteration 10, loss = 0.26956159\n",
      "Iteration 11, loss = 0.26846116\n",
      "Iteration 12, loss = 0.27014146\n",
      "Iteration 13, loss = 0.27058057\n",
      "Iteration 14, loss = 0.27079866\n",
      "Iteration 15, loss = 0.26668562\n",
      "Iteration 16, loss = 0.26012376\n",
      "Iteration 17, loss = 0.25996659\n",
      "Iteration 18, loss = 0.26242659\n",
      "Iteration 19, loss = 0.26204022\n",
      "Iteration 20, loss = 0.26099314\n",
      "Iteration 21, loss = 0.26173423\n",
      "Iteration 22, loss = 0.25898780\n",
      "Iteration 23, loss = 0.25884459\n",
      "Iteration 24, loss = 0.25740253\n",
      "Iteration 25, loss = 0.25961267\n",
      "Iteration 26, loss = 0.25906162\n",
      "Iteration 27, loss = 0.25823804\n",
      "Iteration 28, loss = 0.25670945\n",
      "Iteration 29, loss = 0.25745858\n",
      "Iteration 30, loss = 0.25844053\n",
      "Iteration 31, loss = 0.25661233\n",
      "Iteration 32, loss = 0.25714210\n",
      "Iteration 33, loss = 0.25988589\n",
      "Iteration 34, loss = 0.26075137\n",
      "Iteration 35, loss = 0.26219823\n",
      "Iteration 36, loss = 0.25861344\n",
      "Iteration 37, loss = 0.25743115\n",
      "Iteration 38, loss = 0.26019598\n",
      "Iteration 39, loss = 0.25649596\n",
      "Iteration 40, loss = 0.25749612\n",
      "Iteration 41, loss = 0.26454367\n",
      "Iteration 42, loss = 0.26070327\n",
      "Iteration 43, loss = 0.25566664\n",
      "Iteration 44, loss = 0.25738582\n",
      "Iteration 45, loss = 0.25571767\n",
      "Iteration 46, loss = 0.25669669\n",
      "Iteration 47, loss = 0.25949312\n",
      "Iteration 48, loss = 0.25554268\n",
      "Iteration 49, loss = 0.25494504\n",
      "Iteration 50, loss = 0.25725210\n",
      "Iteration 51, loss = 0.25634936\n",
      "Iteration 52, loss = 0.25539986\n",
      "Iteration 53, loss = 0.25883489\n",
      "Iteration 54, loss = 0.25959323\n",
      "Iteration 55, loss = 0.26029152\n",
      "Iteration 56, loss = 0.25601634\n",
      "Iteration 57, loss = 0.25934361\n",
      "Iteration 58, loss = 0.25375830\n",
      "Iteration 59, loss = 0.25648503\n",
      "Iteration 60, loss = 0.25246916\n",
      "Iteration 61, loss = 0.25528138\n",
      "Iteration 62, loss = 0.26105891\n",
      "Iteration 63, loss = 0.25807591\n",
      "Iteration 64, loss = 0.25602675\n",
      "Iteration 65, loss = 0.25378010\n",
      "Iteration 66, loss = 0.25463779\n",
      "Iteration 67, loss = 0.25567661\n",
      "Iteration 68, loss = 0.25425159\n",
      "Iteration 69, loss = 0.25357130\n",
      "Iteration 70, loss = 0.25369497\n",
      "Iteration 71, loss = 0.25404609\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34871220\n",
      "Iteration 2, loss = 0.29981910\n",
      "Iteration 3, loss = 0.29185756\n",
      "Iteration 4, loss = 0.30017587\n",
      "Iteration 5, loss = 0.29834249\n",
      "Iteration 6, loss = 0.29422057\n",
      "Iteration 7, loss = 0.30070252\n",
      "Iteration 8, loss = 0.28407037\n",
      "Iteration 9, loss = 0.27963147\n",
      "Iteration 10, loss = 0.28080668\n",
      "Iteration 11, loss = 0.28355050\n",
      "Iteration 12, loss = 0.28023564\n",
      "Iteration 13, loss = 0.28010103\n",
      "Iteration 14, loss = 0.27455026\n",
      "Iteration 15, loss = 0.28526906\n",
      "Iteration 16, loss = 0.27852308\n",
      "Iteration 17, loss = 0.27462149\n",
      "Iteration 18, loss = 0.27743606\n",
      "Iteration 19, loss = 0.28895725\n",
      "Iteration 20, loss = 0.28475098\n",
      "Iteration 21, loss = 0.28036930\n",
      "Iteration 22, loss = 0.27169199\n",
      "Iteration 23, loss = 0.28344635\n",
      "Iteration 24, loss = 0.28039466\n",
      "Iteration 25, loss = 0.28113810\n",
      "Iteration 26, loss = 0.27874194\n",
      "Iteration 27, loss = 0.28485145\n",
      "Iteration 28, loss = 0.27753205\n",
      "Iteration 29, loss = 0.28464029\n",
      "Iteration 30, loss = 0.27865688\n",
      "Iteration 31, loss = 0.27479750\n",
      "Iteration 32, loss = 0.28201473\n",
      "Iteration 33, loss = 0.27963003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34238184\n",
      "Iteration 2, loss = 0.31384285\n",
      "Iteration 3, loss = 0.30361478\n",
      "Iteration 4, loss = 0.30548195\n",
      "Iteration 5, loss = 0.29190243\n",
      "Iteration 6, loss = 0.29103140\n",
      "Iteration 7, loss = 0.28946634\n",
      "Iteration 8, loss = 0.28631575\n",
      "Iteration 9, loss = 0.28216691\n",
      "Iteration 10, loss = 0.28566592\n",
      "Iteration 11, loss = 0.29092133\n",
      "Iteration 12, loss = 0.28104918\n",
      "Iteration 13, loss = 0.29106475\n",
      "Iteration 14, loss = 0.29157574\n",
      "Iteration 15, loss = 0.28143914\n",
      "Iteration 16, loss = 0.28289649\n",
      "Iteration 17, loss = 0.28134580\n",
      "Iteration 18, loss = 0.28604292\n",
      "Iteration 19, loss = 0.27760337\n",
      "Iteration 20, loss = 0.27864260\n",
      "Iteration 21, loss = 0.29139117\n",
      "Iteration 22, loss = 0.28209263\n",
      "Iteration 23, loss = 0.28480288\n",
      "Iteration 24, loss = 0.27685347\n",
      "Iteration 25, loss = 0.28133348\n",
      "Iteration 26, loss = 0.27697744\n",
      "Iteration 27, loss = 0.28828846\n",
      "Iteration 28, loss = 0.27967856\n",
      "Iteration 29, loss = 0.28461807\n",
      "Iteration 30, loss = 0.28111258\n",
      "Iteration 31, loss = 0.27982044\n",
      "Iteration 32, loss = 0.28103362\n",
      "Iteration 33, loss = 0.28396862\n",
      "Iteration 34, loss = 0.29236654\n",
      "Iteration 35, loss = 0.28160088\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03730475\n",
      "Iteration 2, loss = 0.52652208\n",
      "Iteration 3, loss = 0.53688611\n",
      "Iteration 4, loss = 0.51532897\n",
      "Iteration 5, loss = 0.45662602\n",
      "Iteration 6, loss = 0.44074748\n",
      "Iteration 7, loss = 0.38952303\n",
      "Iteration 8, loss = 0.35552700\n",
      "Iteration 9, loss = 0.37544397\n",
      "Iteration 10, loss = 0.36104833\n",
      "Iteration 11, loss = 0.34926239\n",
      "Iteration 12, loss = 0.35322118\n",
      "Iteration 13, loss = 0.34761807\n",
      "Iteration 14, loss = 0.37255946\n",
      "Iteration 15, loss = 0.33101116\n",
      "Iteration 16, loss = 0.31430193\n",
      "Iteration 17, loss = 0.32052047\n",
      "Iteration 18, loss = 0.30944213\n",
      "Iteration 19, loss = 0.30697693\n",
      "Iteration 20, loss = 0.33581713\n",
      "Iteration 21, loss = 0.33031619\n",
      "Iteration 22, loss = 0.32358899\n",
      "Iteration 23, loss = 0.31380988\n",
      "Iteration 24, loss = 0.30066387\n",
      "Iteration 25, loss = 0.28981257\n",
      "Iteration 26, loss = 0.31025298\n",
      "Iteration 27, loss = 0.29804076\n",
      "Iteration 28, loss = 0.29650502\n",
      "Iteration 29, loss = 0.29480945\n",
      "Iteration 30, loss = 0.28688976\n",
      "Iteration 31, loss = 0.28709372\n",
      "Iteration 32, loss = 0.27866164\n",
      "Iteration 33, loss = 0.27312111\n",
      "Iteration 34, loss = 0.31659889\n",
      "Iteration 35, loss = 0.29414948\n",
      "Iteration 36, loss = 0.28156818\n",
      "Iteration 37, loss = 0.28526314\n",
      "Iteration 38, loss = 0.31120116\n",
      "Iteration 39, loss = 0.28912902\n",
      "Iteration 40, loss = 0.27391718\n",
      "Iteration 41, loss = 0.28063642\n",
      "Iteration 42, loss = 0.28396534\n",
      "Iteration 43, loss = 0.27088316\n",
      "Iteration 44, loss = 0.27864633\n",
      "Iteration 45, loss = 0.27422212\n",
      "Iteration 46, loss = 0.28068271\n",
      "Iteration 47, loss = 0.28768431\n",
      "Iteration 48, loss = 0.28755169\n",
      "Iteration 49, loss = 0.29074401\n",
      "Iteration 50, loss = 0.26797507\n",
      "Iteration 51, loss = 0.28172250\n",
      "Iteration 52, loss = 0.27057843\n",
      "Iteration 1, loss = 0.42151894\n",
      "Iteration 2, loss = 0.34186727\n",
      "Iteration 3, loss = 0.31679228\n",
      "Iteration 4, loss = 0.30287305\n",
      "Iteration 5, loss = 0.29309950\n",
      "Iteration 6, loss = 0.28576152\n",
      "Iteration 7, loss = 0.28978015\n",
      "Iteration 8, loss = 0.28198268\n",
      "Iteration 9, loss = 0.28089363\n",
      "Iteration 10, loss = 0.27712700\n",
      "Iteration 11, loss = 0.27628600\n",
      "Iteration 12, loss = 0.29583465\n",
      "Iteration 13, loss = 0.28339407\n",
      "Iteration 14, loss = 0.28101336\n",
      "Iteration 15, loss = 0.27754933\n",
      "Iteration 16, loss = 0.27840929\n",
      "Iteration 17, loss = 0.28164941\n",
      "Iteration 18, loss = 0.28252430\n",
      "Iteration 19, loss = 0.27841760\n",
      "Iteration 20, loss = 0.28397716\n",
      "Iteration 21, loss = 0.27713089\n",
      "Iteration 22, loss = 0.27606080\n",
      "Iteration 23, loss = 0.27220012\n",
      "Iteration 24, loss = 0.26823327\n",
      "Iteration 25, loss = 0.27431739\n",
      "Iteration 26, loss = 0.26965281\n",
      "Iteration 27, loss = 0.26360360\n",
      "Iteration 28, loss = 0.26928454\n",
      "Iteration 29, loss = 0.26748154\n",
      "Iteration 30, loss = 0.26909094\n",
      "Iteration 31, loss = 0.26898226\n",
      "Iteration 32, loss = 0.26927090\n",
      "Iteration 33, loss = 0.26961169\n",
      "Iteration 34, loss = 0.27268473\n",
      "Iteration 35, loss = 0.26548275\n",
      "Iteration 36, loss = 0.26328813\n",
      "Iteration 37, loss = 0.26824222\n",
      "Iteration 38, loss = 0.26555288\n",
      "Iteration 39, loss = 0.26849856\n",
      "Iteration 40, loss = 0.26605283\n",
      "Iteration 41, loss = 0.26944256\n",
      "Iteration 42, loss = 0.26130335\n",
      "Iteration 43, loss = 0.26077314\n",
      "Iteration 44, loss = 0.25786360\n",
      "Iteration 45, loss = 0.26107840\n",
      "Iteration 46, loss = 0.26205652\n",
      "Iteration 47, loss = 0.26292893\n",
      "Iteration 48, loss = 0.26386465\n",
      "Iteration 49, loss = 0.26263117\n",
      "Iteration 50, loss = 0.26438390\n",
      "Iteration 51, loss = 0.26133263\n",
      "Iteration 52, loss = 0.26193811\n",
      "Iteration 53, loss = 0.26047349\n",
      "Iteration 54, loss = 0.26515849\n",
      "Iteration 55, loss = 0.26781119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32904531\n",
      "Iteration 2, loss = 0.29110035\n",
      "Iteration 3, loss = 0.29830697\n",
      "Iteration 4, loss = 0.28318135\n",
      "Iteration 5, loss = 0.27201083\n",
      "Iteration 6, loss = 0.27022070\n",
      "Iteration 7, loss = 0.27244079\n",
      "Iteration 8, loss = 0.27501578\n",
      "Iteration 9, loss = 0.27454934\n",
      "Iteration 10, loss = 0.27265697\n",
      "Iteration 11, loss = 0.26967268\n",
      "Iteration 12, loss = 0.26772307\n",
      "Iteration 13, loss = 0.26164884\n",
      "Iteration 14, loss = 0.26199522\n",
      "Iteration 15, loss = 0.26499844\n",
      "Iteration 16, loss = 0.26487155\n",
      "Iteration 17, loss = 0.26220143\n",
      "Iteration 18, loss = 0.26251682\n",
      "Iteration 19, loss = 0.26034051\n",
      "Iteration 20, loss = 0.26279473\n",
      "Iteration 21, loss = 0.26090456\n",
      "Iteration 22, loss = 0.25993794\n",
      "Iteration 23, loss = 0.26188459\n",
      "Iteration 24, loss = 0.25959759\n",
      "Iteration 25, loss = 0.26023603\n",
      "Iteration 26, loss = 0.25755082\n",
      "Iteration 27, loss = 0.25804581\n",
      "Iteration 28, loss = 0.25996110\n",
      "Iteration 29, loss = 0.25777890\n",
      "Iteration 30, loss = 0.25990389\n",
      "Iteration 31, loss = 0.25792748\n",
      "Iteration 32, loss = 0.26395468\n",
      "Iteration 33, loss = 0.25691033\n",
      "Iteration 34, loss = 0.25835515\n",
      "Iteration 35, loss = 0.25964617\n",
      "Iteration 36, loss = 0.25858099\n",
      "Iteration 37, loss = 0.26110598\n",
      "Iteration 38, loss = 0.25752426\n",
      "Iteration 39, loss = 0.25715615\n",
      "Iteration 40, loss = 0.25326858\n",
      "Iteration 41, loss = 0.25340135\n",
      "Iteration 42, loss = 0.25303187\n",
      "Iteration 43, loss = 0.25336714\n",
      "Iteration 44, loss = 0.25513878\n",
      "Iteration 45, loss = 0.25416924\n",
      "Iteration 46, loss = 0.25260805\n",
      "Iteration 47, loss = 0.25626126\n",
      "Iteration 48, loss = 0.25502780\n",
      "Iteration 49, loss = 0.25477410\n",
      "Iteration 50, loss = 0.25401240\n",
      "Iteration 51, loss = 0.25079106\n",
      "Iteration 52, loss = 0.25291992\n",
      "Iteration 53, loss = 0.25373357\n",
      "Iteration 54, loss = 0.25711021\n",
      "Iteration 55, loss = 0.26007343\n",
      "Iteration 56, loss = 0.25728247\n",
      "Iteration 57, loss = 0.25498677\n",
      "Iteration 58, loss = 0.25465040\n",
      "Iteration 59, loss = 0.25347917\n",
      "Iteration 60, loss = 0.25006074\n",
      "Iteration 61, loss = 0.25500249\n",
      "Iteration 62, loss = 0.25671231\n",
      "Iteration 63, loss = 0.25372548\n",
      "Iteration 64, loss = 0.25364752\n",
      "Iteration 65, loss = 0.25439743\n",
      "Iteration 66, loss = 0.25722324\n",
      "Iteration 67, loss = 0.25804147\n",
      "Iteration 68, loss = 0.25606356\n",
      "Iteration 69, loss = 0.25184108\n",
      "Iteration 70, loss = 0.25290860\n",
      "Iteration 71, loss = 0.25878823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33969944\n",
      "Iteration 2, loss = 0.31447001\n",
      "Iteration 3, loss = 0.30077515\n",
      "Iteration 4, loss = 0.29296112\n",
      "Iteration 5, loss = 0.29624165\n",
      "Iteration 6, loss = 0.28895337\n",
      "Iteration 7, loss = 0.28212130\n",
      "Iteration 8, loss = 0.29246379\n",
      "Iteration 9, loss = 0.28455260\n",
      "Iteration 10, loss = 0.28570243\n",
      "Iteration 11, loss = 0.27872090\n",
      "Iteration 12, loss = 0.27897943\n",
      "Iteration 13, loss = 0.27810370\n",
      "Iteration 14, loss = 0.28219594\n",
      "Iteration 15, loss = 0.27571478\n",
      "Iteration 16, loss = 0.27969054\n",
      "Iteration 17, loss = 0.28249519\n",
      "Iteration 18, loss = 0.28444932\n",
      "Iteration 19, loss = 0.28018518\n",
      "Iteration 20, loss = 0.27543873\n",
      "Iteration 21, loss = 0.28336504\n",
      "Iteration 22, loss = 0.28011209\n",
      "Iteration 23, loss = 0.28260725\n",
      "Iteration 24, loss = 0.28103323\n",
      "Iteration 25, loss = 0.27666036\n",
      "Iteration 26, loss = 0.27555071\n",
      "Iteration 27, loss = 0.27562886\n",
      "Iteration 28, loss = 0.27443534\n",
      "Iteration 29, loss = 0.27585572\n",
      "Iteration 30, loss = 0.28362881\n",
      "Iteration 31, loss = 0.28319067\n",
      "Iteration 32, loss = 0.27869514\n",
      "Iteration 33, loss = 0.27837626\n",
      "Iteration 34, loss = 0.27506372\n",
      "Iteration 35, loss = 0.28144086\n",
      "Iteration 36, loss = 0.28676845\n",
      "Iteration 37, loss = 0.27660267\n",
      "Iteration 38, loss = 0.28035506\n",
      "Iteration 39, loss = 0.27865599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34772470\n",
      "Iteration 2, loss = 0.30868981\n",
      "Iteration 3, loss = 0.30100217\n",
      "Iteration 4, loss = 0.29633502\n",
      "Iteration 5, loss = 0.28880930\n",
      "Iteration 6, loss = 0.29520778\n",
      "Iteration 7, loss = 0.29105342\n",
      "Iteration 8, loss = 0.28922895\n",
      "Iteration 9, loss = 0.28774358\n",
      "Iteration 10, loss = 0.29083045\n",
      "Iteration 11, loss = 0.29504800\n",
      "Iteration 12, loss = 0.28148057\n",
      "Iteration 13, loss = 0.28895930\n",
      "Iteration 14, loss = 0.28373150\n",
      "Iteration 15, loss = 0.28125956\n",
      "Iteration 16, loss = 0.28586745\n",
      "Iteration 17, loss = 0.28441730\n",
      "Iteration 18, loss = 0.28368598\n",
      "Iteration 19, loss = 0.29072603\n",
      "Iteration 20, loss = 0.28072587\n",
      "Iteration 21, loss = 0.28169707\n",
      "Iteration 22, loss = 0.28171458\n",
      "Iteration 23, loss = 0.27588566\n",
      "Iteration 24, loss = 0.28317490\n",
      "Iteration 25, loss = 0.28260859\n",
      "Iteration 26, loss = 0.27978162\n",
      "Iteration 27, loss = 0.28348853\n",
      "Iteration 28, loss = 0.27940686\n",
      "Iteration 29, loss = 0.30550173\n",
      "Iteration 30, loss = 0.28348657\n",
      "Iteration 31, loss = 0.28131422\n",
      "Iteration 32, loss = 0.29738083\n",
      "Iteration 33, loss = 0.28740456\n",
      "Iteration 34, loss = 0.28238663\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.49594265\n",
      "Iteration 2, loss = 1.22921284\n",
      "Iteration 3, loss = 0.47256405\n",
      "Iteration 4, loss = 0.45062218\n",
      "Iteration 5, loss = 0.51261721\n",
      "Iteration 6, loss = 0.44927954\n",
      "Iteration 7, loss = 0.46803254\n",
      "Iteration 8, loss = 0.40689024\n",
      "Iteration 9, loss = 0.36773671\n",
      "Iteration 10, loss = 0.35649508\n",
      "Iteration 11, loss = 0.37534321\n",
      "Iteration 12, loss = 0.33554524\n",
      "Iteration 13, loss = 0.33289111\n",
      "Iteration 14, loss = 0.33867661\n",
      "Iteration 15, loss = 0.35318457\n",
      "Iteration 16, loss = 0.32943565\n",
      "Iteration 17, loss = 0.31917649\n",
      "Iteration 18, loss = 0.30649750\n",
      "Iteration 19, loss = 0.36253560\n",
      "Iteration 20, loss = 0.35142757\n",
      "Iteration 21, loss = 0.31832238\n",
      "Iteration 22, loss = 0.32347994\n",
      "Iteration 23, loss = 0.32871071\n",
      "Iteration 24, loss = 0.31214576\n",
      "Iteration 25, loss = 0.29464860\n",
      "Iteration 26, loss = 0.31809194\n",
      "Iteration 27, loss = 0.30749369\n",
      "Iteration 28, loss = 0.32543451\n",
      "Iteration 29, loss = 0.34444654\n",
      "Iteration 30, loss = 0.31269871\n",
      "Iteration 31, loss = 0.33041913\n",
      "Iteration 32, loss = 0.30674637\n",
      "Iteration 33, loss = 0.34182164\n",
      "Iteration 34, loss = 0.30604591\n",
      "Iteration 35, loss = 0.28594169\n",
      "Iteration 36, loss = 0.30086925\n",
      "Iteration 37, loss = 0.30667731\n",
      "Iteration 38, loss = 0.30616852\n",
      "Iteration 39, loss = 0.30666560\n",
      "Iteration 40, loss = 0.30159996\n",
      "Iteration 41, loss = 0.30412169\n",
      "Iteration 42, loss = 0.28408027\n",
      "Iteration 43, loss = 0.29041946\n",
      "Iteration 44, loss = 0.29737091\n",
      "Iteration 45, loss = 0.29042439\n",
      "Iteration 46, loss = 0.28349025\n",
      "Iteration 47, loss = 0.27507149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39242055\n",
      "Iteration 2, loss = 0.31204405\n",
      "Iteration 3, loss = 0.28957022\n",
      "Iteration 4, loss = 0.29374337\n",
      "Iteration 5, loss = 0.29989757\n",
      "Iteration 6, loss = 0.29258826\n",
      "Iteration 7, loss = 0.28457235\n",
      "Iteration 8, loss = 0.29022549\n",
      "Iteration 9, loss = 0.28188179\n",
      "Iteration 10, loss = 0.26999265\n",
      "Iteration 11, loss = 0.27272760\n",
      "Iteration 12, loss = 0.27325180\n",
      "Iteration 13, loss = 0.27196838\n",
      "Iteration 14, loss = 0.28348588\n",
      "Iteration 15, loss = 0.27541411\n",
      "Iteration 16, loss = 0.27938415\n",
      "Iteration 17, loss = 0.26783011\n",
      "Iteration 18, loss = 0.26672062\n",
      "Iteration 19, loss = 0.27240025\n",
      "Iteration 20, loss = 0.27135273\n",
      "Iteration 21, loss = 0.27228484\n",
      "Iteration 22, loss = 0.27035860\n",
      "Iteration 23, loss = 0.27653132\n",
      "Iteration 24, loss = 0.26120957\n",
      "Iteration 25, loss = 0.27231464\n",
      "Iteration 26, loss = 0.26824774\n",
      "Iteration 27, loss = 0.26252530\n",
      "Iteration 28, loss = 0.27605703\n",
      "Iteration 29, loss = 0.27084002\n",
      "Iteration 30, loss = 0.26141037\n",
      "Iteration 31, loss = 0.27804010\n",
      "Iteration 32, loss = 0.26967168\n",
      "Iteration 33, loss = 0.26427760\n",
      "Iteration 34, loss = 0.26486691\n",
      "Iteration 35, loss = 0.25939272\n",
      "Iteration 36, loss = 0.26296934\n",
      "Iteration 37, loss = 0.26197350\n",
      "Iteration 38, loss = 0.26352083\n",
      "Iteration 39, loss = 0.26417965\n",
      "Iteration 40, loss = 0.25778622\n",
      "Iteration 41, loss = 0.27372531\n",
      "Iteration 42, loss = 0.27316681\n",
      "Iteration 43, loss = 0.26286818\n",
      "Iteration 44, loss = 0.26230359\n",
      "Iteration 45, loss = 0.26592481\n",
      "Iteration 46, loss = 0.26076650\n",
      "Iteration 47, loss = 0.27010382\n",
      "Iteration 48, loss = 0.26818141\n",
      "Iteration 49, loss = 0.26268712\n",
      "Iteration 50, loss = 0.26497388\n",
      "Iteration 51, loss = 0.25999259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32904531\n",
      "Iteration 2, loss = 0.29110035\n",
      "Iteration 3, loss = 0.29830697\n",
      "Iteration 4, loss = 0.28318135\n",
      "Iteration 5, loss = 0.27201083\n",
      "Iteration 6, loss = 0.27022070\n",
      "Iteration 7, loss = 0.27244079\n",
      "Iteration 8, loss = 0.27501578\n",
      "Iteration 9, loss = 0.27454934\n",
      "Iteration 10, loss = 0.27265697\n",
      "Iteration 11, loss = 0.26967268\n",
      "Iteration 12, loss = 0.26772307\n",
      "Iteration 13, loss = 0.26164884\n",
      "Iteration 14, loss = 0.26199522\n",
      "Iteration 15, loss = 0.26499844\n",
      "Iteration 16, loss = 0.26487155\n",
      "Iteration 17, loss = 0.26220143\n",
      "Iteration 18, loss = 0.26251682\n",
      "Iteration 19, loss = 0.26034051\n",
      "Iteration 20, loss = 0.26279473\n",
      "Iteration 21, loss = 0.26090456\n",
      "Iteration 22, loss = 0.25993794\n",
      "Iteration 23, loss = 0.26188459\n",
      "Iteration 24, loss = 0.25959759\n",
      "Iteration 25, loss = 0.26023603\n",
      "Iteration 26, loss = 0.25755082\n",
      "Iteration 27, loss = 0.25804581\n",
      "Iteration 28, loss = 0.25996110\n",
      "Iteration 29, loss = 0.25777890\n",
      "Iteration 30, loss = 0.25990389\n",
      "Iteration 31, loss = 0.25792748\n",
      "Iteration 32, loss = 0.26395468\n",
      "Iteration 33, loss = 0.25691033\n",
      "Iteration 34, loss = 0.25835515\n",
      "Iteration 35, loss = 0.25964617\n",
      "Iteration 36, loss = 0.25858099\n",
      "Iteration 37, loss = 0.26110598\n",
      "Iteration 38, loss = 0.25752426\n",
      "Iteration 39, loss = 0.25715615\n",
      "Iteration 40, loss = 0.25326858\n",
      "Iteration 41, loss = 0.25340135\n",
      "Iteration 42, loss = 0.25303187\n",
      "Iteration 43, loss = 0.25336714\n",
      "Iteration 44, loss = 0.25513878\n",
      "Iteration 45, loss = 0.25416924\n",
      "Iteration 46, loss = 0.25260805\n",
      "Iteration 47, loss = 0.25626126\n",
      "Iteration 48, loss = 0.25502780\n",
      "Iteration 49, loss = 0.25477410\n",
      "Iteration 50, loss = 0.25401240\n",
      "Iteration 51, loss = 0.25079106\n",
      "Iteration 52, loss = 0.25291992\n",
      "Iteration 53, loss = 0.25373357\n",
      "Iteration 54, loss = 0.25711021\n",
      "Iteration 55, loss = 0.26007343\n",
      "Iteration 56, loss = 0.25728247\n",
      "Iteration 57, loss = 0.25498677\n",
      "Iteration 58, loss = 0.25465040\n",
      "Iteration 59, loss = 0.25347917\n",
      "Iteration 60, loss = 0.25006074\n",
      "Iteration 61, loss = 0.25500249\n",
      "Iteration 62, loss = 0.25671231\n",
      "Iteration 63, loss = 0.25372548\n",
      "Iteration 64, loss = 0.25364752\n",
      "Iteration 65, loss = 0.25439743\n",
      "Iteration 66, loss = 0.25722324\n",
      "Iteration 67, loss = 0.25804147\n",
      "Iteration 68, loss = 0.25606356\n",
      "Iteration 69, loss = 0.25184108\n",
      "Iteration 70, loss = 0.25290860\n",
      "Iteration 71, loss = 0.25878823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.39443838\n",
      "Iteration 2, loss = 0.33292099\n",
      "Iteration 3, loss = 0.32900879\n",
      "Iteration 4, loss = 0.29889072\n",
      "Iteration 5, loss = 0.29166310\n",
      "Iteration 6, loss = 0.29106813\n",
      "Iteration 7, loss = 0.28847920\n",
      "Iteration 8, loss = 0.30751759\n",
      "Iteration 9, loss = 0.30405878\n",
      "Iteration 10, loss = 0.30945946\n",
      "Iteration 11, loss = 0.29037193\n",
      "Iteration 12, loss = 0.28887894\n",
      "Iteration 13, loss = 0.27965071\n",
      "Iteration 14, loss = 0.30863038\n",
      "Iteration 15, loss = 0.28092835\n",
      "Iteration 16, loss = 0.28911840\n",
      "Iteration 17, loss = 0.27822269\n",
      "Iteration 18, loss = 0.27948408\n",
      "Iteration 19, loss = 0.28948319\n",
      "Iteration 20, loss = 0.29806050\n",
      "Iteration 21, loss = 0.28314987\n",
      "Iteration 22, loss = 0.27505109\n",
      "Iteration 23, loss = 0.27916823\n",
      "Iteration 24, loss = 0.27733262\n",
      "Iteration 25, loss = 0.27643289\n",
      "Iteration 26, loss = 0.29130423\n",
      "Iteration 27, loss = 0.28522523\n",
      "Iteration 28, loss = 0.28498711\n",
      "Iteration 29, loss = 0.29282798\n",
      "Iteration 30, loss = 0.28597295\n",
      "Iteration 31, loss = 0.27376143\n",
      "Iteration 32, loss = 0.27422798\n",
      "Iteration 33, loss = 0.27979881\n",
      "Iteration 34, loss = 0.28598303\n",
      "Iteration 35, loss = 0.28640313\n",
      "Iteration 36, loss = 0.28398905\n",
      "Iteration 37, loss = 0.28586615\n",
      "Iteration 38, loss = 0.28168766\n",
      "Iteration 39, loss = 0.28092529\n",
      "Iteration 40, loss = 0.28327101\n",
      "Iteration 41, loss = 0.28525217\n",
      "Iteration 42, loss = 0.28102315\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33242854\n",
      "Iteration 2, loss = 0.31074654\n",
      "Iteration 3, loss = 0.29896558\n",
      "Iteration 4, loss = 0.28379778\n",
      "Iteration 5, loss = 0.28541444\n",
      "Iteration 6, loss = 0.28944530\n",
      "Iteration 7, loss = 0.29107797\n",
      "Iteration 8, loss = 0.28482574\n",
      "Iteration 9, loss = 0.27932808\n",
      "Iteration 10, loss = 0.28020947\n",
      "Iteration 11, loss = 0.27995085\n",
      "Iteration 12, loss = 0.28132452\n",
      "Iteration 13, loss = 0.28209399\n",
      "Iteration 14, loss = 0.29129478\n",
      "Iteration 15, loss = 0.28026812\n",
      "Iteration 16, loss = 0.27967520\n",
      "Iteration 17, loss = 0.27887279\n",
      "Iteration 18, loss = 0.27645326\n",
      "Iteration 19, loss = 0.27746107\n",
      "Iteration 20, loss = 0.28138703\n",
      "Iteration 21, loss = 0.27854840\n",
      "Iteration 22, loss = 0.28120016\n",
      "Iteration 23, loss = 0.28509941\n",
      "Iteration 24, loss = 0.28299080\n",
      "Iteration 25, loss = 0.27729659\n",
      "Iteration 26, loss = 0.27421597\n",
      "Iteration 27, loss = 0.27801730\n",
      "Iteration 28, loss = 0.28217224\n",
      "Iteration 29, loss = 0.27550744\n",
      "Iteration 30, loss = 0.27389538\n",
      "Iteration 31, loss = 0.27250447\n",
      "Iteration 32, loss = 0.27927032\n",
      "Iteration 33, loss = 0.28050985\n",
      "Iteration 34, loss = 0.27376671\n",
      "Iteration 35, loss = 0.27917285\n",
      "Iteration 36, loss = 0.28395756\n",
      "Iteration 37, loss = 0.27420055\n",
      "Iteration 38, loss = 0.27695567\n",
      "Iteration 39, loss = 0.28316998\n",
      "Iteration 40, loss = 0.27682415\n",
      "Iteration 41, loss = 0.27439887\n",
      "Iteration 42, loss = 0.28077775\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.03413256\n",
      "Iteration 2, loss = 3.07114095\n",
      "Iteration 3, loss = 1.11477156\n",
      "Iteration 4, loss = 0.44204779\n",
      "Iteration 5, loss = 0.45532293\n",
      "Iteration 6, loss = 0.40033971\n",
      "Iteration 7, loss = 0.38091723\n",
      "Iteration 8, loss = 0.39747437\n",
      "Iteration 9, loss = 0.40465199\n",
      "Iteration 10, loss = 0.41344020\n",
      "Iteration 11, loss = 0.38110640\n",
      "Iteration 12, loss = 0.39583711\n",
      "Iteration 13, loss = 0.36775032\n",
      "Iteration 14, loss = 0.39125929\n",
      "Iteration 15, loss = 0.36414286\n",
      "Iteration 16, loss = 0.36872219\n",
      "Iteration 17, loss = 0.36541256\n",
      "Iteration 18, loss = 0.36454400\n",
      "Iteration 19, loss = 0.37291661\n",
      "Iteration 20, loss = 0.35169187\n",
      "Iteration 21, loss = 0.41795901\n",
      "Iteration 22, loss = 0.32059130\n",
      "Iteration 23, loss = 0.31682976\n",
      "Iteration 24, loss = 0.32532298\n",
      "Iteration 25, loss = 0.33116416\n",
      "Iteration 26, loss = 0.31867744\n",
      "Iteration 27, loss = 0.29765014\n",
      "Iteration 28, loss = 0.32097079\n",
      "Iteration 29, loss = 0.31699989\n",
      "Iteration 30, loss = 0.31152578\n",
      "Iteration 31, loss = 0.30375591\n",
      "Iteration 32, loss = 0.30039964\n",
      "Iteration 33, loss = 0.29378106\n",
      "Iteration 34, loss = 0.30104505\n",
      "Iteration 35, loss = 0.31430391\n",
      "Iteration 36, loss = 0.32032709\n",
      "Iteration 37, loss = 0.29475830\n",
      "Iteration 38, loss = 0.30452927\n",
      "Iteration 39, loss = 0.32654990\n",
      "Iteration 40, loss = 0.31229484\n",
      "Iteration 23, loss = 0.27988077\n",
      "Iteration 24, loss = 0.28031619\n",
      "Iteration 25, loss = 0.29849990\n",
      "Iteration 26, loss = 0.28053530\n",
      "Iteration 27, loss = 0.28798878\n",
      "Iteration 28, loss = 0.28110858\n",
      "Iteration 29, loss = 0.27508688\n",
      "Iteration 30, loss = 0.28056612\n",
      "Iteration 31, loss = 0.28385828\n",
      "Iteration 32, loss = 0.28053998\n",
      "Iteration 33, loss = 0.28835195\n",
      "Iteration 34, loss = 0.27869765\n",
      "Iteration 35, loss = 0.28081795\n",
      "Iteration 36, loss = 0.28706433\n",
      "Iteration 37, loss = 0.28518791\n",
      "Iteration 38, loss = 0.28930397\n",
      "Iteration 39, loss = 0.28047430\n",
      "Iteration 40, loss = 0.27632530\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.62706357\n",
      "Iteration 2, loss = 0.55629896\n",
      "Iteration 3, loss = 0.41722030\n",
      "Iteration 4, loss = 0.40907008\n",
      "Iteration 5, loss = 0.41138787\n",
      "Iteration 6, loss = 0.39820108\n",
      "Iteration 7, loss = 0.36213394\n",
      "Iteration 8, loss = 0.36245695\n",
      "Iteration 9, loss = 0.37257134\n",
      "Iteration 10, loss = 0.38364261\n",
      "Iteration 11, loss = 0.33257169\n",
      "Iteration 12, loss = 0.34460911\n",
      "Iteration 13, loss = 0.34227834\n",
      "Iteration 14, loss = 0.33524577\n",
      "Iteration 15, loss = 0.33397612\n",
      "Iteration 16, loss = 0.33797543\n",
      "Iteration 17, loss = 0.32768054\n",
      "Iteration 18, loss = 0.31329356\n",
      "Iteration 19, loss = 0.30191182\n",
      "Iteration 20, loss = 0.30947770\n",
      "Iteration 21, loss = 0.30191768\n",
      "Iteration 22, loss = 0.29710415\n",
      "Iteration 23, loss = 0.28373661\n",
      "Iteration 24, loss = 0.28913701\n",
      "Iteration 25, loss = 0.27867094\n",
      "Iteration 26, loss = 0.28857820\n",
      "Iteration 27, loss = 0.26955110\n",
      "Iteration 28, loss = 0.27159739\n",
      "Iteration 29, loss = 0.26655253\n",
      "Iteration 30, loss = 0.29026483\n",
      "Iteration 31, loss = 0.28287326\n",
      "Iteration 32, loss = 0.26922311\n",
      "Iteration 33, loss = 0.27111372\n",
      "Iteration 34, loss = 0.27848335\n",
      "Iteration 35, loss = 0.26453662\n",
      "Iteration 36, loss = 0.27167554\n",
      "Iteration 37, loss = 0.27504083\n",
      "Iteration 38, loss = 0.27304957\n",
      "Iteration 39, loss = 0.26102637\n",
      "Iteration 40, loss = 0.26355637\n",
      "Iteration 41, loss = 0.25886670\n",
      "Iteration 42, loss = 0.27197636\n",
      "Iteration 43, loss = 0.26509269\n",
      "Iteration 44, loss = 0.27040923\n",
      "Iteration 45, loss = 0.26027314\n",
      "Iteration 46, loss = 0.26578179\n",
      "Iteration 47, loss = 0.26304403\n",
      "Iteration 48, loss = 0.27098182\n",
      "Iteration 49, loss = 0.25686861\n",
      "Iteration 50, loss = 0.25887245\n",
      "Iteration 51, loss = 0.26960251\n",
      "Iteration 52, loss = 0.27005669\n",
      "Iteration 53, loss = 0.25519648\n",
      "Iteration 54, loss = 0.26327628\n",
      "Iteration 55, loss = 0.25664010\n",
      "Iteration 56, loss = 0.26070010\n",
      "Iteration 57, loss = 0.25805642\n",
      "Iteration 58, loss = 0.25466238\n",
      "Iteration 59, loss = 0.27238738\n",
      "Iteration 60, loss = 0.25996688\n",
      "Iteration 61, loss = 0.26579419\n",
      "Iteration 62, loss = 0.26373347\n",
      "Iteration 63, loss = 0.25838941\n",
      "Iteration 64, loss = 0.26116575\n",
      "Iteration 65, loss = 0.26035650\n",
      "Iteration 66, loss = 0.25889587\n",
      "Iteration 67, loss = 0.25870187\n",
      "Iteration 68, loss = 0.25710754\n",
      "Iteration 69, loss = 0.25478785\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.38533449\n",
      "Iteration 2, loss = 0.78497038\n",
      "Iteration 3, loss = 0.40781880\n",
      "Iteration 4, loss = 0.36660022\n",
      "Iteration 5, loss = 0.37169451\n",
      "Iteration 6, loss = 0.42997659\n",
      "Iteration 7, loss = 0.40835345\n",
      "Iteration 8, loss = 0.41248069\n",
      "Iteration 9, loss = 0.41981221\n",
      "Iteration 10, loss = 0.41149161\n",
      "Iteration 11, loss = 0.39686428\n",
      "Iteration 12, loss = 0.39961607\n",
      "Iteration 13, loss = 0.39299466\n",
      "Iteration 14, loss = 0.38896295\n",
      "Iteration 15, loss = 0.39922846\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.54253066\n",
      "Iteration 2, loss = 2.52914855\n",
      "Iteration 3, loss = 2.55054785\n",
      "Iteration 4, loss = 1.71393858\n",
      "Iteration 5, loss = 0.93363453\n",
      "Iteration 6, loss = 0.61990076\n",
      "Iteration 7, loss = 0.32450744\n",
      "Iteration 8, loss = 0.31591583\n",
      "Iteration 9, loss = 0.31176526\n",
      "Iteration 10, loss = 0.31547522\n",
      "Iteration 11, loss = 0.30345723\n",
      "Iteration 12, loss = 0.32107919\n",
      "Iteration 13, loss = 0.33835823\n",
      "Iteration 14, loss = 0.33754201\n",
      "Iteration 15, loss = 0.31552879\n",
      "Iteration 16, loss = 0.34924676\n",
      "Iteration 17, loss = 0.32793207\n",
      "Iteration 18, loss = 0.34031884\n",
      "Iteration 19, loss = 0.34067932\n",
      "Iteration 20, loss = 0.34898794\n",
      "Iteration 21, loss = 0.34937620\n",
      "Iteration 22, loss = 0.35436421\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.93477381\n",
      "Iteration 2, loss = 0.45963750\n",
      "Iteration 3, loss = 0.43059504\n",
      "Iteration 4, loss = 0.42807264\n",
      "Iteration 5, loss = 0.43102386\n",
      "Iteration 6, loss = 0.44070639\n",
      "Iteration 7, loss = 0.43935673\n",
      "Iteration 8, loss = 0.39118774\n",
      "Iteration 9, loss = 0.36298326\n",
      "Iteration 10, loss = 0.35317001\n",
      "Iteration 11, loss = 0.34614297\n",
      "Iteration 12, loss = 0.33337969\n",
      "Iteration 13, loss = 0.32435777\n",
      "Iteration 14, loss = 0.35465160\n",
      "Iteration 15, loss = 0.34302293\n",
      "Iteration 16, loss = 0.30878997\n",
      "Iteration 17, loss = 0.31243914\n",
      "Iteration 18, loss = 0.31110067\n",
      "Iteration 19, loss = 0.31140267\n",
      "Iteration 20, loss = 0.32592510\n",
      "Iteration 21, loss = 0.31942046\n",
      "Iteration 22, loss = 0.29082931\n",
      "Iteration 23, loss = 0.29568169\n",
      "Iteration 24, loss = 0.30215217\n",
      "Iteration 25, loss = 0.29562651\n",
      "Iteration 26, loss = 0.30644527\n",
      "Iteration 27, loss = 0.29200685\n",
      "Iteration 28, loss = 0.30050760\n",
      "Iteration 29, loss = 0.28680879\n",
      "Iteration 30, loss = 0.27936613\n",
      "Iteration 31, loss = 0.29455102\n",
      "Iteration 32, loss = 0.28358411\n",
      "Iteration 33, loss = 0.27456133\n",
      "Iteration 34, loss = 0.27873207\n",
      "Iteration 35, loss = 0.29094966\n",
      "Iteration 36, loss = 0.28498096\n",
      "Iteration 37, loss = 0.28406208\n",
      "Iteration 38, loss = 0.29751181\n",
      "Iteration 39, loss = 0.27387838\n",
      "Iteration 40, loss = 0.28445138\n",
      "Iteration 41, loss = 0.28204166\n",
      "Iteration 42, loss = 0.27344077\n",
      "Iteration 43, loss = 0.28147335\n",
      "Iteration 44, loss = 0.28697449\n",
      "Iteration 45, loss = 0.28020272\n",
      "Iteration 46, loss = 0.28472694\n",
      "Iteration 47, loss = 0.29006092\n",
      "Iteration 48, loss = 0.28493134\n",
      "Iteration 49, loss = 0.28207002\n",
      "Iteration 50, loss = 0.26908924\n",
      "Iteration 51, loss = 0.27693988\n",
      "Iteration 52, loss = 0.26722338\n",
      "Iteration 53, loss = 0.27586417\n",
      "Iteration 54, loss = 0.27420734\n",
      "Iteration 55, loss = 0.26905369\n",
      "Iteration 56, loss = 0.27216774\n",
      "Iteration 57, loss = 0.30027229\n",
      "Iteration 58, loss = 0.28713191\n",
      "Iteration 59, loss = 0.28089439\n",
      "Iteration 60, loss = 0.29629246\n",
      "Iteration 61, loss = 0.27814092\n",
      "Iteration 62, loss = 0.31247171\n",
      "Iteration 63, loss = 0.32421701\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.68665274\n",
      "Iteration 2, loss = 3.89114516\n",
      "Iteration 3, loss = 2.22759245\n",
      "Iteration 4, loss = 0.77720727\n",
      "Iteration 5, loss = 0.54876126\n",
      "Iteration 6, loss = 0.52227800\n",
      "Iteration 7, loss = 0.43890457\n",
      "Iteration 8, loss = 0.43432380\n",
      "Iteration 9, loss = 0.42406803\n",
      "Iteration 10, loss = 0.41877661\n",
      "Iteration 11, loss = 0.43140491\n",
      "Iteration 12, loss = 0.42316950\n",
      "Iteration 13, loss = 0.40203399\n",
      "Iteration 14, loss = 0.40307887\n",
      "Iteration 15, loss = 0.43665594\n",
      "Iteration 16, loss = 0.40266738\n",
      "Iteration 17, loss = 0.38802340\n",
      "Iteration 18, loss = 0.40108343\n",
      "Iteration 19, loss = 0.38483767\n",
      "Iteration 20, loss = 0.33089557\n",
      "Iteration 21, loss = 0.38923103\n",
      "Iteration 22, loss = 0.36196799\n",
      "Iteration 23, loss = 0.34387786\n",
      "Iteration 24, loss = 0.34435720\n",
      "Iteration 25, loss = 0.35019657\n",
      "Iteration 26, loss = 0.35432173\n",
      "Iteration 27, loss = 0.37415821\n",
      "Iteration 28, loss = 0.37493382\n",
      "Iteration 29, loss = 0.38185301\n",
      "Iteration 30, loss = 0.41033261\n",
      "Iteration 31, loss = 0.39827739\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.10930715\n",
      "Iteration 2, loss = 4.36821206\n",
      "Iteration 3, loss = 4.39131603\n",
      "Iteration 4, loss = 3.79918226\n",
      "Iteration 5, loss = 3.66638127\n",
      "Iteration 6, loss = 4.39996863\n",
      "Iteration 7, loss = 4.35359181\n",
      "Iteration 8, loss = 4.01722371\n",
      "Iteration 9, loss = 3.63590415\n",
      "Iteration 10, loss = 3.91636384\n",
      "Iteration 11, loss = 3.76894974\n",
      "Iteration 12, loss = 3.67037918\n",
      "Iteration 13, loss = 3.66821195\n",
      "Iteration 14, loss = 3.83913512\n",
      "Iteration 15, loss = 4.01906860\n",
      "Iteration 16, loss = 3.82959755\n",
      "Iteration 17, loss = 3.36431140\n",
      "Iteration 18, loss = 3.81678926\n",
      "Iteration 19, loss = 3.92410408\n",
      "Iteration 20, loss = 3.73265039\n",
      "Iteration 21, loss = 3.71394174\n",
      "Iteration 22, loss = 3.59675965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.28042078\n",
      "Iteration 21, loss = 0.27569442\n",
      "Iteration 22, loss = 0.27933167\n",
      "Iteration 23, loss = 0.27792424\n",
      "Iteration 24, loss = 0.27968770\n",
      "Iteration 25, loss = 0.27708390\n",
      "Iteration 26, loss = 0.28308456\n",
      "Iteration 27, loss = 0.28120179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.26128764\n",
      "Iteration 2, loss = 1.03597093\n",
      "Iteration 3, loss = 0.63149425\n",
      "Iteration 4, loss = 0.59722453\n",
      "Iteration 5, loss = 0.59435127\n",
      "Iteration 6, loss = 0.58778426\n",
      "Iteration 7, loss = 0.58463501\n",
      "Iteration 8, loss = 0.56724102\n",
      "Iteration 9, loss = 0.57630242\n",
      "Iteration 10, loss = 0.57626226\n",
      "Iteration 11, loss = 0.55846420\n",
      "Iteration 12, loss = 0.54905214\n",
      "Iteration 13, loss = 0.56347716\n",
      "Iteration 14, loss = 0.56043639\n",
      "Iteration 15, loss = 0.46378976\n",
      "Iteration 16, loss = 0.41582934\n",
      "Iteration 17, loss = 0.41831270\n",
      "Iteration 18, loss = 0.39833979\n",
      "Iteration 19, loss = 0.39844558\n",
      "Iteration 20, loss = 0.42383444\n",
      "Iteration 21, loss = 0.44510972\n",
      "Iteration 22, loss = 0.50244599\n",
      "Iteration 23, loss = 0.38534115\n",
      "Iteration 24, loss = 0.40304438\n",
      "Iteration 25, loss = 0.39241295\n",
      "Iteration 26, loss = 0.46135475\n",
      "Iteration 27, loss = 0.39051306\n",
      "Iteration 28, loss = 0.37279027\n",
      "Iteration 29, loss = 0.37221762\n",
      "Iteration 30, loss = 0.36212196\n",
      "Iteration 31, loss = 0.37824355\n",
      "Iteration 32, loss = 0.38944722\n",
      "Iteration 33, loss = 0.36154203\n",
      "Iteration 34, loss = 0.36825157\n",
      "Iteration 35, loss = 0.37821589\n",
      "Iteration 36, loss = 0.35497234\n",
      "Iteration 37, loss = 0.35888621\n",
      "Iteration 38, loss = 0.39704523\n",
      "Iteration 39, loss = 0.34288308\n",
      "Iteration 40, loss = 0.47014984\n",
      "Iteration 41, loss = 0.54898191\n",
      "Iteration 42, loss = 0.54605685\n",
      "Iteration 43, loss = 0.42687051\n",
      "Iteration 44, loss = 0.36358539\n",
      "Iteration 45, loss = 0.36888797\n",
      "Iteration 46, loss = 0.35386674\n",
      "Iteration 47, loss = 0.40136121\n",
      "Iteration 48, loss = 0.38024464\n",
      "Iteration 49, loss = 0.34901344\n",
      "Iteration 50, loss = 0.35513469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.09942500\n",
      "Iteration 2, loss = 2.70574001\n",
      "Iteration 3, loss = 0.62048009\n",
      "Iteration 4, loss = 0.43295366\n",
      "Iteration 5, loss = 0.36098918\n",
      "Iteration 6, loss = 0.42314021\n",
      "Iteration 7, loss = 0.37728536\n",
      "Iteration 8, loss = 0.38944177\n",
      "Iteration 9, loss = 0.36988345\n",
      "Iteration 10, loss = 0.39445837\n",
      "Iteration 11, loss = 0.37833830\n",
      "Iteration 12, loss = 0.41667994\n",
      "Iteration 13, loss = 0.40590750\n",
      "Iteration 14, loss = 0.39924996\n",
      "Iteration 15, loss = 0.37488251\n",
      "Iteration 16, loss = 0.40506877\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.79314259\n",
      "Iteration 2, loss = 4.41274373\n",
      "Iteration 3, loss = 3.83466502\n",
      "Iteration 4, loss = 4.03073543\n",
      "Iteration 5, loss = 3.82297806\n",
      "Iteration 6, loss = 3.97197669\n",
      "Iteration 7, loss = 3.77466575\n",
      "Iteration 8, loss = 3.83693643\n",
      "Iteration 9, loss = 3.51879992\n",
      "Iteration 10, loss = 3.61812599\n",
      "Iteration 11, loss = 3.45569295\n",
      "Iteration 12, loss = 3.58472828\n",
      "Iteration 13, loss = 3.46007027\n",
      "Iteration 14, loss = 3.71769804\n",
      "Iteration 15, loss = 3.58305320\n",
      "Iteration 16, loss = 3.94872858\n",
      "Iteration 17, loss = 3.15400239\n",
      "Iteration 18, loss = 3.78860536\n",
      "Iteration 19, loss = 3.74534977\n",
      "Iteration 20, loss = 3.57607956\n",
      "Iteration 21, loss = 3.87954875\n",
      "Iteration 22, loss = 3.10683174\n",
      "Iteration 23, loss = 3.33360521\n",
      "Iteration 24, loss = 3.33762608\n",
      "Iteration 25, loss = 3.53083842\n",
      "Iteration 26, loss = 3.19636200\n",
      "Iteration 27, loss = 3.17241434\n",
      "Iteration 28, loss = 3.19516814\n",
      "Iteration 29, loss = 3.37906069\n",
      "Iteration 30, loss = 3.15244447\n",
      "Iteration 31, loss = 3.71087611\n",
      "Iteration 32, loss = 3.72093389\n",
      "Iteration 33, loss = 3.50986623\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.17825869\n",
      "Iteration 2, loss = 0.62578469\n",
      "Iteration 3, loss = 0.50415622\n",
      "Iteration 4, loss = 0.49180423\n",
      "Iteration 5, loss = 0.49744428\n",
      "Iteration 6, loss = 0.41131888\n",
      "Iteration 7, loss = 0.37701041\n",
      "Iteration 8, loss = 0.39525768\n",
      "Iteration 9, loss = 0.36269040\n",
      "Iteration 10, loss = 0.37889527\n",
      "Iteration 11, loss = 0.38749152\n",
      "Iteration 12, loss = 0.38215274\n",
      "Iteration 13, loss = 0.44328627\n",
      "Iteration 14, loss = 0.36939454\n",
      "Iteration 15, loss = 0.35285661\n",
      "Iteration 16, loss = 0.35394960\n",
      "Iteration 17, loss = 0.36774084\n",
      "Iteration 18, loss = 0.40975432\n",
      "Iteration 19, loss = 0.38528926\n",
      "Iteration 20, loss = 0.40859657\n",
      "Iteration 21, loss = 0.36343969\n",
      "Iteration 22, loss = 0.36786118\n",
      "Iteration 23, loss = 0.36525911\n",
      "Iteration 24, loss = 0.37132325\n",
      "Iteration 25, loss = 0.36782948\n",
      "Iteration 26, loss = 0.34797754\n",
      "Iteration 27, loss = 0.34616891\n",
      "Iteration 28, loss = 0.36118271\n",
      "Iteration 29, loss = 0.33362927\n",
      "Iteration 30, loss = 0.31982255\n",
      "Iteration 31, loss = 0.32403086\n",
      "Iteration 32, loss = 0.32026498\n",
      "Iteration 33, loss = 0.33572858\n",
      "Iteration 34, loss = 0.32860074\n",
      "Iteration 35, loss = 0.32944647\n",
      "Iteration 36, loss = 0.32618801\n",
      "Iteration 37, loss = 0.32182157\n",
      "Iteration 38, loss = 0.34863975\n",
      "Iteration 39, loss = 0.31818420\n",
      "Iteration 40, loss = 0.31201336\n",
      "Iteration 41, loss = 0.33214920\n",
      "Iteration 42, loss = 0.32457873\n",
      "Iteration 43, loss = 0.32129459\n",
      "Iteration 44, loss = 0.31903197\n",
      "Iteration 45, loss = 0.33076629\n",
      "Iteration 46, loss = 0.31008246\n",
      "Iteration 47, loss = 0.31113027\n",
      "Iteration 48, loss = 0.30891656\n",
      "Iteration 49, loss = 0.29685750\n",
      "Iteration 50, loss = 0.30426244\n",
      "Iteration 51, loss = 0.31767059\n",
      "Iteration 52, loss = 0.30287546\n",
      "Iteration 53, loss = 0.30247036\n",
      "Iteration 54, loss = 0.29808782\n",
      "Iteration 55, loss = 0.31894005\n",
      "Iteration 56, loss = 0.29961985\n",
      "Iteration 57, loss = 0.29422164\n",
      "Iteration 58, loss = 0.27938241\n",
      "Iteration 59, loss = 0.30487597\n",
      "Iteration 60, loss = 0.30500330\n",
      "Iteration 61, loss = 0.27341541\n",
      "Iteration 62, loss = 0.27191543\n",
      "Iteration 63, loss = 0.27543449\n",
      "Iteration 64, loss = 0.34630695\n",
      "Iteration 65, loss = 0.30044369\n",
      "Iteration 66, loss = 0.27695412\n",
      "Iteration 67, loss = 0.27007211\n",
      "Iteration 68, loss = 0.28086233\n",
      "Iteration 69, loss = 0.26821375\n",
      "Iteration 70, loss = 0.26104198\n",
      "Iteration 71, loss = 0.27116898\n",
      "Iteration 72, loss = 0.26487204\n",
      "Iteration 73, loss = 0.26950214\n",
      "Iteration 74, loss = 0.28040846\n",
      "Iteration 75, loss = 0.30254775\n",
      "Iteration 76, loss = 0.26908930\n",
      "Iteration 77, loss = 0.27809652\n",
      "Iteration 78, loss = 0.27108279\n",
      "Iteration 79, loss = 0.27550834\n",
      "Iteration 80, loss = 0.26680365\n",
      "Iteration 81, loss = 0.30900032\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.64656992\n",
      "Iteration 2, loss = 2.85644078\n",
      "Iteration 3, loss = 0.56463944\n",
      "Iteration 4, loss = 0.38881809\n",
      "Iteration 5, loss = 0.41137671\n",
      "Iteration 6, loss = 0.44509022\n",
      "Iteration 7, loss = 0.44822282\n",
      "Iteration 8, loss = 0.42628329\n",
      "Iteration 9, loss = 0.44315188\n",
      "Iteration 10, loss = 0.44267914\n",
      "Iteration 11, loss = 0.45026918\n",
      "Iteration 12, loss = 0.44896291\n",
      "Iteration 13, loss = 0.43305744\n",
      "Iteration 14, loss = 0.41699642\n",
      "Iteration 15, loss = 0.38998699\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.13141820\n",
      "Iteration 2, loss = 4.65737259\n",
      "Iteration 3, loss = 4.15162905\n",
      "Iteration 4, loss = 5.00824936\n",
      "Iteration 5, loss = 4.59070631\n",
      "Iteration 6, loss = 4.10651095\n",
      "Iteration 7, loss = 4.36415005\n",
      "Iteration 8, loss = 3.94395569\n",
      "Iteration 9, loss = 3.86260786\n",
      "Iteration 10, loss = 3.55305842\n",
      "Iteration 11, loss = 4.19611880\n",
      "Iteration 12, loss = 4.01240149\n",
      "Iteration 13, loss = 3.45597005\n",
      "Iteration 14, loss = 3.80199982\n",
      "Iteration 15, loss = 3.68434207\n",
      "Iteration 16, loss = 4.07876279\n",
      "Iteration 17, loss = 3.86092840\n",
      "Iteration 18, loss = 3.87733321\n",
      "Iteration 19, loss = 3.55352376\n",
      "Iteration 20, loss = 3.96036538\n",
      "Iteration 21, loss = 3.70642126\n",
      "Iteration 22, loss = 3.65447851\n",
      "Iteration 23, loss = 3.27971356\n",
      "Iteration 24, loss = 3.11963025\n",
      "Iteration 25, loss = 3.23969687\n",
      "Iteration 26, loss = 3.32286648\n",
      "Iteration 27, loss = 3.27228271\n",
      "Iteration 28, loss = 3.66950440\n",
      "Iteration 29, loss = 3.17762340\n",
      "Iteration 30, loss = 3.05375218\n",
      "Iteration 31, loss = 2.92791251\n",
      "Iteration 32, loss = 3.44660393\n",
      "Iteration 33, loss = 3.15442891\n",
      "Iteration 34, loss = 3.28159611\n",
      "Iteration 35, loss = 3.51574597\n",
      "Iteration 36, loss = 3.27738230\n",
      "Iteration 37, loss = 3.14180988\n",
      "Iteration 12, loss = 0.33843686\n",
      "Iteration 13, loss = 0.33073238\n",
      "Iteration 14, loss = 0.34857813\n",
      "Iteration 15, loss = 0.32466532\n",
      "Iteration 16, loss = 0.31929802\n",
      "Iteration 17, loss = 0.30489573\n",
      "Iteration 18, loss = 0.30278815\n",
      "Iteration 19, loss = 0.30386948\n",
      "Iteration 20, loss = 0.31009412\n",
      "Iteration 21, loss = 0.29126980\n",
      "Iteration 22, loss = 0.29962275\n",
      "Iteration 23, loss = 0.29676968\n",
      "Iteration 24, loss = 0.28835326\n",
      "Iteration 25, loss = 0.29539973\n",
      "Iteration 26, loss = 0.30785738\n",
      "Iteration 27, loss = 0.29848506\n",
      "Iteration 28, loss = 0.28663958\n",
      "Iteration 29, loss = 0.29018834\n",
      "Iteration 30, loss = 0.28354246\n",
      "Iteration 31, loss = 0.28385747\n",
      "Iteration 32, loss = 0.28169338\n",
      "Iteration 33, loss = 0.27131886\n",
      "Iteration 34, loss = 0.27636691\n",
      "Iteration 35, loss = 0.29458031\n",
      "Iteration 36, loss = 0.28171488\n",
      "Iteration 37, loss = 0.27392482\n",
      "Iteration 38, loss = 0.27618959\n",
      "Iteration 39, loss = 0.26012561\n",
      "Iteration 40, loss = 0.26886888\n",
      "Iteration 41, loss = 0.26996396\n",
      "Iteration 42, loss = 0.27737112\n",
      "Iteration 43, loss = 0.26490474\n",
      "Iteration 44, loss = 0.27714178\n",
      "Iteration 45, loss = 0.26608518\n",
      "Iteration 46, loss = 0.31660378\n",
      "Iteration 47, loss = 0.31589898\n",
      "Iteration 48, loss = 0.29907910\n",
      "Iteration 49, loss = 0.29248184\n",
      "Iteration 50, loss = 0.27749405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.57966557\n",
      "Iteration 2, loss = 3.19750499\n",
      "Iteration 3, loss = 1.89897742\n",
      "Iteration 4, loss = 0.45881206\n",
      "Iteration 5, loss = 0.41815436\n",
      "Iteration 6, loss = 0.37998628\n",
      "Iteration 7, loss = 0.35688794\n",
      "Iteration 8, loss = 0.37105300\n",
      "Iteration 9, loss = 0.36066253\n",
      "Iteration 10, loss = 0.36604555\n",
      "Iteration 11, loss = 0.38309959\n",
      "Iteration 12, loss = 0.36295484\n",
      "Iteration 13, loss = 0.34017160\n",
      "Iteration 14, loss = 0.35272253\n",
      "Iteration 15, loss = 0.34061423\n",
      "Iteration 16, loss = 0.33001668\n",
      "Iteration 17, loss = 0.32292533\n",
      "Iteration 18, loss = 0.32856059\n",
      "Iteration 19, loss = 0.32916748\n",
      "Iteration 20, loss = 0.32870208\n",
      "Iteration 21, loss = 0.31851248\n",
      "Iteration 22, loss = 0.30788555\n",
      "Iteration 23, loss = 0.31577360\n",
      "Iteration 24, loss = 0.29546881\n",
      "Iteration 25, loss = 0.29685307\n",
      "Iteration 26, loss = 0.30363836\n",
      "Iteration 27, loss = 0.29798555\n",
      "Iteration 28, loss = 0.30986874\n",
      "Iteration 29, loss = 0.30019612\n",
      "Iteration 30, loss = 0.30620369\n",
      "Iteration 31, loss = 0.31120203\n",
      "Iteration 32, loss = 0.32033342\n",
      "Iteration 33, loss = 0.31602087\n",
      "Iteration 34, loss = 0.35109913\n",
      "Iteration 35, loss = 0.35595903\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.59549668\n",
      "Iteration 2, loss = 0.63276781\n",
      "Iteration 3, loss = 0.54859803\n",
      "Iteration 4, loss = 0.52967872\n",
      "Iteration 5, loss = 0.52633184\n",
      "Iteration 6, loss = 0.48850707\n",
      "Iteration 7, loss = 0.44820327\n",
      "Iteration 8, loss = 0.39805811\n",
      "Iteration 9, loss = 0.38317355\n",
      "Iteration 10, loss = 0.40259630\n",
      "Iteration 11, loss = 0.38459660\n",
      "Iteration 12, loss = 0.35909248\n",
      "Iteration 13, loss = 0.35604715\n",
      "Iteration 14, loss = 0.32144682\n",
      "Iteration 15, loss = 0.38436114\n",
      "Iteration 16, loss = 0.35814025\n",
      "Iteration 17, loss = 0.33235443\n",
      "Iteration 18, loss = 0.36034877\n",
      "Iteration 19, loss = 0.34682141\n",
      "Iteration 20, loss = 0.35356440\n",
      "Iteration 21, loss = 0.34509070\n",
      "Iteration 22, loss = 0.34017861\n",
      "Iteration 23, loss = 0.33304916\n",
      "Iteration 24, loss = 0.34683576\n",
      "Iteration 25, loss = 0.35108543\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.91398744\n",
      "Iteration 2, loss = 1.46154348\n",
      "Iteration 3, loss = 0.59383843\n",
      "Iteration 4, loss = 0.42093987\n",
      "Iteration 5, loss = 0.36803431\n",
      "Iteration 6, loss = 0.44155170\n",
      "Iteration 7, loss = 0.41007733\n",
      "Iteration 8, loss = 0.37638517\n",
      "Iteration 9, loss = 0.35262382\n",
      "Iteration 10, loss = 0.36787421\n",
      "Iteration 11, loss = 0.33500303\n",
      "Iteration 12, loss = 0.33842903\n",
      "Iteration 13, loss = 0.33053883\n",
      "Iteration 14, loss = 0.32952708\n",
      "Iteration 15, loss = 0.33429349\n",
      "Iteration 16, loss = 0.31785721\n",
      "Iteration 17, loss = 0.34269949\n",
      "Iteration 18, loss = 0.30667370\n",
      "Iteration 19, loss = 0.29521135\n",
      "Iteration 20, loss = 0.30052193\n",
      "Iteration 21, loss = 0.28906328\n",
      "Iteration 22, loss = 0.30537362\n",
      "Iteration 23, loss = 0.29297789\n",
      "Iteration 24, loss = 0.29106727\n",
      "Iteration 25, loss = 0.28649571\n",
      "Iteration 26, loss = 0.28832692\n",
      "Iteration 27, loss = 0.29004134\n",
      "Iteration 28, loss = 0.29531996\n",
      "Iteration 29, loss = 0.29543124\n",
      "Iteration 30, loss = 0.28360393\n",
      "Iteration 31, loss = 0.30283322\n",
      "Iteration 32, loss = 0.29026206\n",
      "Iteration 33, loss = 0.30274090\n",
      "Iteration 34, loss = 0.28939621\n",
      "Iteration 35, loss = 0.28651716\n",
      "Iteration 36, loss = 0.29638962\n",
      "Iteration 37, loss = 0.29102460\n",
      "Iteration 38, loss = 0.28734001\n",
      "Iteration 39, loss = 0.29076583\n",
      "Iteration 40, loss = 0.28300318\n",
      "Iteration 41, loss = 0.28598011\n",
      "Iteration 42, loss = 0.30050065\n",
      "Iteration 43, loss = 0.28886998\n",
      "Iteration 44, loss = 0.31064871\n",
      "Iteration 45, loss = 0.29268079\n",
      "Iteration 46, loss = 0.28967595\n",
      "Iteration 47, loss = 0.28855428\n",
      "Iteration 48, loss = 0.30144263\n",
      "Iteration 49, loss = 0.28588218\n",
      "Iteration 50, loss = 0.28689155\n",
      "Iteration 51, loss = 0.29238200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.21753662\n",
      "Iteration 2, loss = 1.49440650\n",
      "Iteration 3, loss = 0.41123100\n",
      "Iteration 4, loss = 0.36783297\n",
      "Iteration 5, loss = 0.39434308\n",
      "Iteration 6, loss = 0.41729132\n",
      "Iteration 7, loss = 0.39953083\n",
      "Iteration 8, loss = 0.41725396\n",
      "Iteration 9, loss = 0.43025526\n",
      "Iteration 10, loss = 0.38391686\n",
      "Iteration 11, loss = 0.39682283\n",
      "Iteration 12, loss = 0.37311052\n",
      "Iteration 13, loss = 0.37005024\n",
      "Iteration 14, loss = 0.38344750\n",
      "Iteration 15, loss = 0.36540997\n",
      "Iteration 16, loss = 0.36977932\n",
      "Iteration 17, loss = 0.36844927\n",
      "Iteration 18, loss = 0.34325364\n",
      "Iteration 19, loss = 0.36366560\n",
      "Iteration 20, loss = 0.35801090\n",
      "Iteration 21, loss = 0.34999234\n",
      "Iteration 22, loss = 0.32970164\n",
      "Iteration 23, loss = 0.32073256\n",
      "Iteration 24, loss = 0.32153505\n",
      "Iteration 25, loss = 0.34255540\n",
      "Iteration 26, loss = 0.35243624\n",
      "Iteration 27, loss = 0.34835685\n",
      "Iteration 28, loss = 0.37441150\n",
      "Iteration 29, loss = 0.35425253\n",
      "Iteration 30, loss = 0.36507331\n",
      "Iteration 31, loss = 0.36070357\n",
      "Iteration 32, loss = 0.34156110\n",
      "Iteration 33, loss = 0.36827163\n",
      "Iteration 34, loss = 0.35022545\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 0.28155430\n",
      "Iteration 49, loss = 0.27600521\n",
      "Iteration 50, loss = 0.26775796\n",
      "Iteration 51, loss = 0.26161241\n",
      "Iteration 52, loss = 0.28481188\n",
      "Iteration 53, loss = 0.29014417\n",
      "Iteration 54, loss = 0.27205913\n",
      "Iteration 55, loss = 0.26466583\n",
      "Iteration 56, loss = 0.25918039\n",
      "Iteration 57, loss = 0.26151096\n",
      "Iteration 58, loss = 0.26228186\n",
      "Iteration 59, loss = 0.26331525\n",
      "Iteration 60, loss = 0.25875009\n",
      "Iteration 61, loss = 0.25641850\n",
      "Iteration 62, loss = 0.26125741\n",
      "Iteration 63, loss = 0.26637000\n",
      "Iteration 64, loss = 0.26322905\n",
      "Iteration 65, loss = 0.25856132\n",
      "Iteration 66, loss = 0.25992137\n",
      "Iteration 67, loss = 0.26505743\n",
      "Iteration 68, loss = 0.25805234\n",
      "Iteration 69, loss = 0.26054176\n",
      "Iteration 70, loss = 0.25538632\n",
      "Iteration 71, loss = 0.26537055\n",
      "Iteration 72, loss = 0.25992874\n",
      "Iteration 73, loss = 0.26612860\n",
      "Iteration 74, loss = 0.25960155\n",
      "Iteration 75, loss = 0.25761366\n",
      "Iteration 76, loss = 0.25550727\n",
      "Iteration 77, loss = 0.26007770\n",
      "Iteration 78, loss = 0.26051240\n",
      "Iteration 79, loss = 0.25833742\n",
      "Iteration 80, loss = 0.26243087\n",
      "Iteration 81, loss = 0.25738038\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.79314259\n",
      "Iteration 2, loss = 4.41274373\n",
      "Iteration 3, loss = 3.83466502\n",
      "Iteration 4, loss = 4.03073543\n",
      "Iteration 5, loss = 3.82297806\n",
      "Iteration 6, loss = 3.97197669\n",
      "Iteration 7, loss = 3.77466575\n",
      "Iteration 8, loss = 3.83693643\n",
      "Iteration 9, loss = 3.51879992\n",
      "Iteration 10, loss = 3.61812599\n",
      "Iteration 11, loss = 3.45569295\n",
      "Iteration 12, loss = 3.58472828\n",
      "Iteration 13, loss = 3.46007027\n",
      "Iteration 14, loss = 3.71769804\n",
      "Iteration 15, loss = 3.58305320\n",
      "Iteration 16, loss = 3.94872858\n",
      "Iteration 17, loss = 3.15400239\n",
      "Iteration 18, loss = 3.78860536\n",
      "Iteration 19, loss = 3.74534977\n",
      "Iteration 20, loss = 3.57607956\n",
      "Iteration 21, loss = 3.87954875\n",
      "Iteration 22, loss = 3.10683174\n",
      "Iteration 23, loss = 3.33360521\n",
      "Iteration 24, loss = 3.33762608\n",
      "Iteration 25, loss = 3.53083842\n",
      "Iteration 26, loss = 3.19636200\n",
      "Iteration 27, loss = 3.17241434\n",
      "Iteration 28, loss = 3.19516814\n",
      "Iteration 29, loss = 3.37906069\n",
      "Iteration 30, loss = 3.15244447\n",
      "Iteration 31, loss = 3.71087611\n",
      "Iteration 32, loss = 3.72093389\n",
      "Iteration 33, loss = 3.50986623\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.91398744\n",
      "Iteration 2, loss = 1.46154348\n",
      "Iteration 3, loss = 0.59383843\n",
      "Iteration 4, loss = 0.42093987\n",
      "Iteration 5, loss = 0.36803431\n",
      "Iteration 6, loss = 0.44155170\n",
      "Iteration 7, loss = 0.41007733\n",
      "Iteration 8, loss = 0.37638517\n",
      "Iteration 9, loss = 0.35262382\n",
      "Iteration 10, loss = 0.36787421\n",
      "Iteration 11, loss = 0.33500303\n",
      "Iteration 12, loss = 0.33842903\n",
      "Iteration 13, loss = 0.33053883\n",
      "Iteration 14, loss = 0.32952708\n",
      "Iteration 15, loss = 0.33429349\n",
      "Iteration 16, loss = 0.31785721\n",
      "Iteration 17, loss = 0.34269949\n",
      "Iteration 18, loss = 0.30667370\n",
      "Iteration 19, loss = 0.29521135\n",
      "Iteration 20, loss = 0.30052193\n",
      "Iteration 21, loss = 0.28906328\n",
      "Iteration 22, loss = 0.30537362\n",
      "Iteration 23, loss = 0.29297789\n",
      "Iteration 24, loss = 0.29106727\n",
      "Iteration 25, loss = 0.28649571\n",
      "Iteration 26, loss = 0.28832692\n",
      "Iteration 27, loss = 0.29004134\n",
      "Iteration 28, loss = 0.29531996\n",
      "Iteration 29, loss = 0.29543124\n",
      "Iteration 30, loss = 0.28360393\n",
      "Iteration 31, loss = 0.30283322\n",
      "Iteration 32, loss = 0.29026206\n",
      "Iteration 33, loss = 0.30274090\n",
      "Iteration 34, loss = 0.28939621\n",
      "Iteration 35, loss = 0.28651716\n",
      "Iteration 36, loss = 0.29638962\n",
      "Iteration 37, loss = 0.29102460\n",
      "Iteration 38, loss = 0.28734001\n",
      "Iteration 39, loss = 0.29076583\n",
      "Iteration 40, loss = 0.28300318\n",
      "Iteration 41, loss = 0.28598011\n",
      "Iteration 42, loss = 0.30050065\n",
      "Iteration 43, loss = 0.28886998\n",
      "Iteration 44, loss = 0.31064871\n",
      "Iteration 45, loss = 0.29268079\n",
      "Iteration 46, loss = 0.28967595\n",
      "Iteration 47, loss = 0.28855428\n",
      "Iteration 48, loss = 0.30144263\n",
      "Iteration 49, loss = 0.28588218\n",
      "Iteration 50, loss = 0.28689155\n",
      "Iteration 51, loss = 0.29238200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.42016330\n",
      "Iteration 2, loss = 2.10049625\n",
      "Iteration 3, loss = 0.57970451\n",
      "Iteration 4, loss = 0.48060755\n",
      "Iteration 5, loss = 0.42975520\n",
      "Iteration 6, loss = 0.44813971\n",
      "Iteration 7, loss = 0.40906134\n",
      "Iteration 8, loss = 0.43798888\n",
      "Iteration 9, loss = 0.43839032\n",
      "Iteration 10, loss = 0.43532763\n",
      "Iteration 11, loss = 0.43562327\n",
      "Iteration 12, loss = 0.41781084\n",
      "Iteration 13, loss = 0.42620083\n",
      "Iteration 14, loss = 0.44178357\n",
      "Iteration 15, loss = 0.41138721\n",
      "Iteration 16, loss = 0.40683153\n",
      "Iteration 17, loss = 0.39917287\n",
      "Iteration 18, loss = 0.38283113\n",
      "Iteration 19, loss = 0.39567180\n",
      "Iteration 20, loss = 0.36296543\n",
      "Iteration 21, loss = 0.36233920\n",
      "Iteration 22, loss = 0.36649478\n",
      "Iteration 23, loss = 0.35447110\n",
      "Iteration 24, loss = 0.34446977\n",
      "Iteration 25, loss = 0.34759664\n",
      "Iteration 26, loss = 0.35501177\n",
      "Iteration 27, loss = 0.33202787\n",
      "Iteration 28, loss = 0.36435560\n",
      "Iteration 29, loss = 0.34332833\n",
      "Iteration 30, loss = 0.31531000\n",
      "Iteration 31, loss = 0.35045442\n",
      "Iteration 32, loss = 0.34509376\n",
      "Iteration 33, loss = 0.34103446\n",
      "Iteration 34, loss = 0.34742211\n",
      "Iteration 35, loss = 0.35069711\n",
      "Iteration 36, loss = 0.35869847\n",
      "Iteration 37, loss = 0.31822824\n",
      "Iteration 38, loss = 0.34400329\n",
      "Iteration 39, loss = 0.34895976\n",
      "Iteration 40, loss = 0.36190276\n",
      "Iteration 41, loss = 0.36515641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.30300115\n",
      "Iteration 42, loss = 0.31049387\n",
      "Iteration 43, loss = 0.31248265\n",
      "Iteration 44, loss = 0.31201241\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.20899925\n",
      "Iteration 2, loss = 0.65856363\n",
      "Iteration 3, loss = 0.48165801\n",
      "Iteration 4, loss = 0.46737801\n",
      "Iteration 5, loss = 0.48225891\n",
      "Iteration 6, loss = 0.51062058\n",
      "Iteration 7, loss = 0.42735684\n",
      "Iteration 8, loss = 0.47928670\n",
      "Iteration 9, loss = 0.41597692\n",
      "Iteration 10, loss = 0.37359306\n",
      "Iteration 11, loss = 0.40741364\n",
      "Iteration 12, loss = 0.38983454\n",
      "Iteration 13, loss = 0.39554396\n",
      "Iteration 14, loss = 0.43475731\n",
      "Iteration 15, loss = 0.40322389\n",
      "Iteration 16, loss = 0.38275737\n",
      "Iteration 17, loss = 0.36958765\n",
      "Iteration 18, loss = 0.36317078\n",
      "Iteration 19, loss = 0.34899029\n",
      "Iteration 20, loss = 0.38540267\n",
      "Iteration 21, loss = 0.36138041\n",
      "Iteration 22, loss = 0.34986616\n",
      "Iteration 23, loss = 0.35504899\n",
      "Iteration 24, loss = 0.34098256\n",
      "Iteration 25, loss = 0.31767887\n",
      "Iteration 26, loss = 0.35272722\n",
      "Iteration 27, loss = 0.32289229\n",
      "Iteration 28, loss = 0.33547339\n",
      "Iteration 29, loss = 0.32672522\n",
      "Iteration 30, loss = 0.32249429\n",
      "Iteration 31, loss = 0.32759010\n",
      "Iteration 32, loss = 0.31041436\n",
      "Iteration 33, loss = 0.33212879\n",
      "Iteration 34, loss = 0.35880059\n",
      "Iteration 35, loss = 0.32828649\n",
      "Iteration 36, loss = 0.33833941\n",
      "Iteration 37, loss = 0.32515363\n",
      "Iteration 38, loss = 0.33129814\n",
      "Iteration 39, loss = 0.34474944\n",
      "Iteration 40, loss = 0.33740601\n",
      "Iteration 41, loss = 0.34373288\n",
      "Iteration 42, loss = 0.37064532\n",
      "Iteration 43, loss = 0.33518871\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.64656992\n",
      "Iteration 2, loss = 2.85644078\n",
      "Iteration 3, loss = 0.56463944\n",
      "Iteration 4, loss = 0.38881809\n",
      "Iteration 5, loss = 0.41137671\n",
      "Iteration 6, loss = 0.44509022\n",
      "Iteration 7, loss = 0.44822282\n",
      "Iteration 8, loss = 0.42628329\n",
      "Iteration 9, loss = 0.44315188\n",
      "Iteration 10, loss = 0.44267914\n",
      "Iteration 11, loss = 0.45026918\n",
      "Iteration 12, loss = 0.44896291\n",
      "Iteration 13, loss = 0.43305744\n",
      "Iteration 14, loss = 0.41699642\n",
      "Iteration 15, loss = 0.38998699\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.17633273\n",
      "Iteration 2, loss = 2.90299961\n",
      "Iteration 3, loss = 1.32735331\n",
      "Iteration 4, loss = 0.42579510\n",
      "Iteration 5, loss = 0.41564170\n",
      "Iteration 6, loss = 0.39684540\n",
      "Iteration 7, loss = 0.42422992\n",
      "Iteration 8, loss = 0.41486771\n",
      "Iteration 9, loss = 0.43783232\n",
      "Iteration 10, loss = 0.41768865\n",
      "Iteration 11, loss = 0.43662762\n",
      "Iteration 12, loss = 0.39961253\n",
      "Iteration 13, loss = 0.41312635\n",
      "Iteration 14, loss = 0.43039274\n",
      "Iteration 15, loss = 0.41818409\n",
      "Iteration 16, loss = 0.41696041\n",
      "Iteration 17, loss = 0.45601176\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.51532897\n",
      "Iteration 5, loss = 0.45662602\n",
      "Iteration 6, loss = 0.44074748\n",
      "Iteration 7, loss = 0.38952303\n",
      "Iteration 8, loss = 0.35552700\n",
      "Iteration 9, loss = 0.37544397\n",
      "Iteration 10, loss = 0.36104833\n",
      "Iteration 11, loss = 0.34926239\n",
      "Iteration 12, loss = 0.35322118\n",
      "Iteration 13, loss = 0.34761807\n",
      "Iteration 14, loss = 0.37255946\n",
      "Iteration 15, loss = 0.33101116\n",
      "Iteration 16, loss = 0.31430193\n",
      "Iteration 17, loss = 0.32052047\n",
      "Iteration 18, loss = 0.30944213\n",
      "Iteration 19, loss = 0.30697693\n",
      "Iteration 20, loss = 0.33581713\n",
      "Iteration 21, loss = 0.33031619\n",
      "Iteration 22, loss = 0.32358899\n",
      "Iteration 23, loss = 0.31380988\n",
      "Iteration 24, loss = 0.30066387\n",
      "Iteration 25, loss = 0.28981257\n",
      "Iteration 26, loss = 0.31025298\n",
      "Iteration 27, loss = 0.29804076\n",
      "Iteration 28, loss = 0.29650502\n",
      "Iteration 29, loss = 0.29480945\n",
      "Iteration 30, loss = 0.28688976\n",
      "Iteration 31, loss = 0.28709372\n",
      "Iteration 32, loss = 0.27866164\n",
      "Iteration 33, loss = 0.27312111\n",
      "Iteration 34, loss = 0.31659889\n",
      "Iteration 35, loss = 0.29414948\n",
      "Iteration 36, loss = 0.28156818\n",
      "Iteration 37, loss = 0.28526314\n",
      "Iteration 38, loss = 0.31120116\n",
      "Iteration 39, loss = 0.28912902\n",
      "Iteration 40, loss = 0.27391718\n",
      "Iteration 41, loss = 0.28063642\n",
      "Iteration 42, loss = 0.28396534\n",
      "Iteration 43, loss = 0.27088316\n",
      "Iteration 44, loss = 0.27864633\n",
      "Iteration 45, loss = 0.27422212\n",
      "Iteration 46, loss = 0.28068271\n",
      "Iteration 47, loss = 0.28768431\n",
      "Iteration 48, loss = 0.28755169\n",
      "Iteration 49, loss = 0.29074401\n",
      "Iteration 50, loss = 0.26797507\n",
      "Iteration 51, loss = 0.28172250\n",
      "Iteration 52, loss = 0.27057843\n",
      "Iteration 53, loss = 0.26722382\n",
      "Iteration 54, loss = 0.26322614\n",
      "Iteration 55, loss = 0.26639811\n",
      "Iteration 56, loss = 0.26285160\n",
      "Iteration 57, loss = 0.26835541\n",
      "Iteration 58, loss = 0.25492556\n",
      "Iteration 59, loss = 0.26085761\n",
      "Iteration 60, loss = 0.26058261\n",
      "Iteration 61, loss = 0.26051932\n",
      "Iteration 62, loss = 0.26700199\n",
      "Iteration 63, loss = 0.27049569\n",
      "Iteration 64, loss = 0.25868281\n",
      "Iteration 65, loss = 0.26020009\n",
      "Iteration 66, loss = 0.26465336\n",
      "Iteration 67, loss = 0.26305809\n",
      "Iteration 68, loss = 0.25809537\n",
      "Iteration 69, loss = 0.25490357\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.38533449\n",
      "Iteration 2, loss = 0.78497038\n",
      "Iteration 3, loss = 0.40781880\n",
      "Iteration 4, loss = 0.36660022\n",
      "Iteration 5, loss = 0.37169451\n",
      "Iteration 6, loss = 0.42997659\n",
      "Iteration 7, loss = 0.40835345\n",
      "Iteration 8, loss = 0.41248069\n",
      "Iteration 9, loss = 0.41981221\n",
      "Iteration 10, loss = 0.41149161\n",
      "Iteration 11, loss = 0.39686428\n",
      "Iteration 12, loss = 0.39961607\n",
      "Iteration 13, loss = 0.39299466\n",
      "Iteration 14, loss = 0.38896295\n",
      "Iteration 15, loss = 0.39922846\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.09942500\n",
      "Iteration 2, loss = 2.70574001\n",
      "Iteration 3, loss = 0.62048009\n",
      "Iteration 4, loss = 0.43295366\n",
      "Iteration 5, loss = 0.36098918\n",
      "Iteration 6, loss = 0.42314021\n",
      "Iteration 7, loss = 0.37728536\n",
      "Iteration 8, loss = 0.38944177\n",
      "Iteration 9, loss = 0.36988345\n",
      "Iteration 10, loss = 0.39445837\n",
      "Iteration 11, loss = 0.37833830\n",
      "Iteration 12, loss = 0.41667994\n",
      "Iteration 13, loss = 0.40590750\n",
      "Iteration 14, loss = 0.39924996\n",
      "Iteration 15, loss = 0.37488251\n",
      "Iteration 16, loss = 0.40506877\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.93477381\n",
      "Iteration 2, loss = 0.45963750\n",
      "Iteration 3, loss = 0.43059504\n",
      "Iteration 4, loss = 0.42807264\n",
      "Iteration 5, loss = 0.43102386\n",
      "Iteration 6, loss = 0.44070639\n",
      "Iteration 7, loss = 0.43935673\n",
      "Iteration 8, loss = 0.39118774\n",
      "Iteration 9, loss = 0.36298326\n",
      "Iteration 10, loss = 0.35317001\n",
      "Iteration 11, loss = 0.34614297\n",
      "Iteration 12, loss = 0.33337969\n",
      "Iteration 13, loss = 0.32435777\n",
      "Iteration 14, loss = 0.35465160\n",
      "Iteration 15, loss = 0.34302293\n",
      "Iteration 16, loss = 0.30878997\n",
      "Iteration 17, loss = 0.31243914\n",
      "Iteration 18, loss = 0.31110067\n",
      "Iteration 19, loss = 0.31140267\n",
      "Iteration 20, loss = 0.32592510\n",
      "Iteration 21, loss = 0.31942046\n",
      "Iteration 22, loss = 0.29082931\n",
      "Iteration 23, loss = 0.29568169\n",
      "Iteration 24, loss = 0.30215217\n",
      "Iteration 25, loss = 0.29562651\n",
      "Iteration 26, loss = 0.30644527\n",
      "Iteration 27, loss = 0.29200685\n",
      "Iteration 28, loss = 0.30050760\n",
      "Iteration 29, loss = 0.28680879\n",
      "Iteration 30, loss = 0.27936613\n",
      "Iteration 31, loss = 0.29455102\n",
      "Iteration 32, loss = 0.28358411\n",
      "Iteration 33, loss = 0.27456133\n",
      "Iteration 34, loss = 0.27873207\n",
      "Iteration 35, loss = 0.29094966\n",
      "Iteration 36, loss = 0.28498096\n",
      "Iteration 37, loss = 0.28406208\n",
      "Iteration 38, loss = 0.29751181\n",
      "Iteration 39, loss = 0.27387838\n",
      "Iteration 40, loss = 0.28445138\n",
      "Iteration 41, loss = 0.28204166\n",
      "Iteration 42, loss = 0.27344077\n",
      "Iteration 43, loss = 0.28147335\n",
      "Iteration 44, loss = 0.28697449\n",
      "Iteration 45, loss = 0.28020272\n",
      "Iteration 46, loss = 0.28472694\n",
      "Iteration 47, loss = 0.29006092\n",
      "Iteration 48, loss = 0.28493134\n",
      "Iteration 49, loss = 0.28207002\n",
      "Iteration 50, loss = 0.26908924\n",
      "Iteration 51, loss = 0.27693988\n",
      "Iteration 52, loss = 0.26722338\n",
      "Iteration 53, loss = 0.27586417\n",
      "Iteration 54, loss = 0.27420734\n",
      "Iteration 55, loss = 0.26905369\n",
      "Iteration 56, loss = 0.27216774\n",
      "Iteration 57, loss = 0.30027229\n",
      "Iteration 58, loss = 0.28713191\n",
      "Iteration 59, loss = 0.28089439\n",
      "Iteration 60, loss = 0.29629246\n",
      "Iteration 61, loss = 0.27814092\n",
      "Iteration 62, loss = 0.31247171\n",
      "Iteration 63, loss = 0.32421701\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.42016330\n",
      "Iteration 2, loss = 2.10049625\n",
      "Iteration 3, loss = 0.57970451\n",
      "Iteration 4, loss = 0.48060755\n",
      "Iteration 5, loss = 0.42975520\n",
      "Iteration 6, loss = 0.44813971\n",
      "Iteration 7, loss = 0.40906134\n",
      "Iteration 8, loss = 0.43798888\n",
      "Iteration 9, loss = 0.43839032\n",
      "Iteration 10, loss = 0.43532763\n",
      "Iteration 11, loss = 0.43562327\n",
      "Iteration 12, loss = 0.41781084\n",
      "Iteration 13, loss = 0.42620083\n",
      "Iteration 14, loss = 0.44178357\n",
      "Iteration 15, loss = 0.41138721\n",
      "Iteration 16, loss = 0.40683153\n",
      "Iteration 17, loss = 0.39917287\n",
      "Iteration 18, loss = 0.38283113\n",
      "Iteration 19, loss = 0.39567180\n",
      "Iteration 20, loss = 0.36296543\n",
      "Iteration 21, loss = 0.36233920\n",
      "Iteration 22, loss = 0.36649478\n",
      "Iteration 23, loss = 0.35447110\n",
      "Iteration 24, loss = 0.34446977\n",
      "Iteration 25, loss = 0.34759664\n",
      "Iteration 26, loss = 0.35501177\n",
      "Iteration 27, loss = 0.33202787\n",
      "Iteration 28, loss = 0.36435560\n",
      "Iteration 29, loss = 0.34332833\n",
      "Iteration 30, loss = 0.31531000\n",
      "Iteration 31, loss = 0.35045442\n",
      "Iteration 32, loss = 0.34509376\n",
      "Iteration 33, loss = 0.34103446\n",
      "Iteration 34, loss = 0.34742211\n",
      "Iteration 35, loss = 0.35069711\n",
      "Iteration 36, loss = 0.35869847\n",
      "Iteration 37, loss = 0.31822824\n",
      "Iteration 38, loss = 0.34400329\n",
      "Iteration 39, loss = 0.34895976\n",
      "Iteration 40, loss = 0.36190276\n",
      "Iteration 41, loss = 0.36515641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.26722382\n",
      "Iteration 54, loss = 0.26322614\n",
      "Iteration 55, loss = 0.26639811\n",
      "Iteration 56, loss = 0.26285160\n",
      "Iteration 57, loss = 0.26835541\n",
      "Iteration 58, loss = 0.25492556\n",
      "Iteration 59, loss = 0.26085761\n",
      "Iteration 60, loss = 0.26058261\n",
      "Iteration 61, loss = 0.26051932\n",
      "Iteration 62, loss = 0.26700199\n",
      "Iteration 63, loss = 0.27049569\n",
      "Iteration 64, loss = 0.25868281\n",
      "Iteration 65, loss = 0.26020009\n",
      "Iteration 66, loss = 0.26465336\n",
      "Iteration 67, loss = 0.26305809\n",
      "Iteration 68, loss = 0.25809537\n",
      "Iteration 69, loss = 0.25490357\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.57966557\n",
      "Iteration 2, loss = 3.19750499\n",
      "Iteration 3, loss = 1.89897742\n",
      "Iteration 4, loss = 0.45881206\n",
      "Iteration 5, loss = 0.41815436\n",
      "Iteration 6, loss = 0.37998628\n",
      "Iteration 7, loss = 0.35688794\n",
      "Iteration 8, loss = 0.37105300\n",
      "Iteration 9, loss = 0.36066253\n",
      "Iteration 10, loss = 0.36604555\n",
      "Iteration 11, loss = 0.38309959\n",
      "Iteration 12, loss = 0.36295484\n",
      "Iteration 13, loss = 0.34017160\n",
      "Iteration 14, loss = 0.35272253\n",
      "Iteration 15, loss = 0.34061423\n",
      "Iteration 16, loss = 0.33001668\n",
      "Iteration 17, loss = 0.32292533\n",
      "Iteration 18, loss = 0.32856059\n",
      "Iteration 19, loss = 0.32916748\n",
      "Iteration 20, loss = 0.32870208\n",
      "Iteration 21, loss = 0.31851248\n",
      "Iteration 22, loss = 0.30788555\n",
      "Iteration 23, loss = 0.31577360\n",
      "Iteration 24, loss = 0.29546881\n",
      "Iteration 25, loss = 0.29685307\n",
      "Iteration 26, loss = 0.30363836\n",
      "Iteration 27, loss = 0.29798555\n",
      "Iteration 28, loss = 0.30986874\n",
      "Iteration 29, loss = 0.30019612\n",
      "Iteration 30, loss = 0.30620369\n",
      "Iteration 31, loss = 0.31120203\n",
      "Iteration 32, loss = 0.32033342\n",
      "Iteration 33, loss = 0.31602087\n",
      "Iteration 34, loss = 0.35109913\n",
      "Iteration 35, loss = 0.35595903\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.59549668\n",
      "Iteration 2, loss = 0.63276781\n",
      "Iteration 3, loss = 0.54859803\n",
      "Iteration 4, loss = 0.52967872\n",
      "Iteration 5, loss = 0.52633184\n",
      "Iteration 6, loss = 0.48850707\n",
      "Iteration 7, loss = 0.44820327\n",
      "Iteration 8, loss = 0.39805811\n",
      "Iteration 9, loss = 0.38317355\n",
      "Iteration 10, loss = 0.40259630\n",
      "Iteration 11, loss = 0.38459660\n",
      "Iteration 12, loss = 0.35909248\n",
      "Iteration 13, loss = 0.35604715\n",
      "Iteration 14, loss = 0.32144682\n",
      "Iteration 15, loss = 0.38436114\n",
      "Iteration 16, loss = 0.35814025\n",
      "Iteration 17, loss = 0.33235443\n",
      "Iteration 18, loss = 0.36034877\n",
      "Iteration 19, loss = 0.34682141\n",
      "Iteration 20, loss = 0.35356440\n",
      "Iteration 21, loss = 0.34509070\n",
      "Iteration 22, loss = 0.34017861\n",
      "Iteration 23, loss = 0.33304916\n",
      "Iteration 24, loss = 0.34683576\n",
      "Iteration 25, loss = 0.35108543\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.46538807\n",
      "Iteration 2, loss = 1.33071117\n",
      "Iteration 3, loss = 0.58969596\n",
      "Iteration 4, loss = 0.59870268\n",
      "Iteration 5, loss = 0.50863552\n",
      "Iteration 6, loss = 0.44253398\n",
      "Iteration 7, loss = 0.42469279\n",
      "Iteration 8, loss = 0.41601616\n",
      "Iteration 9, loss = 0.40065707\n",
      "Iteration 10, loss = 0.37452007\n",
      "Iteration 11, loss = 0.34852594\n",
      "Iteration 12, loss = 0.39387575\n",
      "Iteration 13, loss = 0.40470873\n",
      "Iteration 14, loss = 0.37239574\n",
      "Iteration 15, loss = 0.39015353\n",
      "Iteration 16, loss = 0.36535318\n",
      "Iteration 17, loss = 0.41607654\n",
      "Iteration 18, loss = 0.39264905\n",
      "Iteration 19, loss = 0.39533615\n",
      "Iteration 20, loss = 0.39668761\n",
      "Iteration 21, loss = 0.36462175\n",
      "Iteration 22, loss = 0.35300379\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.20899925\n",
      "Iteration 2, loss = 0.65856363\n",
      "Iteration 3, loss = 0.48165801\n",
      "Iteration 4, loss = 0.46737801\n",
      "Iteration 5, loss = 0.48225891\n",
      "Iteration 6, loss = 0.51062058\n",
      "Iteration 7, loss = 0.42735684\n",
      "Iteration 8, loss = 0.47928670\n",
      "Iteration 9, loss = 0.41597692\n",
      "Iteration 10, loss = 0.37359306\n",
      "Iteration 11, loss = 0.40741364\n",
      "Iteration 12, loss = 0.38983454\n",
      "Iteration 13, loss = 0.39554396\n",
      "Iteration 14, loss = 0.43475731\n",
      "Iteration 15, loss = 0.40322389\n",
      "Iteration 16, loss = 0.38275737\n",
      "Iteration 17, loss = 0.36958765\n",
      "Iteration 18, loss = 0.36317078\n",
      "Iteration 19, loss = 0.34899029\n",
      "Iteration 20, loss = 0.38540267\n",
      "Iteration 21, loss = 0.36138041\n",
      "Iteration 22, loss = 0.34986616\n",
      "Iteration 23, loss = 0.35504899\n",
      "Iteration 24, loss = 0.34098256\n",
      "Iteration 25, loss = 0.31767887\n",
      "Iteration 26, loss = 0.35272722\n",
      "Iteration 27, loss = 0.32289229\n",
      "Iteration 28, loss = 0.33547339\n",
      "Iteration 29, loss = 0.32672522\n",
      "Iteration 30, loss = 0.32249429\n",
      "Iteration 31, loss = 0.32759010\n",
      "Iteration 32, loss = 0.31041436\n",
      "Iteration 33, loss = 0.33212879\n",
      "Iteration 34, loss = 0.35880059\n",
      "Iteration 35, loss = 0.32828649\n",
      "Iteration 36, loss = 0.33833941\n",
      "Iteration 37, loss = 0.32515363\n",
      "Iteration 38, loss = 0.33129814\n",
      "Iteration 39, loss = 0.34474944\n",
      "Iteration 40, loss = 0.33740601\n",
      "Iteration 41, loss = 0.34373288\n",
      "Iteration 42, loss = 0.37064532\n",
      "Iteration 43, loss = 0.33518871\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.17633273\n",
      "Iteration 2, loss = 2.90299961\n",
      "Iteration 3, loss = 1.32735331\n",
      "Iteration 4, loss = 0.42579510\n",
      "Iteration 5, loss = 0.41564170\n",
      "Iteration 6, loss = 0.39684540\n",
      "Iteration 7, loss = 0.42422992\n",
      "Iteration 8, loss = 0.41486771\n",
      "Iteration 9, loss = 0.43783232\n",
      "Iteration 10, loss = 0.41768865\n",
      "Iteration 11, loss = 0.43662762\n",
      "Iteration 12, loss = 0.39961253\n",
      "Iteration 13, loss = 0.41312635\n",
      "Iteration 14, loss = 0.43039274\n",
      "Iteration 15, loss = 0.41818409\n",
      "Iteration 16, loss = 0.41696041\n",
      "Iteration 17, loss = 0.45601176\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.19590162\n",
      "Iteration 2, loss = 4.91209489\n",
      "Iteration 3, loss = 4.25464976\n",
      "Iteration 4, loss = 4.25657706\n",
      "Iteration 5, loss = 4.30405371\n",
      "Iteration 6, loss = 3.84573388\n",
      "Iteration 7, loss = 3.93858292\n",
      "Iteration 8, loss = 4.08247743\n",
      "Iteration 9, loss = 3.83390388\n",
      "Iteration 10, loss = 3.67889351\n",
      "Iteration 11, loss = 3.82252767\n",
      "Iteration 12, loss = 3.58430224\n",
      "Iteration 13, loss = 4.08572761\n",
      "Iteration 14, loss = 4.07822708\n",
      "Iteration 15, loss = 4.10690992\n",
      "Iteration 16, loss = 3.94561221\n",
      "Iteration 17, loss = 3.86299827\n",
      "Iteration 18, loss = 3.55328299\n",
      "Iteration 19, loss = 3.70575437\n",
      "Iteration 20, loss = 3.54216546\n",
      "Iteration 21, loss = 3.75291160\n",
      "Iteration 22, loss = 3.82372573\n",
      "Iteration 23, loss = 3.45524703\n",
      "Iteration 24, loss = 3.46854275\n",
      "Iteration 25, loss = 3.62264498\n",
      "Iteration 26, loss = 3.58029215\n",
      "Iteration 27, loss = 4.22127356\n",
      "Iteration 28, loss = 3.62517990\n",
      "Iteration 29, loss = 3.76363082\n",
      "Iteration 30, loss = 3.70451077\n",
      "Iteration 31, loss = 3.85279669\n",
      "Iteration 32, loss = 4.01545379\n",
      "Iteration 33, loss = 3.74784261\n",
      "Iteration 34, loss = 3.69917742\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23, loss = 0.28182723\n",
      "Iteration 24, loss = 0.27821578\n",
      "Iteration 25, loss = 0.28814125\n",
      "Iteration 26, loss = 0.28696574\n",
      "Iteration 27, loss = 0.27912478\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.54433163\n",
      "Iteration 2, loss = 1.23369870\n",
      "Iteration 3, loss = 0.54347917\n",
      "Iteration 4, loss = 0.46718929\n",
      "Iteration 5, loss = 0.44809008\n",
      "Iteration 6, loss = 0.40219987\n",
      "Iteration 7, loss = 0.43904978\n",
      "Iteration 8, loss = 0.36968434\n",
      "Iteration 9, loss = 0.35539988\n",
      "Iteration 10, loss = 0.33723355\n",
      "Iteration 11, loss = 0.37337154\n",
      "Iteration 12, loss = 0.32009673\n",
      "Iteration 13, loss = 0.36363730\n",
      "Iteration 14, loss = 0.32885513\n",
      "Iteration 15, loss = 0.33645709\n",
      "Iteration 16, loss = 0.33316510\n",
      "Iteration 17, loss = 0.35240865\n",
      "Iteration 18, loss = 0.33077383\n",
      "Iteration 19, loss = 0.34145935\n",
      "Iteration 20, loss = 0.34463674\n",
      "Iteration 21, loss = 0.34317202\n",
      "Iteration 22, loss = 0.32428196\n",
      "Iteration 23, loss = 0.30767372\n",
      "Iteration 24, loss = 0.32360772\n",
      "Iteration 25, loss = 0.31472858\n",
      "Iteration 26, loss = 0.31729762\n",
      "Iteration 27, loss = 0.30017170\n",
      "Iteration 28, loss = 0.32431325\n",
      "Iteration 29, loss = 0.31151224\n",
      "Iteration 30, loss = 0.32782614\n",
      "Iteration 31, loss = 0.32251985\n",
      "Iteration 32, loss = 0.30293319\n",
      "Iteration 33, loss = 0.30679752\n",
      "Iteration 34, loss = 0.30093440\n",
      "Iteration 35, loss = 0.37189336\n",
      "Iteration 36, loss = 0.31392622\n",
      "Iteration 37, loss = 0.32460557\n",
      "Iteration 38, loss = 0.31621157\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.03413256\n",
      "Iteration 2, loss = 3.07114095\n",
      "Iteration 3, loss = 1.11477156\n",
      "Iteration 4, loss = 0.44204779\n",
      "Iteration 5, loss = 0.45532293\n",
      "Iteration 6, loss = 0.40033971\n",
      "Iteration 7, loss = 0.38091723\n",
      "Iteration 8, loss = 0.39747437\n",
      "Iteration 9, loss = 0.40465199\n",
      "Iteration 10, loss = 0.41344020\n",
      "Iteration 11, loss = 0.38110640\n",
      "Iteration 12, loss = 0.39583711\n",
      "Iteration 13, loss = 0.36775032\n",
      "Iteration 14, loss = 0.39125929\n",
      "Iteration 15, loss = 0.36414286\n",
      "Iteration 16, loss = 0.36872219\n",
      "Iteration 17, loss = 0.36541256\n",
      "Iteration 18, loss = 0.36454400\n",
      "Iteration 19, loss = 0.37291661\n",
      "Iteration 20, loss = 0.35169187\n",
      "Iteration 21, loss = 0.41795901\n",
      "Iteration 22, loss = 0.32059130\n",
      "Iteration 23, loss = 0.31682976\n",
      "Iteration 24, loss = 0.32532298\n",
      "Iteration 25, loss = 0.33116416\n",
      "Iteration 26, loss = 0.31867744\n",
      "Iteration 27, loss = 0.29765014\n",
      "Iteration 28, loss = 0.32097079\n",
      "Iteration 29, loss = 0.31699989\n",
      "Iteration 30, loss = 0.31152578\n",
      "Iteration 31, loss = 0.30375591\n",
      "Iteration 32, loss = 0.30039964\n",
      "Iteration 33, loss = 0.29378106\n",
      "Iteration 34, loss = 0.30104505\n",
      "Iteration 35, loss = 0.31430391\n",
      "Iteration 36, loss = 0.32032709\n",
      "Iteration 37, loss = 0.29475830\n",
      "Iteration 38, loss = 0.30452927\n",
      "Iteration 39, loss = 0.32654990\n",
      "Iteration 40, loss = 0.31229484\n",
      "Iteration 41, loss = 0.30300115\n",
      "Iteration 42, loss = 0.31049387\n",
      "Iteration 43, loss = 0.31248265\n",
      "Iteration 44, loss = 0.31201241\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.46538807\n",
      "Iteration 2, loss = 1.33071117\n",
      "Iteration 3, loss = 0.58969596\n",
      "Iteration 4, loss = 0.59870268\n",
      "Iteration 5, loss = 0.50863552\n",
      "Iteration 6, loss = 0.44253398\n",
      "Iteration 7, loss = 0.42469279\n",
      "Iteration 8, loss = 0.41601616\n",
      "Iteration 9, loss = 0.40065707\n",
      "Iteration 10, loss = 0.37452007\n",
      "Iteration 11, loss = 0.34852594\n",
      "Iteration 12, loss = 0.39387575\n",
      "Iteration 13, loss = 0.40470873\n",
      "Iteration 14, loss = 0.37239574\n",
      "Iteration 15, loss = 0.39015353\n",
      "Iteration 16, loss = 0.36535318\n",
      "Iteration 17, loss = 0.41607654\n",
      "Iteration 18, loss = 0.39264905\n",
      "Iteration 19, loss = 0.39533615\n",
      "Iteration 20, loss = 0.39668761\n",
      "Iteration 21, loss = 0.36462175\n",
      "Iteration 22, loss = 0.35300379\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.21753662\n",
      "Iteration 2, loss = 1.49440650\n",
      "Iteration 3, loss = 0.41123100\n",
      "Iteration 4, loss = 0.36783297\n",
      "Iteration 5, loss = 0.39434308\n",
      "Iteration 6, loss = 0.41729132\n",
      "Iteration 7, loss = 0.39953083\n",
      "Iteration 8, loss = 0.41725396\n",
      "Iteration 9, loss = 0.43025526\n",
      "Iteration 10, loss = 0.38391686\n",
      "Iteration 11, loss = 0.39682283\n",
      "Iteration 12, loss = 0.37311052\n",
      "Iteration 13, loss = 0.37005024\n",
      "Iteration 14, loss = 0.38344750\n",
      "Iteration 15, loss = 0.36540997\n",
      "Iteration 16, loss = 0.36977932\n",
      "Iteration 17, loss = 0.36844927\n",
      "Iteration 18, loss = 0.34325364\n",
      "Iteration 19, loss = 0.36366560\n",
      "Iteration 20, loss = 0.35801090\n",
      "Iteration 21, loss = 0.34999234\n",
      "Iteration 22, loss = 0.32970164\n",
      "Iteration 23, loss = 0.32073256\n",
      "Iteration 24, loss = 0.32153505\n",
      "Iteration 25, loss = 0.34255540\n",
      "Iteration 26, loss = 0.35243624\n",
      "Iteration 27, loss = 0.34835685\n",
      "Iteration 28, loss = 0.37441150\n",
      "Iteration 29, loss = 0.35425253\n",
      "Iteration 30, loss = 0.36507331\n",
      "Iteration 31, loss = 0.36070357\n",
      "Iteration 32, loss = 0.34156110\n",
      "Iteration 33, loss = 0.36827163\n",
      "Iteration 34, loss = 0.35022545\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.19590162\n",
      "Iteration 2, loss = 4.91209489\n",
      "Iteration 3, loss = 4.25464976\n",
      "Iteration 4, loss = 4.25657706\n",
      "Iteration 5, loss = 4.30405371\n",
      "Iteration 6, loss = 3.84573388\n",
      "Iteration 7, loss = 3.93858292\n",
      "Iteration 8, loss = 4.08247743\n",
      "Iteration 9, loss = 3.83390388\n",
      "Iteration 10, loss = 3.67889351\n",
      "Iteration 11, loss = 3.82252767\n",
      "Iteration 12, loss = 3.58430224\n",
      "Iteration 13, loss = 4.08572761\n",
      "Iteration 14, loss = 4.07822708\n",
      "Iteration 15, loss = 4.10690992\n",
      "Iteration 16, loss = 3.94561221\n",
      "Iteration 17, loss = 3.86299827\n",
      "Iteration 18, loss = 3.55328299\n",
      "Iteration 19, loss = 3.70575437\n",
      "Iteration 20, loss = 3.54216546\n",
      "Iteration 21, loss = 3.75291160\n",
      "Iteration 22, loss = 3.82372573\n",
      "Iteration 23, loss = 3.45524703\n",
      "Iteration 24, loss = 3.46854275\n",
      "Iteration 25, loss = 3.62264498\n",
      "Iteration 26, loss = 3.58029215\n",
      "Iteration 27, loss = 4.22127356\n",
      "Iteration 28, loss = 3.62517990\n",
      "Iteration 29, loss = 3.76363082\n",
      "Iteration 30, loss = 3.70451077\n",
      "Iteration 31, loss = 3.85279669\n",
      "Iteration 32, loss = 4.01545379\n",
      "Iteration 33, loss = 3.74784261\n",
      "Iteration 34, loss = 3.69917742\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.26886888\n",
      "Iteration 41, loss = 0.26996396\n",
      "Iteration 42, loss = 0.27737112\n",
      "Iteration 43, loss = 0.26490474\n",
      "Iteration 44, loss = 0.27714178\n",
      "Iteration 45, loss = 0.26608518\n",
      "Iteration 46, loss = 0.31660378\n",
      "Iteration 47, loss = 0.31589898\n",
      "Iteration 48, loss = 0.29907910\n",
      "Iteration 49, loss = 0.29248184\n",
      "Iteration 50, loss = 0.27749405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.54433163\n",
      "Iteration 2, loss = 1.23369870\n",
      "Iteration 3, loss = 0.54347917\n",
      "Iteration 4, loss = 0.46718929\n",
      "Iteration 5, loss = 0.44809008\n",
      "Iteration 6, loss = 0.40219987\n",
      "Iteration 7, loss = 0.43904978\n",
      "Iteration 8, loss = 0.36968434\n",
      "Iteration 9, loss = 0.35539988\n",
      "Iteration 10, loss = 0.33723355\n",
      "Iteration 11, loss = 0.37337154\n",
      "Iteration 12, loss = 0.32009673\n",
      "Iteration 13, loss = 0.36363730\n",
      "Iteration 14, loss = 0.32885513\n",
      "Iteration 15, loss = 0.33645709\n",
      "Iteration 16, loss = 0.33316510\n",
      "Iteration 17, loss = 0.35240865\n",
      "Iteration 18, loss = 0.33077383\n",
      "Iteration 19, loss = 0.34145935\n",
      "Iteration 20, loss = 0.34463674\n",
      "Iteration 21, loss = 0.34317202\n",
      "Iteration 22, loss = 0.32428196\n",
      "Iteration 23, loss = 0.30767372\n",
      "Iteration 24, loss = 0.32360772\n",
      "Iteration 25, loss = 0.31472858\n",
      "Iteration 26, loss = 0.31729762\n",
      "Iteration 27, loss = 0.30017170\n",
      "Iteration 28, loss = 0.32431325\n",
      "Iteration 29, loss = 0.31151224\n",
      "Iteration 30, loss = 0.32782614\n",
      "Iteration 31, loss = 0.32251985\n",
      "Iteration 32, loss = 0.30293319\n",
      "Iteration 33, loss = 0.30679752\n",
      "Iteration 34, loss = 0.30093440\n",
      "Iteration 35, loss = 0.37189336\n",
      "Iteration 36, loss = 0.31392622\n",
      "Iteration 37, loss = 0.32460557\n",
      "Iteration 38, loss = 0.31621157\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.54253066\n",
      "Iteration 2, loss = 2.52914855\n",
      "Iteration 3, loss = 2.55054785\n",
      "Iteration 4, loss = 1.71393858\n",
      "Iteration 5, loss = 0.93363453\n",
      "Iteration 6, loss = 0.61990076\n",
      "Iteration 7, loss = 0.32450744\n",
      "Iteration 8, loss = 0.31591583\n",
      "Iteration 9, loss = 0.31176526\n",
      "Iteration 10, loss = 0.31547522\n",
      "Iteration 11, loss = 0.30345723\n",
      "Iteration 12, loss = 0.32107919\n",
      "Iteration 13, loss = 0.33835823\n",
      "Iteration 14, loss = 0.33754201\n",
      "Iteration 15, loss = 0.31552879\n",
      "Iteration 16, loss = 0.34924676\n",
      "Iteration 17, loss = 0.32793207\n",
      "Iteration 18, loss = 0.34031884\n",
      "Iteration 19, loss = 0.34067932\n",
      "Iteration 20, loss = 0.34898794\n",
      "Iteration 21, loss = 0.34937620\n",
      "Iteration 22, loss = 0.35436421\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.37398098\n",
      "Iteration 2, loss = 4.90374058\n",
      "Iteration 3, loss = 4.58748924\n",
      "Iteration 4, loss = 4.01410781\n",
      "Iteration 5, loss = 4.08404351\n",
      "Iteration 6, loss = 4.20448858\n",
      "Iteration 7, loss = 4.02262764\n",
      "Iteration 8, loss = 3.83087678\n",
      "Iteration 9, loss = 4.26451066\n",
      "Iteration 10, loss = 3.72117783\n",
      "Iteration 11, loss = 3.65182835\n",
      "Iteration 12, loss = 3.44922449\n",
      "Iteration 13, loss = 3.74572569\n",
      "Iteration 14, loss = 3.71573199\n",
      "Iteration 15, loss = 3.66143587\n",
      "Iteration 16, loss = 3.96159290\n",
      "Iteration 17, loss = 3.62269570\n",
      "Iteration 18, loss = 4.03616277\n",
      "Iteration 19, loss = 3.92312012\n",
      "Iteration 20, loss = 3.73454363\n",
      "Iteration 21, loss = 3.45361179\n",
      "Iteration 22, loss = 3.76848446\n",
      "Iteration 23, loss = 3.35642469\n",
      "Iteration 24, loss = 3.45716218\n",
      "Iteration 25, loss = 3.38619034\n",
      "Iteration 26, loss = 3.16788810\n",
      "Iteration 27, loss = 3.25595869\n",
      "Iteration 28, loss = 2.99337294\n",
      "Iteration 29, loss = 3.67742217\n",
      "Iteration 30, loss = 3.28091744\n",
      "Iteration 31, loss = 3.46333482\n",
      "Iteration 32, loss = 3.05641889\n",
      "Iteration 33, loss = 3.32343219\n",
      "Iteration 34, loss = 3.16346010\n",
      "Iteration 35, loss = 2.89451583\n",
      "Iteration 36, loss = 2.84003087\n",
      "Iteration 37, loss = 3.11601688\n",
      "Iteration 38, loss = 3.17501195\n",
      "Iteration 39, loss = 3.32709986\n",
      "Iteration 40, loss = 3.40325336\n",
      "Iteration 41, loss = 3.43052738\n",
      "Iteration 42, loss = 3.44373942\n",
      "Iteration 43, loss = 3.53224768\n",
      "Iteration 44, loss = 3.52620584\n",
      "Iteration 45, loss = 3.20580492\n",
      "Iteration 46, loss = 3.65488211\n",
      "Iteration 47, loss = 3.39938172\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.68665274\n",
      "Iteration 2, loss = 3.89114516\n",
      "Iteration 3, loss = 2.22759245\n",
      "Iteration 4, loss = 0.77720727\n",
      "Iteration 5, loss = 0.54876126\n",
      "Iteration 6, loss = 0.52227800\n",
      "Iteration 7, loss = 0.43890457\n",
      "Iteration 8, loss = 0.43432380\n",
      "Iteration 9, loss = 0.42406803\n",
      "Iteration 10, loss = 0.41877661\n",
      "Iteration 11, loss = 0.43140491\n",
      "Iteration 12, loss = 0.42316950\n",
      "Iteration 13, loss = 0.40203399\n",
      "Iteration 14, loss = 0.40307887\n",
      "Iteration 15, loss = 0.43665594\n",
      "Iteration 16, loss = 0.40266738\n",
      "Iteration 17, loss = 0.38802340\n",
      "Iteration 18, loss = 0.40108343\n",
      "Iteration 19, loss = 0.38483767\n",
      "Iteration 20, loss = 0.33089557\n",
      "Iteration 21, loss = 0.38923103\n",
      "Iteration 22, loss = 0.36196799\n",
      "Iteration 23, loss = 0.34387786\n",
      "Iteration 24, loss = 0.34435720\n",
      "Iteration 25, loss = 0.35019657\n",
      "Iteration 26, loss = 0.35432173\n",
      "Iteration 27, loss = 0.37415821\n",
      "Iteration 28, loss = 0.37493382\n",
      "Iteration 29, loss = 0.38185301\n",
      "Iteration 30, loss = 0.41033261\n",
      "Iteration 31, loss = 0.39827739\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.13141820\n",
      "Iteration 2, loss = 4.65737259\n",
      "Iteration 3, loss = 4.15162905\n",
      "Iteration 4, loss = 5.00824936\n",
      "Iteration 5, loss = 4.59070631\n",
      "Iteration 6, loss = 4.10651095\n",
      "Iteration 7, loss = 4.36415005\n",
      "Iteration 8, loss = 3.94395569\n",
      "Iteration 9, loss = 3.86260786\n",
      "Iteration 10, loss = 3.55305842\n",
      "Iteration 11, loss = 4.19611880\n",
      "Iteration 12, loss = 4.01240149\n",
      "Iteration 13, loss = 3.45597005\n",
      "Iteration 14, loss = 3.80199982\n",
      "Iteration 15, loss = 3.68434207\n",
      "Iteration 16, loss = 4.07876279\n",
      "Iteration 17, loss = 3.86092840\n",
      "Iteration 18, loss = 3.87733321\n",
      "Iteration 19, loss = 3.55352376\n",
      "Iteration 20, loss = 3.96036538\n",
      "Iteration 21, loss = 3.70642126\n",
      "Iteration 22, loss = 3.65447851\n",
      "Iteration 23, loss = 3.27971356\n",
      "Iteration 24, loss = 3.11963025\n",
      "Iteration 25, loss = 3.23969687\n",
      "Iteration 26, loss = 3.32286648\n",
      "Iteration 27, loss = 3.27228271\n",
      "Iteration 28, loss = 3.66950440\n",
      "Iteration 29, loss = 3.17762340\n",
      "Iteration 30, loss = 3.05375218\n",
      "Iteration 31, loss = 2.92791251\n",
      "Iteration 32, loss = 3.44660393\n",
      "Iteration 33, loss = 3.15442891\n",
      "Iteration 34, loss = 3.28159611\n",
      "Iteration 35, loss = 3.51574597\n",
      "Iteration 36, loss = 3.27738230\n",
      "Iteration 37, loss = 3.14180988\n",
      "Iteration 38, loss = 3.01954100\n",
      "Iteration 39, loss = 2.71593628\n",
      "Iteration 40, loss = 2.99600454\n",
      "Iteration 41, loss = 2.97198114\n",
      "Iteration 42, loss = 3.21064866\n",
      "Iteration 43, loss = 3.11409320\n",
      "Iteration 44, loss = 3.20966350\n",
      "Iteration 45, loss = 2.89522614\n",
      "Iteration 46, loss = 3.17020376\n",
      "Iteration 47, loss = 3.09535598\n",
      "Iteration 48, loss = 3.06522461\n",
      "Iteration 49, loss = 3.12517354\n",
      "Iteration 50, loss = 2.85844845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 3.01954100\n",
      "Iteration 39, loss = 2.71593628\n",
      "Iteration 40, loss = 2.99600454\n",
      "Iteration 41, loss = 2.97198114\n",
      "Iteration 42, loss = 3.21064866\n",
      "Iteration 43, loss = 3.11409320\n",
      "Iteration 44, loss = 3.20966350\n",
      "Iteration 45, loss = 2.89522614\n",
      "Iteration 46, loss = 3.17020376\n",
      "Iteration 47, loss = 3.09535598\n",
      "Iteration 48, loss = 3.06522461\n",
      "Iteration 49, loss = 3.12517354\n",
      "Iteration 50, loss = 2.85844845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.26128764\n",
      "Iteration 2, loss = 1.03597093\n",
      "Iteration 3, loss = 0.63149425\n",
      "Iteration 4, loss = 0.59722453\n",
      "Iteration 5, loss = 0.59435127\n",
      "Iteration 6, loss = 0.58778426\n",
      "Iteration 7, loss = 0.58463501\n",
      "Iteration 8, loss = 0.56724102\n",
      "Iteration 9, loss = 0.57630242\n",
      "Iteration 10, loss = 0.57626226\n",
      "Iteration 11, loss = 0.55846420\n",
      "Iteration 12, loss = 0.54905214\n",
      "Iteration 13, loss = 0.56347716\n",
      "Iteration 14, loss = 0.56043639\n",
      "Iteration 15, loss = 0.46378976\n",
      "Iteration 16, loss = 0.41582934\n",
      "Iteration 17, loss = 0.41831270\n",
      "Iteration 18, loss = 0.39833979\n",
      "Iteration 19, loss = 0.39844558\n",
      "Iteration 20, loss = 0.42383444\n",
      "Iteration 21, loss = 0.44510972\n",
      "Iteration 22, loss = 0.50244599\n",
      "Iteration 23, loss = 0.38534115\n",
      "Iteration 24, loss = 0.40304438\n",
      "Iteration 25, loss = 0.39241295\n",
      "Iteration 26, loss = 0.46135475\n",
      "Iteration 27, loss = 0.39051306\n",
      "Iteration 28, loss = 0.37279027\n",
      "Iteration 29, loss = 0.37221762\n",
      "Iteration 30, loss = 0.36212196\n",
      "Iteration 31, loss = 0.37824355\n",
      "Iteration 32, loss = 0.38944722\n",
      "Iteration 33, loss = 0.36154203\n",
      "Iteration 34, loss = 0.36825157\n",
      "Iteration 35, loss = 0.37821589\n",
      "Iteration 36, loss = 0.35497234\n",
      "Iteration 37, loss = 0.35888621\n",
      "Iteration 38, loss = 0.39704523\n",
      "Iteration 39, loss = 0.34288308\n",
      "Iteration 40, loss = 0.47014984\n",
      "Iteration 41, loss = 0.54898191\n",
      "Iteration 42, loss = 0.54605685\n",
      "Iteration 43, loss = 0.42687051\n",
      "Iteration 44, loss = 0.36358539\n",
      "Iteration 45, loss = 0.36888797\n",
      "Iteration 46, loss = 0.35386674\n",
      "Iteration 47, loss = 0.40136121\n",
      "Iteration 48, loss = 0.38024464\n",
      "Iteration 49, loss = 0.34901344\n",
      "Iteration 50, loss = 0.35513469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.07546820\n",
      "Iteration 2, loss = 2.97577840\n",
      "Iteration 3, loss = 0.48796539\n",
      "Iteration 4, loss = 0.38080380\n",
      "Iteration 5, loss = 0.41527376\n",
      "Iteration 6, loss = 0.41150811\n",
      "Iteration 7, loss = 0.42337959\n",
      "Iteration 8, loss = 0.39849934\n",
      "Iteration 9, loss = 0.39455736\n",
      "Iteration 10, loss = 0.38813883\n",
      "Iteration 11, loss = 0.41830484\n",
      "Iteration 12, loss = 0.39991301\n",
      "Iteration 13, loss = 0.41016003\n",
      "Iteration 14, loss = 0.40459811\n",
      "Iteration 15, loss = 0.39901760\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.68297390\n",
      "Iteration 2, loss = 5.15130256\n",
      "Iteration 3, loss = 5.10833344\n",
      "Iteration 4, loss = 4.97184428\n",
      "Iteration 5, loss = 4.93906049\n",
      "Iteration 6, loss = 4.40668518\n",
      "Iteration 7, loss = 4.54476335\n",
      "Iteration 8, loss = 4.23964141\n",
      "Iteration 9, loss = 4.37008180\n",
      "Iteration 10, loss = 4.04693743\n",
      "Iteration 11, loss = 4.10396258\n",
      "Iteration 12, loss = 4.17448935\n",
      "Iteration 13, loss = 4.42297489\n",
      "Iteration 14, loss = 4.27259664\n",
      "Iteration 15, loss = 3.85125916\n",
      "Iteration 16, loss = 4.21845526\n",
      "Iteration 17, loss = 4.35359881\n",
      "Iteration 18, loss = 3.87701439\n",
      "Iteration 19, loss = 4.09817355\n",
      "Iteration 20, loss = 4.10854906\n",
      "Iteration 21, loss = 3.84892243\n",
      "Iteration 22, loss = 4.08661211\n",
      "Iteration 23, loss = 3.70055204\n",
      "Iteration 24, loss = 3.77054182\n",
      "Iteration 25, loss = 3.98808561\n",
      "Iteration 26, loss = 3.83297536\n",
      "Iteration 27, loss = 4.34264671\n",
      "Iteration 28, loss = 4.37412022\n",
      "Iteration 29, loss = 3.63502627\n",
      "Iteration 30, loss = 3.92959913\n",
      "Iteration 31, loss = 3.75533515\n",
      "Iteration 32, loss = 3.85262933\n",
      "Iteration 33, loss = 4.12682192\n",
      "Iteration 34, loss = 3.70698933\n",
      "Iteration 35, loss = 4.18174241\n",
      "Iteration 36, loss = 3.81858992\n",
      "Iteration 37, loss = 3.68122244\n",
      "Iteration 38, loss = 3.73646916\n",
      "Iteration 39, loss = 3.45258268\n",
      "Iteration 40, loss = 3.38354169\n",
      "Iteration 41, loss = 3.49280564\n",
      "Iteration 42, loss = 3.50978912\n",
      "Iteration 43, loss = 3.41298202\n",
      "Iteration 44, loss = 3.46255983\n",
      "Iteration 45, loss = 3.35545029\n",
      "Iteration 46, loss = 3.78929359\n",
      "Iteration 47, loss = 3.55956303\n",
      "Iteration 48, loss = 3.83409742\n",
      "Iteration 49, loss = 3.43049060\n",
      "Iteration 50, loss = 3.25336280\n",
      "Iteration 51, loss = 3.56387704\n",
      "Iteration 52, loss = 3.78606694\n",
      "Iteration 53, loss = 3.79456307\n",
      "Iteration 54, loss = 3.72211642\n",
      "Iteration 55, loss = 3.50501157\n",
      "Iteration 56, loss = 3.62327830\n",
      "Iteration 57, loss = 3.60225254\n",
      "Iteration 58, loss = 3.24288701\n",
      "Iteration 59, loss = 3.44954623\n",
      "Iteration 60, loss = 3.55131845\n",
      "Iteration 61, loss = 3.18505935\n",
      "Iteration 62, loss = 3.84583495\n",
      "Iteration 63, loss = 3.44261337\n",
      "Iteration 64, loss = 3.58150200\n",
      "Iteration 65, loss = 3.59571792\n",
      "Iteration 66, loss = 3.43534924\n",
      "Iteration 67, loss = 3.26089268\n",
      "Iteration 68, loss = 3.22437494\n",
      "Iteration 69, loss = 3.57361612\n",
      "Iteration 70, loss = 3.26984703\n",
      "Iteration 71, loss = 3.05295256\n",
      "Iteration 72, loss = 3.44723179\n",
      "Iteration 73, loss = 3.16677372\n",
      "Iteration 74, loss = 3.28513682\n",
      "Iteration 75, loss = 3.16765232\n",
      "Iteration 76, loss = 3.33588147\n",
      "Iteration 77, loss = 3.29145564\n",
      "Iteration 78, loss = 3.32588649\n",
      "Iteration 79, loss = 3.31303781\n",
      "Iteration 80, loss = 3.22679447\n",
      "Iteration 81, loss = 3.42113092\n",
      "Iteration 82, loss = 3.25587418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.10930715\n",
      "Iteration 2, loss = 4.36821206\n",
      "Iteration 3, loss = 4.39131603\n",
      "Iteration 4, loss = 3.79918226\n",
      "Iteration 5, loss = 3.66638127\n",
      "Iteration 6, loss = 4.39996863\n",
      "Iteration 7, loss = 4.35359181\n",
      "Iteration 8, loss = 4.01722371\n",
      "Iteration 9, loss = 3.63590415\n",
      "Iteration 10, loss = 3.91636384\n",
      "Iteration 11, loss = 3.76894974\n",
      "Iteration 12, loss = 3.67037918\n",
      "Iteration 13, loss = 3.66821195\n",
      "Iteration 14, loss = 3.83913512\n",
      "Iteration 15, loss = 4.01906860\n",
      "Iteration 16, loss = 3.82959755\n",
      "Iteration 17, loss = 3.36431140\n",
      "Iteration 18, loss = 3.81678926\n",
      "Iteration 19, loss = 3.92410408\n",
      "Iteration 20, loss = 3.73265039\n",
      "Iteration 21, loss = 3.71394174\n",
      "Iteration 22, loss = 3.59675965\n",
      "Iteration 23, loss = 3.34993043\n",
      "Iteration 24, loss = 3.29169209\n",
      "Iteration 25, loss = 3.24860656\n",
      "Iteration 26, loss = 3.08749401\n",
      "Iteration 27, loss = 3.67701335\n",
      "Iteration 28, loss = 3.80522275\n",
      "Iteration 29, loss = 3.37692404\n",
      "Iteration 30, loss = 3.36253970\n",
      "Iteration 31, loss = 2.97187341\n",
      "Iteration 32, loss = 3.14354539\n",
      "Iteration 33, loss = 3.08318277\n",
      "Iteration 34, loss = 2.98646216\n",
      "Iteration 35, loss = 2.74333783\n",
      "Iteration 36, loss = 2.70702681\n",
      "Iteration 37, loss = 2.91573641\n",
      "Iteration 38, loss = 2.67161120\n",
      "Iteration 39, loss = 2.91060821\n",
      "Iteration 40, loss = 2.69312687\n",
      "Iteration 41, loss = 2.99262619\n",
      "Iteration 42, loss = 2.78418267\n",
      "Iteration 43, loss = 2.83550022\n",
      "Iteration 44, loss = 3.19752975\n",
      "Iteration 45, loss = 3.02783753\n",
      "Iteration 46, loss = 2.71906293\n",
      "Iteration 47, loss = 2.52888708\n",
      "Iteration 48, loss = 2.95024350\n",
      "Iteration 49, loss = 2.71608652\n",
      "Iteration 50, loss = 2.69660208\n",
      "Iteration 51, loss = 3.22901808\n",
      "Iteration 52, loss = 3.08428801\n",
      "Iteration 53, loss = 2.72817903\n",
      "Iteration 54, loss = 2.82967482\n",
      "Iteration 55, loss = 2.66706696\n",
      "Iteration 56, loss = 2.63063200\n",
      "Iteration 57, loss = 2.49914447\n",
      "Iteration 58, loss = 2.61158148\n",
      "Iteration 59, loss = 2.43665842\n",
      "Iteration 60, loss = 2.33779531\n",
      "Iteration 61, loss = 2.46311873\n",
      "Iteration 62, loss = 2.34954536\n",
      "Iteration 63, loss = 2.18057208\n",
      "Iteration 64, loss = 2.05281335\n",
      "Iteration 65, loss = 2.34250488\n",
      "Iteration 66, loss = 2.54925422\n",
      "Iteration 67, loss = 3.05442539\n",
      "Iteration 68, loss = 2.59826320\n",
      "Iteration 69, loss = 2.41259319\n",
      "Iteration 70, loss = 2.55070102\n",
      "Iteration 71, loss = 2.50724283\n",
      "Iteration 72, loss = 2.32471077\n",
      "Iteration 73, loss = 2.07791300\n",
      "Iteration 74, loss = 2.49118496\n",
      "Iteration 75, loss = 2.08821253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 3.34993043\n",
      "Iteration 24, loss = 3.29169209\n",
      "Iteration 25, loss = 3.24860656\n",
      "Iteration 26, loss = 3.08749401\n",
      "Iteration 27, loss = 3.67701335\n",
      "Iteration 28, loss = 3.80522275\n",
      "Iteration 29, loss = 3.37692404\n",
      "Iteration 30, loss = 3.36253970\n",
      "Iteration 31, loss = 2.97187341\n",
      "Iteration 32, loss = 3.14354539\n",
      "Iteration 33, loss = 3.08318277\n",
      "Iteration 34, loss = 2.98646216\n",
      "Iteration 35, loss = 2.74333783\n",
      "Iteration 36, loss = 2.70702681\n",
      "Iteration 37, loss = 2.91573641\n",
      "Iteration 38, loss = 2.67161120\n",
      "Iteration 39, loss = 2.91060821\n",
      "Iteration 40, loss = 2.69312687\n",
      "Iteration 41, loss = 2.99262619\n",
      "Iteration 42, loss = 2.78418267\n",
      "Iteration 43, loss = 2.83550022\n",
      "Iteration 44, loss = 3.19752975\n",
      "Iteration 45, loss = 3.02783753\n",
      "Iteration 46, loss = 2.71906293\n",
      "Iteration 47, loss = 2.52888708\n",
      "Iteration 48, loss = 2.95024350\n",
      "Iteration 49, loss = 2.71608652\n",
      "Iteration 50, loss = 2.69660208\n",
      "Iteration 51, loss = 3.22901808\n",
      "Iteration 52, loss = 3.08428801\n",
      "Iteration 53, loss = 2.72817903\n",
      "Iteration 54, loss = 2.82967482\n",
      "Iteration 55, loss = 2.66706696\n",
      "Iteration 56, loss = 2.63063200\n",
      "Iteration 57, loss = 2.49914447\n",
      "Iteration 58, loss = 2.61158148\n",
      "Iteration 59, loss = 2.43665842\n",
      "Iteration 60, loss = 2.33779531\n",
      "Iteration 61, loss = 2.46311873\n",
      "Iteration 62, loss = 2.34954536\n",
      "Iteration 63, loss = 2.18057208\n",
      "Iteration 64, loss = 2.05281335\n",
      "Iteration 65, loss = 2.34250488\n",
      "Iteration 66, loss = 2.54925422\n",
      "Iteration 67, loss = 3.05442539\n",
      "Iteration 68, loss = 2.59826320\n",
      "Iteration 69, loss = 2.41259319\n",
      "Iteration 70, loss = 2.55070102\n",
      "Iteration 71, loss = 2.50724283\n",
      "Iteration 72, loss = 2.32471077\n",
      "Iteration 73, loss = 2.07791300\n",
      "Iteration 74, loss = 2.49118496\n",
      "Iteration 75, loss = 2.08821253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.56605442\n",
      "Iteration 2, loss = 1.07074700\n",
      "Iteration 3, loss = 0.46292353\n",
      "Iteration 4, loss = 0.40393295\n",
      "Iteration 5, loss = 0.37326466\n",
      "Iteration 6, loss = 0.33711878\n",
      "Iteration 7, loss = 0.33769246\n",
      "Iteration 8, loss = 0.34503669\n",
      "Iteration 9, loss = 0.33290654\n",
      "Iteration 10, loss = 0.32389803\n",
      "Iteration 11, loss = 0.32224167\n",
      "Iteration 12, loss = 0.30627045\n",
      "Iteration 13, loss = 0.30285902\n",
      "Iteration 14, loss = 0.30704280\n",
      "Iteration 15, loss = 0.30248151\n",
      "Iteration 16, loss = 0.29078148\n",
      "Iteration 17, loss = 0.30065069\n",
      "Iteration 18, loss = 0.30811866\n",
      "Iteration 19, loss = 0.31075496\n",
      "Iteration 20, loss = 0.29370550\n",
      "Iteration 21, loss = 0.28932057\n",
      "Iteration 22, loss = 0.31667767\n",
      "Iteration 23, loss = 0.28917095\n",
      "Iteration 24, loss = 0.29381728\n",
      "Iteration 25, loss = 0.30530100\n",
      "Iteration 26, loss = 0.30034973\n",
      "Iteration 27, loss = 0.27716184\n",
      "Iteration 28, loss = 0.28090427\n",
      "Iteration 29, loss = 0.27790793\n",
      "Iteration 30, loss = 0.27939208\n",
      "Iteration 31, loss = 0.27257999\n",
      "Iteration 32, loss = 0.28995761\n",
      "Iteration 33, loss = 0.26886044\n",
      "Iteration 34, loss = 0.27562950\n",
      "Iteration 35, loss = 0.26686923\n",
      "Iteration 36, loss = 0.28277444\n",
      "Iteration 37, loss = 0.27054953\n",
      "Iteration 38, loss = 0.27617885\n",
      "Iteration 39, loss = 0.26995424\n",
      "Iteration 40, loss = 0.26825402\n",
      "Iteration 41, loss = 0.26710678\n",
      "Iteration 42, loss = 0.26911195\n",
      "Iteration 43, loss = 0.27389860\n",
      "Iteration 44, loss = 0.26477016\n",
      "Iteration 45, loss = 0.27154487\n",
      "Iteration 46, loss = 0.27492326\n",
      "Iteration 47, loss = 0.28112396\n",
      "Iteration 48, loss = 0.26579300\n",
      "Iteration 49, loss = 0.27537812\n",
      "Iteration 50, loss = 0.26561454\n",
      "Iteration 51, loss = 0.26347717\n",
      "Iteration 52, loss = 0.26167398\n",
      "Iteration 53, loss = 0.25746101\n",
      "Iteration 54, loss = 0.26069094\n",
      "Iteration 55, loss = 0.25812354\n",
      "Iteration 56, loss = 0.25910546\n",
      "Iteration 57, loss = 0.25783787\n",
      "Iteration 58, loss = 0.25277393\n",
      "Iteration 59, loss = 0.25508134\n",
      "Iteration 60, loss = 0.26071897\n",
      "Iteration 61, loss = 0.25118706\n",
      "Iteration 62, loss = 0.25498669\n",
      "Iteration 63, loss = 0.25423486\n",
      "Iteration 64, loss = 0.25280313\n",
      "Iteration 65, loss = 0.25105930\n",
      "Iteration 66, loss = 0.24877720\n",
      "Iteration 67, loss = 0.24778428\n",
      "Iteration 68, loss = 0.25212594\n",
      "Iteration 69, loss = 0.24936085\n",
      "Iteration 70, loss = 0.24716142\n",
      "Iteration 71, loss = 0.24877364\n",
      "Iteration 72, loss = 0.24600522\n",
      "Iteration 73, loss = 0.24882282\n",
      "Iteration 74, loss = 0.24483268\n",
      "Iteration 75, loss = 0.25658905\n",
      "\n",
      "Iteration 6, loss = 0.29103140\n",
      "Iteration 7, loss = 0.28946634\n",
      "Iteration 8, loss = 0.28631575\n",
      "Iteration 9, loss = 0.28216691\n",
      "Iteration 10, loss = 0.28566592\n",
      "Iteration 11, loss = 0.29092133\n",
      "Iteration 12, loss = 0.28104918\n",
      "Iteration 13, loss = 0.29106475\n",
      "Iteration 14, loss = 0.29157574\n",
      "Iteration 15, loss = 0.28143914\n",
      "Iteration 16, loss = 0.28289649\n",
      "Iteration 17, loss = 0.28134580\n",
      "Iteration 18, loss = 0.28604292\n",
      "Iteration 19, loss = 0.27760337\n",
      "Iteration 20, loss = 0.27864260\n",
      "Iteration 21, loss = 0.29139117\n",
      "Iteration 22, loss = 0.28209263\n",
      "Iteration 23, loss = 0.28480288\n",
      "Iteration 24, loss = 0.27685347\n",
      "Iteration 25, loss = 0.28133348\n",
      "Iteration 26, loss = 0.27697744\n",
      "Iteration 27, loss = 0.28828846\n",
      "Iteration 28, loss = 0.27967856\n",
      "Iteration 29, loss = 0.28461807\n",
      "Iteration 30, loss = 0.28111258\n",
      "Iteration 31, loss = 0.27982044\n",
      "Iteration 32, loss = 0.28103362\n",
      "Iteration 33, loss = 0.28396862\n",
      "Iteration 34, loss = 0.29236654\n",
      "Iteration 35, loss = 0.28160088\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.49594265\n",
      "Iteration 2, loss = 1.22921284\n",
      "Iteration 3, loss = 0.47256405\n",
      "Iteration 4, loss = 0.45062218\n",
      "Iteration 5, loss = 0.51261721\n",
      "Iteration 6, loss = 0.44927954\n",
      "Iteration 7, loss = 0.46803254\n",
      "Iteration 8, loss = 0.40689024\n",
      "Iteration 9, loss = 0.36773671\n",
      "Iteration 10, loss = 0.35649508\n",
      "Iteration 11, loss = 0.37534321\n",
      "Iteration 12, loss = 0.33554524\n",
      "Iteration 13, loss = 0.33289111\n",
      "Iteration 14, loss = 0.33867661\n",
      "Iteration 15, loss = 0.35318457\n",
      "Iteration 16, loss = 0.32943565\n",
      "Iteration 17, loss = 0.31917649\n",
      "Iteration 18, loss = 0.30649750\n",
      "Iteration 19, loss = 0.36253560\n",
      "Iteration 20, loss = 0.35142757\n",
      "Iteration 21, loss = 0.31832238\n",
      "Iteration 22, loss = 0.32347994\n",
      "Iteration 23, loss = 0.32871071\n",
      "Iteration 24, loss = 0.31214576\n",
      "Iteration 25, loss = 0.29464860\n",
      "Iteration 26, loss = 0.31809194\n",
      "Iteration 27, loss = 0.30749369\n",
      "Iteration 28, loss = 0.32543451\n",
      "Iteration 29, loss = 0.34444654\n",
      "Iteration 30, loss = 0.31269871\n",
      "Iteration 31, loss = 0.33041913\n",
      "Iteration 32, loss = 0.30674637\n",
      "Iteration 33, loss = 0.34182164\n",
      "Iteration 34, loss = 0.30604591\n",
      "Iteration 35, loss = 0.28594169\n",
      "Iteration 36, loss = 0.30086925\n",
      "Iteration 37, loss = 0.30667731\n",
      "Iteration 38, loss = 0.30616852\n",
      "Iteration 39, loss = 0.30666560\n",
      "Iteration 40, loss = 0.30159996\n",
      "Iteration 41, loss = 0.30412169\n",
      "Iteration 42, loss = 0.28408027\n",
      "Iteration 43, loss = 0.29041946\n",
      "Iteration 44, loss = 0.29737091\n",
      "Iteration 45, loss = 0.29042439\n",
      "Iteration 46, loss = 0.28349025\n",
      "Iteration 47, loss = 0.27507149\n",
      "Iteration 48, loss = 0.28155430\n",
      "Iteration 49, loss = 0.27600521\n",
      "Iteration 50, loss = 0.26775796\n",
      "Iteration 51, loss = 0.26161241\n",
      "Iteration 52, loss = 0.28481188\n",
      "Iteration 53, loss = 0.29014417\n",
      "Iteration 54, loss = 0.27205913\n",
      "Iteration 55, loss = 0.26466583\n",
      "Iteration 56, loss = 0.25918039\n",
      "Iteration 57, loss = 0.26151096\n",
      "Iteration 58, loss = 0.26228186\n",
      "Iteration 59, loss = 0.26331525\n",
      "Iteration 60, loss = 0.25875009\n",
      "Iteration 61, loss = 0.25641850\n",
      "Iteration 62, loss = 0.26125741\n",
      "Iteration 63, loss = 0.26637000\n",
      "Iteration 64, loss = 0.26322905\n",
      "Iteration 65, loss = 0.25856132\n",
      "Iteration 66, loss = 0.25992137\n",
      "Iteration 67, loss = 0.26505743\n",
      "Iteration 68, loss = 0.25805234\n",
      "Iteration 69, loss = 0.26054176\n",
      "Iteration 70, loss = 0.25538632\n",
      "Iteration 71, loss = 0.26537055\n",
      "Iteration 72, loss = 0.25992874\n",
      "Iteration 73, loss = 0.26612860\n",
      "Iteration 74, loss = 0.25960155\n",
      "Iteration 75, loss = 0.25761366\n",
      "Iteration 76, loss = 0.25550727\n",
      "Iteration 77, loss = 0.26007770\n",
      "Iteration 78, loss = 0.26051240\n",
      "Iteration 79, loss = 0.25833742\n",
      "Iteration 80, loss = 0.26243087\n",
      "Iteration 81, loss = 0.25738038\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.68297390\n",
      "Iteration 2, loss = 5.15130256\n",
      "Iteration 3, loss = 5.10833344\n",
      "Iteration 4, loss = 4.97184428\n",
      "Iteration 5, loss = 4.93906049\n",
      "Iteration 6, loss = 4.40668518\n",
      "Iteration 7, loss = 4.54476335\n",
      "Iteration 8, loss = 4.23964141\n",
      "Iteration 9, loss = 4.37008180\n",
      "Iteration 10, loss = 4.04693743\n",
      "Iteration 11, loss = 4.10396258\n",
      "Iteration 12, loss = 4.17448935\n",
      "Iteration 13, loss = 4.42297489\n",
      "Iteration 14, loss = 4.27259664\n",
      "Iteration 15, loss = 3.85125916\n",
      "Iteration 16, loss = 4.21845526\n",
      "Iteration 17, loss = 4.35359881\n",
      "Iteration 18, loss = 3.87701439\n",
      "Iteration 19, loss = 4.09817355\n",
      "Iteration 20, loss = 4.10854906\n",
      "Iteration 21, loss = 3.84892243\n",
      "Iteration 22, loss = 4.08661211\n",
      "Iteration 23, loss = 3.70055204\n",
      "Iteration 24, loss = 3.77054182\n",
      "Iteration 25, loss = 3.98808561\n",
      "Iteration 26, loss = 3.83297536\n",
      "Iteration 27, loss = 4.34264671\n",
      "Iteration 28, loss = 4.37412022\n",
      "Iteration 29, loss = 3.63502627\n",
      "Iteration 30, loss = 3.92959913\n",
      "Iteration 31, loss = 3.75533515\n",
      "Iteration 32, loss = 3.85262933\n",
      "Iteration 33, loss = 4.12682192\n",
      "Iteration 34, loss = 3.70698933\n",
      "Iteration 35, loss = 4.18174241\n",
      "Iteration 36, loss = 3.81858992\n",
      "Iteration 37, loss = 3.68122244\n",
      "Iteration 38, loss = 3.73646916\n",
      "Iteration 39, loss = 3.45258268\n",
      "Iteration 40, loss = 3.38354169\n",
      "Iteration 41, loss = 3.49280564\n",
      "Iteration 42, loss = 3.50978912\n",
      "Iteration 43, loss = 3.41298202\n",
      "Iteration 44, loss = 3.46255983\n",
      "Iteration 45, loss = 3.35545029\n",
      "Iteration 46, loss = 3.78929359\n",
      "Iteration 47, loss = 3.55956303\n",
      "Iteration 48, loss = 3.83409742\n",
      "Iteration 49, loss = 3.43049060\n",
      "Iteration 50, loss = 3.25336280\n",
      "Iteration 51, loss = 3.56387704\n",
      "Iteration 52, loss = 3.78606694\n",
      "Iteration 53, loss = 3.79456307\n",
      "Iteration 54, loss = 3.72211642\n",
      "Iteration 55, loss = 3.50501157\n",
      "Iteration 56, loss = 3.62327830\n",
      "Iteration 57, loss = 3.60225254\n",
      "Iteration 58, loss = 3.24288701\n",
      "Iteration 59, loss = 3.44954623\n",
      "Iteration 60, loss = 3.55131845\n",
      "Iteration 61, loss = 3.18505935\n",
      "Iteration 62, loss = 3.84583495\n",
      "Iteration 63, loss = 3.44261337\n",
      "Iteration 64, loss = 3.58150200\n",
      "Iteration 65, loss = 3.59571792\n",
      "Iteration 66, loss = 3.43534924\n",
      "Iteration 67, loss = 3.26089268\n",
      "Iteration 68, loss = 3.22437494\n",
      "Iteration 69, loss = 3.57361612\n",
      "Iteration 70, loss = 3.26984703\n",
      "Iteration 71, loss = 3.05295256\n",
      "Iteration 72, loss = 3.44723179\n",
      "Iteration 73, loss = 3.16677372\n",
      "Iteration 74, loss = 3.28513682\n",
      "Iteration 75, loss = 3.16765232\n",
      "Iteration 76, loss = 3.33588147\n",
      "Iteration 77, loss = 3.29145564\n",
      "Iteration 78, loss = 3.32588649\n",
      "Iteration 79, loss = 3.31303781\n",
      "Iteration 80, loss = 3.22679447\n",
      "Iteration 81, loss = 3.42113092\n",
      "Iteration 82, loss = 3.25587418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.45927659\n",
      "Iteration 2, loss = 3.41882408\n",
      "Iteration 3, loss = 2.57548731\n",
      "Iteration 4, loss = 2.14007672\n",
      "Iteration 5, loss = 1.30566053\n",
      "Iteration 6, loss = 0.45681398\n",
      "Iteration 7, loss = 0.39696358\n",
      "Iteration 8, loss = 0.36068454\n",
      "Iteration 9, loss = 0.35795099\n",
      "Iteration 10, loss = 0.38182498\n",
      "Iteration 11, loss = 0.40029549\n",
      "Iteration 12, loss = 0.39508752\n",
      "Iteration 13, loss = 0.38449860\n",
      "Iteration 14, loss = 0.37374706\n",
      "Iteration 15, loss = 0.35783196\n",
      "Iteration 16, loss = 0.37253641\n",
      "Iteration 17, loss = 0.37317077\n",
      "Iteration 18, loss = 0.36690837\n",
      "Iteration 19, loss = 0.35972483\n",
      "Iteration 20, loss = 0.35630316\n",
      "Iteration 21, loss = 0.36473011\n",
      "Iteration 22, loss = 0.36230860\n",
      "Iteration 23, loss = 0.37197341\n",
      "Iteration 24, loss = 0.36004322\n",
      "Iteration 25, loss = 0.35660408\n",
      "Iteration 26, loss = 0.38216435\n",
      "Iteration 27, loss = 0.35583512\n",
      "Iteration 28, loss = 0.36873147\n",
      "Iteration 29, loss = 0.34983127\n",
      "Iteration 30, loss = 0.33550910\n",
      "Iteration 31, loss = 0.31884274\n",
      "Iteration 32, loss = 0.31197761\n",
      "Iteration 33, loss = 0.31536338\n",
      "Iteration 34, loss = 0.32152337\n",
      "Iteration 35, loss = 0.32504618\n",
      "Iteration 36, loss = 0.31235352\n",
      "Iteration 37, loss = 0.31157748\n",
      "Iteration 38, loss = 0.32717176\n",
      "Iteration 39, loss = 0.33000933\n",
      "Iteration 40, loss = 0.31753608\n",
      "Iteration 41, loss = 0.31319301\n",
      "Iteration 42, loss = 0.32347450\n",
      "Iteration 43, loss = 0.32298486\n",
      "Iteration 44, loss = 0.33394642\n",
      "Iteration 45, loss = 0.31807914\n",
      "Iteration 46, loss = 0.32215400\n",
      "Iteration 47, loss = 0.31475379\n",
      "Iteration 48, loss = 0.31271033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76, loss = 0.25320183\n",
      "Iteration 77, loss = 0.25100673\n",
      "Iteration 78, loss = 0.24707538\n",
      "Iteration 79, loss = 0.24724871\n",
      "Iteration 80, loss = 0.24598297\n",
      "Iteration 81, loss = 0.24631310\n",
      "Iteration 82, loss = 0.25814868\n",
      "Iteration 83, loss = 0.25727503\n",
      "Iteration 84, loss = 0.24900450\n",
      "Iteration 85, loss = 0.25339651\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP Scaled Best Params found through grid search cv are:\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'constant'}\n",
      "MLP-Gridsearch metrics:\n",
      "Accuracy Score: 0.828581668087841\n",
      "F1 score: 0.799191976639672\n",
      "MLP Confusion Matrix-Gridsearch\n",
      "[[8.37722260e-01 3.63718208e-02 1.25905919e-01]\n",
      " [3.15690964e-01 1.13652906e-01 5.70656130e-01]\n",
      " [5.88176661e-02 6.64512539e-05 9.41115883e-01]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.26922311\n",
      "Iteration 33, loss = 0.27111372\n",
      "Iteration 34, loss = 0.27848335\n",
      "Iteration 35, loss = 0.26453662\n",
      "Iteration 36, loss = 0.27167554\n",
      "Iteration 37, loss = 0.27504083\n",
      "Iteration 38, loss = 0.27304957\n",
      "Iteration 39, loss = 0.26102637\n",
      "Iteration 40, loss = 0.26355637\n",
      "Iteration 41, loss = 0.25886670\n",
      "Iteration 42, loss = 0.27197636\n",
      "Iteration 43, loss = 0.26509269\n",
      "Iteration 44, loss = 0.27040923\n",
      "Iteration 45, loss = 0.26027314\n",
      "Iteration 46, loss = 0.26578179\n",
      "Iteration 47, loss = 0.26304403\n",
      "Iteration 48, loss = 0.27098182\n",
      "Iteration 49, loss = 0.25686861\n",
      "Iteration 50, loss = 0.25887245\n",
      "Iteration 51, loss = 0.26960251\n",
      "Iteration 52, loss = 0.27005669\n",
      "Iteration 53, loss = 0.25519648\n",
      "Iteration 54, loss = 0.26327628\n",
      "Iteration 55, loss = 0.25664010\n",
      "Iteration 56, loss = 0.26070010\n",
      "Iteration 57, loss = 0.25805642\n",
      "Iteration 58, loss = 0.25466238\n",
      "Iteration 59, loss = 0.27238738\n",
      "Iteration 60, loss = 0.25996688\n",
      "Iteration 61, loss = 0.26579419\n",
      "Iteration 62, loss = 0.26373347\n",
      "Iteration 63, loss = 0.25838941\n",
      "Iteration 64, loss = 0.26116575\n",
      "Iteration 65, loss = 0.26035650\n",
      "Iteration 66, loss = 0.25889587\n",
      "Iteration 67, loss = 0.25870187\n",
      "Iteration 68, loss = 0.25710754\n",
      "Iteration 69, loss = 0.25478785\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.07546820\n",
      "Iteration 2, loss = 2.97577840\n",
      "Iteration 3, loss = 0.48796539\n",
      "Iteration 4, loss = 0.38080380\n",
      "Iteration 5, loss = 0.41527376\n",
      "Iteration 6, loss = 0.41150811\n",
      "Iteration 7, loss = 0.42337959\n",
      "Iteration 8, loss = 0.39849934\n",
      "Iteration 9, loss = 0.39455736\n",
      "Iteration 10, loss = 0.38813883\n",
      "Iteration 11, loss = 0.41830484\n",
      "Iteration 12, loss = 0.39991301\n",
      "Iteration 13, loss = 0.41016003\n",
      "Iteration 14, loss = 0.40459811\n",
      "Iteration 15, loss = 0.39901760\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.37398098\n",
      "Iteration 2, loss = 4.90374058\n",
      "Iteration 3, loss = 4.58748924\n",
      "Iteration 4, loss = 4.01410781\n",
      "Iteration 5, loss = 4.08404351\n",
      "Iteration 6, loss = 4.20448858\n",
      "Iteration 7, loss = 4.02262764\n",
      "Iteration 8, loss = 3.83087678\n",
      "Iteration 9, loss = 4.26451066\n",
      "Iteration 10, loss = 3.72117783\n",
      "Iteration 11, loss = 3.65182835\n",
      "Iteration 12, loss = 3.44922449\n",
      "Iteration 13, loss = 3.74572569\n",
      "Iteration 14, loss = 3.71573199\n",
      "Iteration 15, loss = 3.66143587\n",
      "Iteration 16, loss = 3.96159290\n",
      "Iteration 17, loss = 3.62269570\n",
      "Iteration 18, loss = 4.03616277\n",
      "Iteration 19, loss = 3.92312012\n",
      "Iteration 20, loss = 3.73454363\n",
      "Iteration 21, loss = 3.45361179\n",
      "Iteration 22, loss = 3.76848446\n",
      "Iteration 23, loss = 3.35642469\n",
      "Iteration 24, loss = 3.45716218\n",
      "Iteration 25, loss = 3.38619034\n",
      "Iteration 26, loss = 3.16788810\n",
      "Iteration 27, loss = 3.25595869\n",
      "Iteration 28, loss = 2.99337294\n",
      "Iteration 29, loss = 3.67742217\n",
      "Iteration 30, loss = 3.28091744\n",
      "Iteration 31, loss = 3.46333482\n",
      "Iteration 32, loss = 3.05641889\n",
      "Iteration 33, loss = 3.32343219\n",
      "Iteration 34, loss = 3.16346010\n",
      "Iteration 35, loss = 2.89451583\n",
      "Iteration 36, loss = 2.84003087\n",
      "Iteration 37, loss = 3.11601688\n",
      "Iteration 38, loss = 3.17501195\n",
      "Iteration 39, loss = 3.32709986\n",
      "Iteration 40, loss = 3.40325336\n",
      "Iteration 41, loss = 3.43052738\n",
      "Iteration 42, loss = 3.44373942\n",
      "Iteration 43, loss = 3.53224768\n",
      "Iteration 44, loss = 3.52620584\n",
      "Iteration 45, loss = 3.20580492\n",
      "Iteration 46, loss = 3.65488211\n",
      "Iteration 47, loss = 3.39938172\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.17825869\n",
      "Iteration 2, loss = 0.62578469\n",
      "Iteration 3, loss = 0.50415622\n",
      "Iteration 4, loss = 0.49180423\n",
      "Iteration 5, loss = 0.49744428\n",
      "Iteration 6, loss = 0.41131888\n",
      "Iteration 7, loss = 0.37701041\n",
      "Iteration 8, loss = 0.39525768\n",
      "Iteration 9, loss = 0.36269040\n",
      "Iteration 10, loss = 0.37889527\n",
      "Iteration 11, loss = 0.38749152\n",
      "Iteration 12, loss = 0.38215274\n",
      "Iteration 13, loss = 0.44328627\n",
      "Iteration 14, loss = 0.36939454\n",
      "Iteration 15, loss = 0.35285661\n",
      "Iteration 16, loss = 0.35394960\n",
      "Iteration 17, loss = 0.36774084\n",
      "Iteration 18, loss = 0.40975432\n",
      "Iteration 19, loss = 0.38528926\n",
      "Iteration 20, loss = 0.40859657\n",
      "Iteration 21, loss = 0.36343969\n",
      "Iteration 22, loss = 0.36786118\n",
      "Iteration 23, loss = 0.36525911\n",
      "Iteration 24, loss = 0.37132325\n",
      "Iteration 25, loss = 0.36782948\n",
      "Iteration 26, loss = 0.34797754\n",
      "Iteration 27, loss = 0.34616891\n",
      "Iteration 28, loss = 0.36118271\n",
      "Iteration 29, loss = 0.33362927\n",
      "Iteration 30, loss = 0.31982255\n",
      "Iteration 31, loss = 0.32403086\n",
      "Iteration 32, loss = 0.32026498\n",
      "Iteration 33, loss = 0.33572858\n",
      "Iteration 34, loss = 0.32860074\n",
      "Iteration 35, loss = 0.32944647\n",
      "Iteration 36, loss = 0.32618801\n",
      "Iteration 37, loss = 0.32182157\n",
      "Iteration 38, loss = 0.34863975\n",
      "Iteration 39, loss = 0.31818420\n",
      "Iteration 40, loss = 0.31201336\n",
      "Iteration 41, loss = 0.33214920\n",
      "Iteration 42, loss = 0.32457873\n",
      "Iteration 43, loss = 0.32129459\n",
      "Iteration 44, loss = 0.31903197\n",
      "Iteration 45, loss = 0.33076629\n",
      "Iteration 46, loss = 0.31008246\n",
      "Iteration 47, loss = 0.31113027\n",
      "Iteration 48, loss = 0.30891656\n",
      "Iteration 49, loss = 0.29685750\n",
      "Iteration 50, loss = 0.30426244\n",
      "Iteration 51, loss = 0.31767059\n",
      "Iteration 52, loss = 0.30287546\n",
      "Iteration 53, loss = 0.30247036\n",
      "Iteration 54, loss = 0.29808782\n",
      "Iteration 55, loss = 0.31894005\n",
      "Iteration 56, loss = 0.29961985\n",
      "Iteration 57, loss = 0.29422164\n",
      "Iteration 58, loss = 0.27938241\n",
      "Iteration 59, loss = 0.30487597\n",
      "Iteration 60, loss = 0.30500330\n",
      "Iteration 61, loss = 0.27341541\n",
      "Iteration 62, loss = 0.27191543\n",
      "Iteration 63, loss = 0.27543449\n",
      "Iteration 64, loss = 0.34630695\n",
      "Iteration 65, loss = 0.30044369\n",
      "Iteration 66, loss = 0.27695412\n",
      "Iteration 67, loss = 0.27007211\n",
      "Iteration 68, loss = 0.28086233\n",
      "Iteration 69, loss = 0.26821375\n",
      "Iteration 70, loss = 0.26104198\n",
      "Iteration 71, loss = 0.27116898\n",
      "Iteration 72, loss = 0.26487204\n",
      "Iteration 73, loss = 0.26950214\n",
      "Iteration 74, loss = 0.28040846\n",
      "Iteration 75, loss = 0.30254775\n",
      "Iteration 76, loss = 0.26908930\n",
      "Iteration 77, loss = 0.27809652\n",
      "Iteration 78, loss = 0.27108279\n",
      "Iteration 79, loss = 0.27550834\n",
      "Iteration 80, loss = 0.26680365\n",
      "Iteration 81, loss = 0.30900032\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.45927659\n",
      "Iteration 2, loss = 3.41882408\n",
      "Iteration 3, loss = 2.57548731\n",
      "Iteration 4, loss = 2.14007672\n",
      "Iteration 5, loss = 1.30566053\n",
      "Iteration 6, loss = 0.45681398\n",
      "Iteration 7, loss = 0.39696358\n",
      "Iteration 8, loss = 0.36068454\n",
      "Iteration 9, loss = 0.35795099\n",
      "Iteration 10, loss = 0.38182498\n",
      "Iteration 11, loss = 0.40029549\n",
      "Iteration 12, loss = 0.39508752\n",
      "Iteration 13, loss = 0.38449860\n",
      "Iteration 14, loss = 0.37374706\n",
      "Iteration 15, loss = 0.35783196\n",
      "Iteration 16, loss = 0.37253641\n",
      "Iteration 17, loss = 0.37317077\n",
      "Iteration 18, loss = 0.36690837\n",
      "Iteration 19, loss = 0.35972483\n",
      "Iteration 20, loss = 0.35630316\n",
      "Iteration 21, loss = 0.36473011\n",
      "Iteration 22, loss = 0.36230860\n",
      "Iteration 23, loss = 0.37197341\n",
      "Iteration 24, loss = 0.36004322\n",
      "Iteration 25, loss = 0.35660408\n",
      "Iteration 26, loss = 0.38216435\n",
      "Iteration 27, loss = 0.35583512\n",
      "Iteration 28, loss = 0.36873147\n",
      "Iteration 29, loss = 0.34983127\n",
      "Iteration 30, loss = 0.33550910\n",
      "Iteration 31, loss = 0.31884274\n",
      "Iteration 32, loss = 0.31197761\n",
      "Iteration 33, loss = 0.31536338\n",
      "Iteration 34, loss = 0.32152337\n",
      "Iteration 35, loss = 0.32504618\n",
      "Iteration 36, loss = 0.31235352\n",
      "Iteration 37, loss = 0.31157748\n",
      "Iteration 38, loss = 0.32717176\n",
      "Iteration 39, loss = 0.33000933\n",
      "Iteration 40, loss = 0.31753608\n",
      "Iteration 41, loss = 0.31319301\n",
      "Iteration 42, loss = 0.32347450\n",
      "Iteration 43, loss = 0.32298486\n",
      "Iteration 44, loss = 0.33394642\n",
      "Iteration 45, loss = 0.31807914\n",
      "Iteration 46, loss = 0.32215400\n",
      "Iteration 47, loss = 0.31475379\n",
      "Iteration 48, loss = 0.31271033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Gridsearch MLP\n",
    "# Inspired by https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(10,10,10),(10,25,10),(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "mlp_base = MLPClassifier(random_state=random_state, max_iter=300, verbose=10)\n",
    "mlp_clf_grid = GridSearchCV(mlp_base, parameter_space, n_jobs=-1, cv=3)\n",
    "mlp_clf_grid.fit(training_df[X_cols],training_df[y_col])\n",
    "\n",
    "mlp_scaled_best_params = mlp_clf_grid.best_params_\n",
    "print(\"MLP Scaled Best Params found through grid search cv are:\")\n",
    "print(mlp_scaled_best_params)\n",
    "\n",
    "# Testing model\n",
    "mlp_acc_grid,mlp_f1_grid = test_model_metrics(mlp_clf_grid,\"MLP-Gridsearch\",testing_df[X_cols],testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    mlp_clf_grid,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"MLP Confusion Matrix-Gridsearch\")\n",
    "\n",
    "print(\"MLP Confusion Matrix-Gridsearch\")\n",
    "print(disp.confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33c69e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'hidden_layer_sizes': (10, 10, 10),\n",
       " 'learning_rate': 'constant'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_scaled_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f448b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP Scaled Best Params found through grid search cv are:\n",
    "{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f689cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'age_limit',\n",
       " 'view_count',\n",
       " 'like_count',\n",
       " 'view_like_ratio',\n",
       " 'is_comments_enabled',\n",
       " 'is_live_content',\n",
       " 'cat_codes',\n",
       " 'neu',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'compound']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341b6b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ld_score_ohe'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d564e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'age_limit',\n",
       " 'view_count',\n",
       " 'like_count',\n",
       " 'view_like_ratio',\n",
       " 'is_comments_enabled',\n",
       " 'is_live_content',\n",
       " 'cat_codes',\n",
       " 'neu',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'compound',\n",
       " 'ld_score_ohe']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_related_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "5011b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with all required columns\n",
    "tab_df = training_df[all_related_cols].copy()\n",
    "\n",
    "def remap_target(value):\n",
    "    if value == -1:\n",
    "        return \"Negative\"\n",
    "    elif value == 0:\n",
    "        return \"Neutral\"\n",
    "    elif value == 1:\n",
    "        return \"Positive\"\n",
    "\n",
    "tab_df[\"ld_score_ohe\"] = tab_df[\"ld_score_ohe\"].apply(remap_target)\n",
    "\n",
    "# Create tabular object and dataloaders\n",
    "cat_names = ['age_limit','is_comments_enabled','is_live_content','cat_codes']\n",
    "cont_names = ['duration', 'view_count', 'like_count','view_like_ratio','neu','neg','pos','compound']\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "\n",
    "# Creates splits\n",
    "splits = RandomSplitter(valid_pct=0.1)(range_of(tab_df))\n",
    "\n",
    "to = TabularPandas(tab_df, procs=procs,\n",
    "                   cat_names = cat_names,\n",
    "                   cont_names = cont_names,\n",
    "                   y_names=y_col,\n",
    "                   y_block=CategoryBlock,\n",
    "                   splits=splits)\n",
    "\n",
    "dls = to.dataloaders(bs=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ea930b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted f1 score by modifying fastai f1 score\n",
    "# def F1Score(axis=-1, labels=None, pos_label=1, average='weighted', sample_weight=None):\n",
    "#     \"F1 score for single-label classification problems\"\n",
    "#     return skm_to_fastai(skm.f1_score, axis=axis,\n",
    "#                          labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4c735dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss_flat = CrossEntropyLossFlat()\n",
    "f1_score_fai = F1Score()\n",
    "f1_score_multi = F1ScoreMulti(average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a40ba7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tabular learner\n",
    "learn_tab = tabular_learner(dls, layers=[100,200,100], metrics=accuracy,loss_func=cross_entropy_loss_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9a7bda29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.0008317637839354575)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fastai has a useful function to estimate the best learning rate to use.\n",
    "learn_tab.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "444fa81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.537413</td>\n",
       "      <td>0.770312</td>\n",
       "      <td>0.726088</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.387218</td>\n",
       "      <td>1.062577</td>\n",
       "      <td>0.732909</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.352577</td>\n",
       "      <td>1.069417</td>\n",
       "      <td>0.718311</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.318735</td>\n",
       "      <td>1.123301</td>\n",
       "      <td>0.733351</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.308855</td>\n",
       "      <td>0.992836</td>\n",
       "      <td>0.732296</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the data. We are not fine-tuning here. We are learning from our training data alone.\n",
    "learn_tab.fit_one_cycle(5,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5eefce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_test_df = testing_df[X_cols].copy()\n",
    "tab_test_dl = learn_tab.dls.test_dl(tab_test_df,ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "de6a7509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds, _, decoded = learn_tab.get_preds(dl=tab_test_dl, with_decoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "266e1f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2,  ..., 2, 2, 2])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ce84fb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6ca9fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded[decoded == 0] = -1\n",
    "decoded[decoded == 1] = 0\n",
    "decoded[decoded == 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d40c4e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1,  0,  1])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "de533233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6423349568038629"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(testing_df[y_col],decoded,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f07f86",
   "metadata": {},
   "source": [
    "### Lazy Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be19707d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.utils.testing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/lazypredict/Supervised.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, OneHotEncoder\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m all_estimators\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegressorMixin\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassifierMixin\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.utils.testing'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "lzp_clf = LazyClassifier(verbose=2,ignore_warnings=True, custom_metric=None)\n",
    "lzp_models,lzp_predictions = lzp_clf.fit(training_df[X_cols], testing_df[X_cols], training_df[y_col], testing_df[y_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c6cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
