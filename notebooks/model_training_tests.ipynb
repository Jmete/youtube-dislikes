{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983eb787",
   "metadata": {},
   "source": [
    "# Model Training & Testing\n",
    "This notebook is intended to help train and test various models, and expanded from our data_model_prep notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f1e0a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "random_state = 42\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb1f885",
   "metadata": {},
   "source": [
    "## Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5588aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For python pipeline that will be run from root folder of project\n",
    "# ROOT_DIR = os.path.abspath(os.curdir)\n",
    "\n",
    "# # Pickle save paths\n",
    "# training_df_path = os.path.join(ROOT_DIR,\"data/processed/training_df.pkl\")\n",
    "# testing_df_path = os.path.join(ROOT_DIR,\"data/processed/testing_df.pkl\")\n",
    "\n",
    "# Relative path for notebook\n",
    "training_df_path = \"../data/processed/training_df.pkl\"\n",
    "testing_df_path = \"../data/processed/testing_df.pkl\"\n",
    "\n",
    "# Small df for testing\n",
    "training_df_small_path = \"../data/processed/training_df_small.pkl\"\n",
    "# Big df for testing\n",
    "randompct10_pklpath=r\"/run/user/1000/gvfs/smb-share:server=metebox,share=data/JAMES/datasets/youtube-meta/youtube-02-2019-dump/randompct_df_10.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af712c5",
   "metadata": {},
   "source": [
    "## Load training and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88940982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_df = pd.read_pickle(training_df_path)\n",
    "testing_df = pd.read_pickle(testing_df_path)\n",
    "\n",
    "# big df\n",
    "# training_df_big = pd.read_pickle(randompct10_pklpath)\n",
    "\n",
    "# small df\n",
    "training_df = pd.read_pickle(training_df_small_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55753b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat training df and training_df_big for testing if more data helps\n",
    "# training_df = pd.concat([training_df,training_df_big])\n",
    "# training_df = training_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe9d64",
   "metadata": {},
   "source": [
    "## Columns for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "283c877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on what we can get at inference time from the Youtube API or scraping\n",
    "X_cols = [\n",
    "    \"duration\",\n",
    "    \"age_limit\",\n",
    "    \"view_count\",\n",
    "    \"like_count\",\n",
    "    \"view_like_ratio\",\n",
    "    \"is_comments_enabled\",\n",
    "    \"is_live_content\",\n",
    "    \"cat_codes\",\n",
    "    \"neu\",\n",
    "    \"neg\",\n",
    "    \"pos\",\n",
    "    \"compound\",\n",
    "]\n",
    "\n",
    "y_col = \"ld_score_ohe\"\n",
    "\n",
    "# Get all related columns - useful for fastai models\n",
    "all_related_cols = X_cols.copy()\n",
    "all_related_cols.append(y_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cce0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing scaled versions\n",
    "scaler = StandardScaler()\n",
    "training_df_scaled_X = scaler.fit_transform(training_df[X_cols])\n",
    "testing_df_scaled_X = scaler.transform(testing_df[X_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2159ff32",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60cb281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_test_sets(df,X_cols,y_col,random_state=None):\n",
    "    \"\"\"\n",
    "    Takes in a processed dataframe and splits it into appropriate training and test splits.\n",
    "    \"\"\"\n",
    "    X = df[X_cols]\n",
    "    y = df[y_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state=random_state)\n",
    "    print(\"Splits created\")\n",
    "    return X, y, X_train, X_test, y_train,y_test\n",
    "\n",
    "def train_model(clf_object,model_name,X_train,y_train,X_test,y_test):\n",
    "    clf_object.fit(X_train,y_train)\n",
    "    acc, f1 = test_model_metrics(clf=clf_object,model_name=model_name)\n",
    "    return clf_object, acc, f1\n",
    "\n",
    "def test_model_metrics(clf, model_name,X_test,y_test):\n",
    "    testpreds = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test,testpreds)\n",
    "    \n",
    "    if len(y_test.unique()) > 2:\n",
    "        average = \"weighted\"\n",
    "    else:\n",
    "        average = \"binary\"\n",
    "        \n",
    "    f1 = f1_score(y_test,testpreds,average=average)\n",
    "    print(f\"{model_name} metrics:\")\n",
    "    print(f\"Accuracy Score: {acc}\")\n",
    "    print(f\"F1 score: {f1}\")\n",
    "    return acc,f1\n",
    "\n",
    "def cross_val_model(df,clf_object,X_cols,y_col,random_state,scoring,cv=5):\n",
    "    \"\"\"\n",
    "    Takes in a df, processes it, and then outputs a cross-validation f1 score.\n",
    "    Adapted from sklearn docs.\n",
    "    Scoring types available here: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    \"\"\"\n",
    "    X, y, X_train,X_test,y_train,y_test = create_training_test_sets(df,X_cols,y_col,random_state=random_state)\n",
    "    scores = cross_val_score(clf_object, X, y, cv=cv,scoring=scoring,verbose=1,n_jobs=-1)\n",
    "    print(f\"{scores.mean():0.2f} {scoring} with a standard deviation of {scores.std():0.2f}\")\n",
    "    return scores\n",
    "\n",
    "def confusion_matrix_model(df,clf_object,X_cols,y_col,random_state,model_name):\n",
    "    \"\"\"\n",
    "    Takes in a df, processes it, and then outputs a confusion matrix.\n",
    "    Adapted from sklearn docs: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    Scoring types available here: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    \"\"\"\n",
    "    X, y, X_train,X_test,y_train,y_test = create_training_test_sets(df,X_cols,y_col,random_state=random_state)\n",
    "    clf_object.fit(X_train,y_train)\n",
    "    \n",
    "    # Adapted from sklearn docs\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    titles_options = [\n",
    "        (f\"{model_name} Confusion matrix, without normalization\", None),\n",
    "        (f\"{model_name} Normalized confusion matrix\", \"true\"),\n",
    "    ]\n",
    "    \n",
    "    for title, normalize in titles_options:\n",
    "        disp = ConfusionMatrixDisplay.from_estimator(\n",
    "            clf_object,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            cmap=plt.cm.Blues,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "        disp.ax_.set_title(title)\n",
    "\n",
    "        print(title)\n",
    "        print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa59c5",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cb2059",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038e2ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest metrics:\n",
      "Accuracy Score: 0.8010047084121158\n",
      "F1 score: 0.7777931664635174\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "rf_clf = RandomForestClassifier(n_jobs=-1,random_state=random_state)\n",
    "rf_clf.fit(training_df[X_cols],training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "rf_acc,rf_f1 = test_model_metrics(rf_clf,\"Random Forest\",testing_df[X_cols],testing_df[y_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9dedc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Confusion Matrix\n",
      "[[0.87886563 0.0218321  0.09930227]\n",
      " [0.42575647 0.09519601 0.47904752]\n",
      " [0.09678625 0.00381264 0.89940111]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    rf_clf,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"RF Confusion Matrix\")\n",
    "\n",
    "print(\"RF Confusion Matrix\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ec06570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest-Scaled metrics:\n",
      "Accuracy Score: 0.8043599154710266\n",
      "F1 score: 0.7800643006431393\n",
      "RF Confusion Matrix-Scaled\n",
      "[[0.87625478 0.0218321  0.10191312]\n",
      " [0.41832172 0.09504003 0.48663824]\n",
      " [0.09284071 0.00274111 0.90441818]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training model - scaled\n",
    "rf_clf = RandomForestClassifier(n_jobs=-1,random_state=random_state)\n",
    "rf_clf.fit(training_df_scaled_X,training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "rf_acc,rf_f1 = test_model_metrics(rf_clf,\"Random Forest-Scaled\",testing_df_scaled_X,testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    rf_clf,\n",
    "    testing_df_scaled_X,\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"RF Confusion Matrix-Scaled\")\n",
    "\n",
    "print(\"RF Confusion Matrix-Scaled\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cfb58b",
   "metadata": {},
   "source": [
    "### GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8731796e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM metrics:\n",
      "Accuracy Score: 0.8160011863715567\n",
      "F1 score: 0.7907486733672126\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "# Params Based on previous gridsearch cvs\n",
    "gbm_model = lgb.LGBMClassifier(learning_rate=0.05,\n",
    "                               max_depth=20,\n",
    "                               min_child_samples=15,\n",
    "                               num_leaves=100,\n",
    "                               reg_alpha=0.03,\n",
    "                               random_state=random_state)\n",
    "gbm_model.fit(training_df[X_cols],training_df[y_col], verbose=20,eval_metric='logloss')\n",
    "\n",
    "# Testing model\n",
    "gbm_acc,gbm_f1 = test_model_metrics(gbm_model,\"GBM\",testing_df[X_cols],testing_df[y_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d47cd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM Confusion Matrix\n",
      "[[0.86234526 0.03011479 0.10753995]\n",
      " [0.37080171 0.11380888 0.51538941]\n",
      " [0.07906038 0.00130411 0.91963551]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    gbm_model,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"GBM Confusion Matrix\")\n",
    "\n",
    "print(\"GBM Confusion Matrix\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acd63b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM-Scaled metrics:\n",
      "Accuracy Score: 0.8185963741519298\n",
      "F1 score: 0.7941335622498457\n",
      "GBM Confusion Matrix-Scaled\n",
      "[[0.85725861 0.03443619 0.1083052 ]\n",
      " [0.35619216 0.12529895 0.51850889]\n",
      " [0.07671797 0.00105491 0.92222711]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training model - scaled version\n",
    "# Params Based on previous gridsearch cvs\n",
    "gbm_model = lgb.LGBMClassifier(learning_rate=0.05,\n",
    "                               max_depth=20,\n",
    "                               min_child_samples=15,\n",
    "                               num_leaves=100,\n",
    "                               reg_alpha=0.03,\n",
    "                               random_state=random_state)\n",
    "gbm_model.fit(training_df_scaled_X,training_df[y_col], verbose=20,eval_metric='logloss')\n",
    "\n",
    "# Testing model\n",
    "gbm_acc,gbm_f1 = test_model_metrics(gbm_model,\"GBM-Scaled\",testing_df_scaled_X,testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    gbm_model,\n",
    "    testing_df_scaled_X,\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"GBM Confusion Matrix-Scaled\")\n",
    "\n",
    "print(\"GBM Confusion Matrix-Scaled\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b334408",
   "metadata": {},
   "source": [
    "### Neural Net\n",
    "We will use sklearn mlp and fastai to create a tabular learner neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dba7f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.42950026\n",
      "Iteration 2, loss = 3.57755761\n",
      "Iteration 3, loss = 3.78824876\n",
      "Iteration 4, loss = 3.41177869\n",
      "Iteration 5, loss = 4.03336538\n",
      "Iteration 6, loss = 3.35087394\n",
      "Iteration 7, loss = 3.38560261\n",
      "Iteration 8, loss = 3.09514178\n",
      "Iteration 9, loss = 3.04404880\n",
      "Iteration 10, loss = 3.02739196\n",
      "Iteration 11, loss = 2.99754585\n",
      "Iteration 12, loss = 2.95538401\n",
      "Iteration 13, loss = 3.20878666\n",
      "Iteration 14, loss = 3.05539064\n",
      "Iteration 15, loss = 2.96147884\n",
      "Iteration 16, loss = 2.90450433\n",
      "Iteration 17, loss = 3.06705597\n",
      "Iteration 18, loss = 3.11175691\n",
      "Iteration 19, loss = 3.08573033\n",
      "Iteration 20, loss = 3.32206599\n",
      "Iteration 21, loss = 2.96553022\n",
      "Iteration 22, loss = 3.31645275\n",
      "Iteration 23, loss = 2.89414545\n",
      "Iteration 24, loss = 3.21414995\n",
      "Iteration 25, loss = 2.76381431\n",
      "Iteration 26, loss = 2.70946889\n",
      "Iteration 27, loss = 2.71693304\n",
      "Iteration 28, loss = 2.99674700\n",
      "Iteration 29, loss = 3.24565714\n",
      "Iteration 30, loss = 3.46214256\n",
      "Iteration 31, loss = 3.34380238\n",
      "Iteration 32, loss = 3.09215028\n",
      "Iteration 33, loss = 3.22765617\n",
      "Iteration 34, loss = 2.97147225\n",
      "Iteration 35, loss = 2.81289802\n",
      "Iteration 36, loss = 2.69120203\n",
      "Iteration 37, loss = 2.85079362\n",
      "Iteration 38, loss = 3.03948965\n",
      "Iteration 39, loss = 2.96047099\n",
      "Iteration 40, loss = 2.70578158\n",
      "Iteration 41, loss = 2.60396738\n",
      "Iteration 42, loss = 2.60544293\n",
      "Iteration 43, loss = 2.53404259\n",
      "Iteration 44, loss = 2.66488238\n",
      "Iteration 45, loss = 2.40194731\n",
      "Iteration 46, loss = 2.59639349\n",
      "Iteration 47, loss = 2.69051471\n",
      "Iteration 48, loss = 2.49980874\n",
      "Iteration 49, loss = 2.98097198\n",
      "Iteration 50, loss = 2.58632990\n",
      "Iteration 51, loss = 2.28547287\n",
      "Iteration 52, loss = 2.91849998\n",
      "Iteration 53, loss = 2.61975033\n",
      "Iteration 54, loss = 2.55091827\n",
      "Iteration 55, loss = 2.40791105\n",
      "Iteration 56, loss = 2.22966365\n",
      "Iteration 57, loss = 2.54900757\n",
      "Iteration 58, loss = 2.24041658\n",
      "Iteration 59, loss = 2.45846155\n",
      "Iteration 60, loss = 2.37275240\n",
      "Iteration 61, loss = 2.33447023\n",
      "Iteration 62, loss = 2.43394899\n",
      "Iteration 63, loss = 2.43170414\n",
      "Iteration 64, loss = 2.30748960\n",
      "Iteration 65, loss = 2.37320806\n",
      "Iteration 66, loss = 2.19601108\n",
      "Iteration 67, loss = 2.24992562\n",
      "Iteration 68, loss = 2.28525860\n",
      "Iteration 69, loss = 2.22878599\n",
      "Iteration 70, loss = 2.20007116\n",
      "Iteration 71, loss = 2.15162230\n",
      "Iteration 72, loss = 2.38801432\n",
      "Iteration 73, loss = 2.28248844\n",
      "Iteration 74, loss = 2.15232760\n",
      "Iteration 75, loss = 2.15268771\n",
      "Iteration 76, loss = 2.22816894\n",
      "Iteration 77, loss = 2.22494506\n",
      "Iteration 78, loss = 2.10710213\n",
      "Iteration 79, loss = 2.46295932\n",
      "Iteration 80, loss = 2.56852024\n",
      "Iteration 81, loss = 2.46165848\n",
      "Iteration 82, loss = 2.25225992\n",
      "Iteration 83, loss = 2.22587975\n",
      "Iteration 84, loss = 2.10634370\n",
      "Iteration 85, loss = 2.13470242\n",
      "Iteration 86, loss = 2.26051927\n",
      "Iteration 87, loss = 2.14278738\n",
      "Iteration 88, loss = 2.19790151\n",
      "Iteration 89, loss = 2.04064204\n",
      "Iteration 90, loss = 1.98862313\n",
      "Iteration 91, loss = 1.98134105\n",
      "Iteration 92, loss = 2.29529238\n",
      "Iteration 93, loss = 2.13727348\n",
      "Iteration 94, loss = 2.04189583\n",
      "Iteration 95, loss = 2.07503614\n",
      "Iteration 96, loss = 2.05751085\n",
      "Iteration 97, loss = 1.94857561\n",
      "Iteration 98, loss = 1.96403145\n",
      "Iteration 99, loss = 2.04677200\n",
      "Iteration 100, loss = 1.95849962\n",
      "Iteration 101, loss = 1.82914345\n",
      "Iteration 102, loss = 1.86885172\n",
      "Iteration 103, loss = 1.81492600\n",
      "Iteration 104, loss = 1.72991738\n",
      "Iteration 105, loss = 1.70197882\n",
      "Iteration 106, loss = 1.59431871\n",
      "Iteration 107, loss = 1.48138607\n",
      "Iteration 108, loss = 1.49931526\n",
      "Iteration 109, loss = 2.06462557\n",
      "Iteration 110, loss = 1.87300097\n",
      "Iteration 111, loss = 1.70474911\n",
      "Iteration 112, loss = 1.77475162\n",
      "Iteration 113, loss = 1.56643234\n",
      "Iteration 114, loss = 1.62821818\n",
      "Iteration 115, loss = 1.55695604\n",
      "Iteration 116, loss = 1.52972349\n",
      "Iteration 117, loss = 1.57033000\n",
      "Iteration 118, loss = 1.62711615\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP metrics:\n",
      "Accuracy Score: 0.5253772290809328\n",
      "F1 score: 0.5616908327035757\n",
      "MLP Confusion Matrix\n",
      "[[0.88084628 0.03317578 0.08597794]\n",
      " [0.6815535  0.07018821 0.24825829]\n",
      " [0.43572918 0.0317637  0.53250712]]\n",
      "CPU times: user 35min 5s, sys: 1min 16s, total: 36min 21s\n",
      "Wall time: 6min 4s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# Starting with Sklearn MLP\n",
    "# Training model\n",
    "mlp_clf = MLPClassifier(random_state=random_state, max_iter=300, verbose=2)\n",
    "mlp_clf.fit(training_df[X_cols],training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "mlp_acc,mlp_f1 = test_model_metrics(mlp_clf,\"MLP\",testing_df[X_cols],testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    mlp_clf,\n",
    "    testing_df[X_cols],\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"MLP Confusion Matrix\")\n",
    "\n",
    "print(\"MLP Confusion Matrix\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1458ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.47613949\n",
      "Iteration 2, loss = 0.37496478\n",
      "Iteration 3, loss = 0.35207984\n",
      "Iteration 4, loss = 0.33903542\n",
      "Iteration 5, loss = 0.33085433\n",
      "Iteration 6, loss = 0.32486646\n",
      "Iteration 7, loss = 0.32012718\n",
      "Iteration 8, loss = 0.31563970\n",
      "Iteration 9, loss = 0.31146543\n",
      "Iteration 10, loss = 0.30746494\n",
      "Iteration 11, loss = 0.30369885\n",
      "Iteration 12, loss = 0.30047257\n",
      "Iteration 13, loss = 0.29749642\n",
      "Iteration 14, loss = 0.29466035\n",
      "Iteration 15, loss = 0.29177855\n",
      "Iteration 16, loss = 0.28900387\n",
      "Iteration 17, loss = 0.28559381\n",
      "Iteration 18, loss = 0.28316940\n",
      "Iteration 19, loss = 0.28047900\n",
      "Iteration 20, loss = 0.27894433\n",
      "Iteration 21, loss = 0.27648915\n",
      "Iteration 22, loss = 0.27461064\n",
      "Iteration 23, loss = 0.27301444\n",
      "Iteration 24, loss = 0.27134990\n",
      "Iteration 25, loss = 0.26969515\n",
      "Iteration 26, loss = 0.26858023\n",
      "Iteration 27, loss = 0.26776555\n",
      "Iteration 28, loss = 0.26581328\n",
      "Iteration 29, loss = 0.26545548\n",
      "Iteration 30, loss = 0.26447768\n",
      "Iteration 31, loss = 0.26325001\n",
      "Iteration 32, loss = 0.26176016\n",
      "Iteration 33, loss = 0.26179780\n",
      "Iteration 34, loss = 0.26076099\n",
      "Iteration 35, loss = 0.25997865\n",
      "Iteration 36, loss = 0.25912225\n",
      "Iteration 37, loss = 0.25850896\n",
      "Iteration 38, loss = 0.25820565\n",
      "Iteration 39, loss = 0.25741284\n",
      "Iteration 40, loss = 0.25700278\n",
      "Iteration 41, loss = 0.25619450\n",
      "Iteration 42, loss = 0.25527750\n",
      "Iteration 43, loss = 0.25475013\n",
      "Iteration 44, loss = 0.25447527\n",
      "Iteration 45, loss = 0.25411107\n",
      "Iteration 46, loss = 0.25372796\n",
      "Iteration 47, loss = 0.25312610\n",
      "Iteration 48, loss = 0.25351195\n",
      "Iteration 49, loss = 0.25264797\n",
      "Iteration 50, loss = 0.25211882\n",
      "Iteration 51, loss = 0.25165845\n",
      "Iteration 52, loss = 0.25125899\n",
      "Iteration 53, loss = 0.25124592\n",
      "Iteration 54, loss = 0.25080904\n",
      "Iteration 55, loss = 0.25105442\n",
      "Iteration 56, loss = 0.25014536\n",
      "Iteration 57, loss = 0.24941405\n",
      "Iteration 58, loss = 0.24968281\n",
      "Iteration 59, loss = 0.24920944\n",
      "Iteration 60, loss = 0.24847421\n",
      "Iteration 61, loss = 0.24821458\n",
      "Iteration 62, loss = 0.24852379\n",
      "Iteration 63, loss = 0.24878902\n",
      "Iteration 64, loss = 0.24788024\n",
      "Iteration 65, loss = 0.24765855\n",
      "Iteration 66, loss = 0.24782698\n",
      "Iteration 67, loss = 0.24747315\n",
      "Iteration 68, loss = 0.24698417\n",
      "Iteration 69, loss = 0.24676389\n",
      "Iteration 70, loss = 0.24675059\n",
      "Iteration 71, loss = 0.24600733\n",
      "Iteration 72, loss = 0.24677133\n",
      "Iteration 73, loss = 0.24589003\n",
      "Iteration 74, loss = 0.24596793\n",
      "Iteration 75, loss = 0.24582543\n",
      "Iteration 76, loss = 0.24532092\n",
      "Iteration 77, loss = 0.24497867\n",
      "Iteration 78, loss = 0.24550299\n",
      "Iteration 79, loss = 0.24553532\n",
      "Iteration 80, loss = 0.24454704\n",
      "Iteration 81, loss = 0.24486792\n",
      "Iteration 82, loss = 0.24507763\n",
      "Iteration 83, loss = 0.24413689\n",
      "Iteration 84, loss = 0.24435887\n",
      "Iteration 85, loss = 0.24462492\n",
      "Iteration 86, loss = 0.24485636\n",
      "Iteration 87, loss = 0.24417966\n",
      "Iteration 88, loss = 0.24383011\n",
      "Iteration 89, loss = 0.24437454\n",
      "Iteration 90, loss = 0.24385323\n",
      "Iteration 91, loss = 0.24425247\n",
      "Iteration 92, loss = 0.24374839\n",
      "Iteration 93, loss = 0.24386689\n",
      "Iteration 94, loss = 0.24319587\n",
      "Iteration 95, loss = 0.24367006\n",
      "Iteration 96, loss = 0.24257841\n",
      "Iteration 97, loss = 0.24331035\n",
      "Iteration 98, loss = 0.24270132\n",
      "Iteration 99, loss = 0.24270043\n",
      "Iteration 100, loss = 0.24310047\n",
      "Iteration 101, loss = 0.24248738\n",
      "Iteration 102, loss = 0.24228540\n",
      "Iteration 103, loss = 0.24193527\n",
      "Iteration 104, loss = 0.24210525\n",
      "Iteration 105, loss = 0.24211242\n",
      "Iteration 106, loss = 0.24217669\n",
      "Iteration 107, loss = 0.24149504\n",
      "Iteration 108, loss = 0.24203197\n",
      "Iteration 109, loss = 0.24199232\n",
      "Iteration 110, loss = 0.24132418\n",
      "Iteration 111, loss = 0.24144111\n",
      "Iteration 112, loss = 0.24210215\n",
      "Iteration 113, loss = 0.24110206\n",
      "Iteration 114, loss = 0.24172170\n",
      "Iteration 115, loss = 0.24166647\n",
      "Iteration 116, loss = 0.24115670\n",
      "Iteration 117, loss = 0.24134472\n",
      "Iteration 118, loss = 0.24105495\n",
      "Iteration 119, loss = 0.24102108\n",
      "Iteration 120, loss = 0.24113455\n",
      "Iteration 121, loss = 0.24086086\n",
      "Iteration 122, loss = 0.24112731\n",
      "Iteration 123, loss = 0.24096739\n",
      "Iteration 124, loss = 0.24090973\n",
      "Iteration 125, loss = 0.24006153\n",
      "Iteration 126, loss = 0.24087739\n",
      "Iteration 127, loss = 0.24017185\n",
      "Iteration 128, loss = 0.24043433\n",
      "Iteration 129, loss = 0.24002348\n",
      "Iteration 130, loss = 0.24009572\n",
      "Iteration 131, loss = 0.24030119\n",
      "Iteration 132, loss = 0.23983767\n",
      "Iteration 133, loss = 0.24018297\n",
      "Iteration 134, loss = 0.23955637\n",
      "Iteration 135, loss = 0.23973429\n",
      "Iteration 136, loss = 0.23976329\n",
      "Iteration 137, loss = 0.23915398\n",
      "Iteration 138, loss = 0.23945179\n",
      "Iteration 139, loss = 0.23932308\n",
      "Iteration 140, loss = 0.23882502\n",
      "Iteration 141, loss = 0.23922733\n",
      "Iteration 142, loss = 0.23924024\n",
      "Iteration 143, loss = 0.23908971\n",
      "Iteration 144, loss = 0.23901049\n",
      "Iteration 145, loss = 0.23899566\n",
      "Iteration 146, loss = 0.23937379\n",
      "Iteration 147, loss = 0.23829284\n",
      "Iteration 148, loss = 0.23862848\n",
      "Iteration 149, loss = 0.23858441\n",
      "Iteration 150, loss = 0.23892978\n",
      "Iteration 151, loss = 0.23863476\n",
      "Iteration 152, loss = 0.23865280\n",
      "Iteration 153, loss = 0.23820222\n",
      "Iteration 154, loss = 0.23806954\n",
      "Iteration 155, loss = 0.23855156\n",
      "Iteration 156, loss = 0.23771806\n",
      "Iteration 157, loss = 0.23824871\n",
      "Iteration 158, loss = 0.23753992\n",
      "Iteration 159, loss = 0.23816215\n",
      "Iteration 160, loss = 0.23734725\n",
      "Iteration 161, loss = 0.23796537\n",
      "Iteration 162, loss = 0.23770124\n",
      "Iteration 163, loss = 0.23750661\n",
      "Iteration 164, loss = 0.23777948\n",
      "Iteration 165, loss = 0.23751648\n",
      "Iteration 166, loss = 0.23754815\n",
      "Iteration 167, loss = 0.23754409\n",
      "Iteration 168, loss = 0.23735569\n",
      "Iteration 169, loss = 0.23697362\n",
      "Iteration 170, loss = 0.23688686\n",
      "Iteration 171, loss = 0.23724735\n",
      "Iteration 172, loss = 0.23743094\n",
      "Iteration 173, loss = 0.23676311\n",
      "Iteration 174, loss = 0.23675580\n",
      "Iteration 175, loss = 0.23663289\n",
      "Iteration 176, loss = 0.23673385\n",
      "Iteration 177, loss = 0.23705758\n",
      "Iteration 178, loss = 0.23678450\n",
      "Iteration 179, loss = 0.23658969\n",
      "Iteration 180, loss = 0.23583050\n",
      "Iteration 181, loss = 0.23659360\n",
      "Iteration 182, loss = 0.23683306\n",
      "Iteration 183, loss = 0.23672557\n",
      "Iteration 184, loss = 0.23658189\n",
      "Iteration 185, loss = 0.23637075\n",
      "Iteration 186, loss = 0.23604882\n",
      "Iteration 187, loss = 0.23691171\n",
      "Iteration 188, loss = 0.23631680\n",
      "Iteration 189, loss = 0.23592814\n",
      "Iteration 190, loss = 0.23634344\n",
      "Iteration 191, loss = 0.23618001\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP-scaled metrics:\n",
      "Accuracy Score: 0.7290932908216858\n",
      "F1 score: 0.7087351727740249\n",
      "MLP Confusion Matrix-scaled\n",
      "[[8.38262435e-01 2.70087779e-04 1.61467477e-01]\n",
      " [5.55630654e-01 3.32744099e-03 4.41041905e-01]\n",
      " [1.74692040e-01 4.07013930e-04 8.24900946e-01]]\n",
      "CPU times: user 50min 54s, sys: 2min 9s, total: 53min 4s\n",
      "Wall time: 8min 51s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAEWCAYAAAAdG+ASAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAv40lEQVR4nO3deXwV1fnH8c83CfsOYd9VUKKggLsVVyxoBYuWgtbW6k9aN6wb1dYC2mrd12LrbkVFUVFRcEFtRQsqi4gCoojsa9hRhCzP74+ZwE3IcpN7k3tzed685vW6M3PmzJlL8uScOTPnyMxwzrlUlZboAjjnXGXyIOecS2ke5JxzKc2DnHMupXmQc86lNA9yzrmU5kEuBUhqKWmqpG2S7o4hnz9JeiyeZUsESfMknZjocgBIekrS36r6WLfHPhvkJC2RtEtSZpHtn0kySZ3C9RJ/0MJ030vaLmmlpHskpZeQVpKGS/oyPGaFpBcldY/D5QwDsoGGZnZNRTMxs1vN7P/iUJ5CJF0Qflf3Ftk+MNz+VJT5RPVLb2YHm9l/K1Zal2r22SAX+g4YWrASBpy65czjUDOrD5wCnAtcXEK6+4ErgeFAU6Ar8CpwRjnPV5yOwHxL7ie7vwUGS8qI2PYb4Ot4naBI3s4BHuTGAr+OWP8N8HRFMjKzr4APgUOK7pPUBbgMGGpm75vZTjP7wcyeNbPbwjSNJD0tab2kpZJulJQW7rtA0keS7pK0SdJ3kvqH+54Kyz0irFGeWrTGI+lESSsi1v8Y1jy3SVoo6ZRw+2hJz0SkGxA2/TZL+q+kbhH7lki6VtJcSVskvSCpdilf0RrgC+Cn4fFNgWOBiUW+qxclrQnznCrp4HD7MOC8iOt8PaIcf5Q0F/heUka47dRw/+TIJryk5yU9UVwBw9r2vZLWSdoq6QtJh4T76ki6O/y/2RL+f9QprcwlnONnkuaE3+k0ST0i9vWUNDv8f3kBKO37dFHa14Pcx0BDSd3CZuYQ4JkyjimWpCzgeOCzYnafAqwws09LyeJBoBGwH3ACQfD9bcT+o4CFQCZwB/C4JJnZBcCzwB1mVt/M3i2jnAcClwNHmFkDgqCzpJh0XYFxwB+A5sBk4HVJNSOSDQb6AZ2BHsAFpZ2b4A9IwR+VIcBrwM4iad4EugAtgNnhtWFmjxS5zjMjjhlKUCNubGa5RfK7EDhf0smSzgOOJKhRF+c0oA9BLbtReH0bwn13Ab0JAnNTYASQX1qZi5LUE3gC+B3QDHgYmCipVvi9vkrwh7cp8CJwdgnldOWwrwc52FOb6wssAFaW8/jZkjYBrwOPAU8Wk6YZsLqkDCIC7A1mts3MlgB3A+dHJFtqZo+aWR7wb6A10LKcZQXIA2oBWZJqmNkSM/u2mHS/BCaZ2RQzyyH4Ja9D8Ete4AEzW2VmGwmu/7Ayzv0KcKKkRgTf+V61ZjN7IvwOdgKjgUPD9KV5wMyWm9mOYvJbA1xC8J3dD/zazLaVkE8O0AA4CJCZLTCz1WGN+kLgSjNbaWZ5ZjYtLGN5yjwMeNjMPgnz+DdBkD86XGoA95lZjpm9BMwo47pdFDzIBUHuXIJaSEWaqr3MrImZ7W9mN5pZfjFpNhAEpZJkEvyAL43YthRoG7G+puCDmf0Qfqxf3sKa2SKC2tloYF3YfGtTTNI2keUJr2t5SWUCfiirPGEQmgTcCDQzs/9F7peULuk2Sd9K2sqeGmYmpVtexv7XgXRgoZl9FHG+eWHTd7uk483sfeAfwBiC7+YRSQ3D89cmuK9YSDnL3BG4Jmyqbpa0GWhP8F23AVYWua+6tJg8XDnt80HOzJYSdECcDkyopNO8B7STdHgJ+7MJahEdI7Z1oPy1ygLfU7gDpVXkTjN7zsx+Ep7PgNuLyWNVZHkkieAXsqJlKvA0cA3F3xY4FxgInErQXOxUcPqCopeQZ1kdLrcQ1NJbS9rd0RT2wtYPlw/DbQ+YWW8gi6DZeh3B/8+PwP4VKHOk5cAtZtY4YqlrZuMIavptw++5QIcyrstFYZ8PcqGLgJPN7PsS9qdLqh2x1CwhXbHM7BvgIWBc2AlQM8xniKTrwyboeOAWSQ0kdQSupoL3B4E5wOmSmkpqRVBzA4J7cuH9qVoEv7g72HNvKdJ44AxJp0iqQRCYdgLTKlimAh8Q3Bp4sJh9DcJzbCAI0rcW2b+W4J5l1CT1Ibi3+WuCDpoHJbUtIe0Rko4Kr/d7gu8nP6zFPgHcI6lNWHs7JvwOyypzpEeB34fnkKR6ks6Q1ACYDuQCwyXVkDSI4P6hi5EHOcDMvjWzmaUkuZ4gGBQs71fgNMPZ0xTaTND0+TlBUwrgCoJfrMXAR8BzBL9YFTEW+Jyg6fQO8ELEvlrAbQS1kzUEN8tvKJqBmS0EfkUQjLKBM4EzzWxXBctUkK+Z2XvhfbyiniZooq0E5hN0DEV6nOBe4mZJr5Z1rrCp+TRweXgv7cMwjyeL1JgKNCQIRJvCcmwA7gz3XUvQOzwD2EhQ+02Losy7hT9jFxP8HGwCFhF21oTf66BwfSPBPdHKalnsU5Tcj1Y551xsvCbnnEtpHuSccynNg5xzLqV5kHPOpbSkfaFZGXVMNRskuhhJq2c3f4SqLD/mFPdkjIs0/4vPss2seUWPT2/Y0Sx3rxdNimU71r9tZv0qeq6KSt4gV7MBtQ4cnOhiJK3/ffKPRBch6X29uqS3t1yBQzs0jOmtCsv9kVoHDYkq7Y+fPVjWmyuVImmDnHOuGhBQ7COHycODnHMuNkruW/se5JxzsfGanHMudQnSih3xP2l4kHPOVZxI+uZqcpfOOZfkFDRXo1miyU3qp2BI/kWSri9mfwdJ/1Ew4dRcSaeXlacHOedcbJQW3VJWNsEI2WOA/gTj+Q0NpxWIdCMw3sx6Eoym/VBZ+XqQc87FJn41uSOBRWa2OBx66nmCAUkjGcGQWBAMUrqqrEz9npxzLgYqzz25TEmR4zY+Ek5QVKAthYeyX0EwgVOk0cA7kq4A6hGMyFwqD3LOuYoT5eldzTazkqYAiNZQ4Ckzu1vSMcBYSYeUMLcK4EHOOReTctXkyrKSYB6RAu3Ye06RiwimwcTMpiuY6zcTWFdSpn5PzjkXmzRFt5RtBtBFUudwHpUhFJl8HFhGMI8xCiY7rw2sLy1Tr8k55youjs/JmVmupMuBtwmmkHzCzOZJuhmYaWYTCSZUelTSVQSdEBdYGXM4eJBzzsUmjq91mdlkYHKRbSMjPs8HjitPnh7knHMx8Ne6nHOpLslf6/Ig55yruHK8spUoHuScc7HxmpxzLqV5Tc45l7ri+jBwpfAg55yruPK91pUQHuScczHwmpxzLtX5PTnnXErzmpxzLqV5Tc45l7Lk9+SccylOaR7knHMpSoC8ueqcS1kKlyTmQc45FwN5Ta46OOWYbvz9mnNIT0tj7GvTuO/fUwrtb9eyCQ+NPp9GDeqQnpbGTf94jSnT5hfaP338jdz+6GT+8cx7VV38mLw7bT433P0Sefn5nD/wWK664LRC+3fuyuGSUWOZ89UymjaqxxO3XkiHNs0AuOfJt3lm4nTS09K47dpzOOWYrFLzvPjGp5izYBkZGen0Prgj9/5pKDUy0nlg7Lu8+OYMAHLz8vl6yRoWvXMbTRrVq8JvIjbTZy3knsfeID8vnwGnHcFvzjmx0P7PvvyOex97g0VL1vDX64ZwynHdd+9bs34ztzz4Muuyt4DEvSMvoE3LJlV8BRWX7EGuSu4YSjpI0nRJOyVdWxXnjFZamrhzxGB+ceVDHD34b5x9Wm8O7NyqUJprLurHq+/O5oRf3c5Ff36Su/74y0L7/3bVIN6dNq8qix0XeXn5XHfHeF68/1I+Hn8jL78zi68Wry6UZuxr02nUsA6zXxnNJeeexOgHXwPgq8WrmTBlNtNf+DMvPXAp194+nry8/FLz/EX/I/j0pb8w7fk/sWNnDk+/Og2A4eefyofP3cCHz93AyMsGcFyvLtUqwOXl5XPnwxO5b9RveX7MVbwz9XMWL1tbKE3L5o35y5XncNoJh+51/E33judXP+/DCw9dzZN3XUrTxtXn2gHS0tKiWqIhqZ+khZIWSbq+mP33SpoTLl9L2lxm+cp/SRWyERgO3FVF54ta74M7sXh5NktXbiAnN48JU2Zz+gk9Cicyo0G92gA0rF+HNdlbdu86/YQeLFu1ga8Wr6nKYsfFrHlL2K99Jp3aZVKzRgaD+vZi8gdzC6V5c+pchp4RTH058OSefDBjIWbG5A/mMqhvL2rVrEHHtpns1z6TWfOWlJrnaccdjBQ0b3of3JFV6zbtVaaX35nJ2af1rvyLj6P53yynXetmtG3VlBo1Muh7/KFM/WRBoTRtWjahS+fWpBWp9SxetpbcvHyO6tkFgLp1alG7Vs0qK3vMVI6lrKykdGAM0B/IAoZKyopMY2ZXmdlhZnYY8CAwoax8qyTImdk6M5sB5FTF+cqjdfNGrFy755dt1dpNtG7eqFCa2x6ZzOD+R/LlG39l/H2XMOLOFwGoV6cmV/66L7c/WmhI+mpj9fottI1oFrVp2YTV67cUSrNq3Z40GRnpNKxfh41bvt/72BbBsdHkmZObxwuTP93dvC3ww4+7eG/6AgacfFi8LrFKrNuwlZaZe35mWmQ2ZP2GLaUcscfyVdk0qFebP976DOdf+QAPPDmZvLwSpxBNOgrvyUWzROFIYJGZLTazXcDzwMBS0g8FxpWVaXI/4JIkzv7p4Tz3xscc8rO/MPgP/+RfN/0aSfxx2Bn8c9z7fL9jV6KLWK1ce9sLHNvzAI7teUCh7W9N/YKjeuxXrZqqscrNy2fO/CUMv/B0nrznMlau2cik92YluljlUo4glylpZsQyrEhWbYHlEesrwm3FnbMj0Bl4v6zyJVXHQ3jRwYXXqF8l54ym5vGrgcfwi+FjAJjxxXfUrlWDZo3rcfjBHRl48mHcdMVZNGpQh/x8Y+fOHB59cWqVlD1W0dRi27QI0rRt2YTc3Dy2bt9B00b19j523Z5jS8vz9kcnk715O2P/9H97lWfClFmc/dPq1VQFaNGsIWsjbmGsy95K82aNSjki8thGdO3chratmgJwwtFZfLlwOQMqpaSVoxwdD9lmdnicTjsEeMnM8spKWGk1OUmXRdwgbBPNMWb2iJkdbmaHK6NOZRWtkNnzl7J/h+Z0aNOMGhnpDOrbizenFr4vtXLNRvoccSAAXTu1pFbNGmRv2s7pw+7j0IGjOHTgKP457r/c89Q71SbAAfTK6si3y9azdGU2u3JymTBlNv37FL4f2e/47oyb9AkAr73/GX2O6Iok+vfpwYQps9m5K4elK7P5dtl6eh/cqdQ8n351Gu9NX8Bjf7tgrxvRW7bv4H+zF+19P7Qa6NalHctXZbNqzUZycnKZ8uHn9DmqW1THZnVpx7bvd7Bpy3YAZs5dTOf2LSqzuHEXx+bqSqB9xHq7cFtxhhBFUxUqsSZnZmMIbiImtby8fEbcMZ6XH7iM9HTx7MSP+WrxGm743RnMWbCMN6d+wY33vcL9fx7KpUNPwoDLbhqb6GLHRUZGOneMGMzZw8eQl2ecN+Bouu3fmlv/9QaHdevA6Sf04PyBx/L7UU/T6+ejadKwHo/f8lsAuu3fmrNO7cnRg28hIz2NO0cMJj09CFzF5Qlw9W3P075VU0678G4AzjzpMEZc3B+ASf/5nJOOOoh6dWol4JuITUZ6Otf+bgDDRz9Bfr5x5qmHs1+Hljz87BS6HdCWPkdlMf+b5Yy49Rm2bd/BhzMW8Ohz7/L8mKtIT09j+G9P5/IbH8cwDtq/LWeddkSiLyl6AqXF7RGSGUAXSZ0JgtsQ4Ny9TikdBDQBpkdVxDImn44LSa2AmUBDIB/YDmSZ2daSjkmr28JqHTi40stWXW2a8Y9EFyHpfb16W6KLkPQO7dBwVixNyBqZ+1vjM2+NKm32U0PKPJek04H7gHTgCTO7RdLNwEwzmximGQ3UNrO9HjEpTpXckzOzNQRVT+dcionnw8BmNhmYXGTbyCLro8uTZ1J1PDjnqqHkfuHBg5xzLgZK/te6PMg552LiQc45l7KEon4vNVE8yDnnYpPcFTkPcs65GPg9OedcqvMg55xLaR7knHMpLY6vdVUKD3LOuQorx8v3CeNBzjkXEw9yzrmU5kHOOZfakjvGeZBzzsXGa3LOuZQlBdN6JjMPcs65GHjvqnMuxSV5jPMpCZ1zsYnjRDZI6idpoaRFkood3lzSYEnzJc2T9FxZeXpNzjlXcYpfTU5SOsHkV30J5lydIWmimc2PSNMFuAE4zsw2SSpzajMPcs65ChNx7Xg4ElhkZosBJD0PDATmR6S5GBhjZpsAzGxdWZl6c9U5F5O0NEW1AJmSZkYsw4pk1RZYHrG+ItwWqSvQVdL/JH0sqV9Z5fOanHOu4srXXM2OZfrDUAbQBTiRYAbAqZK6m9nmkg7wmpxzrsJEXDseVgLtI9bbhdsirQAmmlmOmX0HfE0Q9ErkQc45F4PoAlyUQW4G0EVSZ0k1gSHAxCJpXiWoxSEpk6D5uri0TD3IOediIkW3lMXMcoHLgbeBBcB4M5sn6WZJA8JkbwMbJM0H/gNcZ2YbSsvX78k55youzq91mdlkYHKRbSMjPhtwdbhExYOcc67CCu7JJTMPcs65mCR5jPMg55yLjdfknHMpLcljnAc551wMfHLpiqvbtAndh/wi0cVw1dj905YkuggpT8gHzXTOpbYkr8h5kHPOxcabq8651BXH8eQqiwc551yF+cPAzrmU50HOOZfSvHfVOZe6/J6ccy6Vyedddc6luiSPcR7knHOxSUvyKOdBzjlXYYrzoJmVwYc/d87FJE3RLdGQ1E/SQkmLJF1fzP4LJK2XNCdc/q+sPL0m55yLSbw6HiSlA2OAvgSzcs2QNNHM5hdJ+oKZXR5tviUGOUkPAlbSfjMbHu1JnHOpK4635I4EFpnZ4iBfPQ8MBIoGuXIprSY3M5aMnXOpTwSPkUQpU1JkXHnEzB6JWG8LLI9YXwEcVUw+Z0vqQzDn6lVmtryYNLuVGOTM7N+R65LqmtkPpWXmnNv3lKPfIdvMDo/xdK8D48xsp6TfAf8GTi7tgDI7HiQdE85x+FW4fqikh2IsqHMuFSgYNDOaJQorgfYR6+3CbbuZ2QYz2xmuPgb0LivTaHpX7wN+CmwIT/I50CeK45xzKU4Ez8lFs0RhBtBFUmdJNYEhwMRC55NaR6wOIJiEulRR9a6a2fIiPSh50RznnEt98ep4MLNcSZcDbwPpwBNmNk/SzcBMM5sIDJc0AMgFNgIXlJVvNEFuuaRjAZNUA7iSKKKnc27fEM93V81sMjC5yLaREZ9vAG4oT57RNFd/D1xG0POxCjgsXHfO7eOk6JdEKbMmZ2bZwHlVUBbnXDWUnuTvrkbTu7qfpNfDVynWSXpN0n5VUTjnXPKTFNWSKNE0V58DxgOtgTbAi8C4yiyUc656CHpX4/fuamWIJsjVNbOxZpYbLs8AtSu7YM65aiDKWlwia3KlvbvaNPz4ZjgawPME77L+kiK9H865fVeS35IrteNhFkFQK7iE30XsM8rZjeucS03VdvhzM+tclQVxzlU/AtKTfNDMqN54kHQIkEXEvTgze7qyCuWcqz6SO8RFEeQkjQJOJAhyk4H+wEeABznn9nFS8s/xEE3v6jnAKcAaM/stcCjQqFJL5ZyrNqr9Gw/ADjPLl5QrqSGwjsLDoVR7R3RqwuUnHUC6xKQvVzPu08Jj8P304Jb8vs9+ZG/fBcArc1Yy+Ys1ALRoUItrT+tKiwa1MOD6CV+wduvOoqeoFt6dNp8b7n6JvPx8zh94LFddcFqh/Tt35XDJqLHM+WoZTRvV44lbL6RDm2YA3PPk2zwzcTrpaWncdu05nHJMFj/uzOGMYfexMyeXvNw8BpzSkxt+dwYAV/z1WT5bsAwz44AOLRgz6nzq161V5dccL1kt63POoW1IE/zvu01M+Xp9sekOa9OQi4/pyO3vLWLZ5h27tzepU4O/nNaFSfPX8d432VVV7Lioth0PEWZKagw8StDjuh2YXpGTSeoH3E8wwsBjZnZbRfKJpzTBlad04bqX5rJ+207+dV4vpi3awNKNhccH/c/C9Tzw/qK9jr+h/0E888kyZi3dRO0aaViJA8Ynt7y8fK67Yzyv/ONy2rRszMm/uZP+fbpz0H57RrYZ+9p0GjWsw+xXRvPyOzMZ/eBrPPH3C/lq8WomTJnN9Bf+zJr1Wzjrsn8w8+WR1KqZwWv/HE79urXIyc2j///dw6nHZnFE987cctUgGtavA8Cf732ZR8d/sFdQrS4EDD6sDQ9+9B2bf8hlxMn788XqrazZVviPXa2MNE46IJPvNuw99uzZPVozb832KipxfCV5jCu7uWpml5rZZjP7F8EEE78Jm63lEjFJRX+C+3tDJWWVN594O6hVQ1Zt3sHqLT+Sm2+8v3Adxx3QLKpjOzatS3qamLV0EwA/5uSzMze/MotbaWbNW8J+7TPp1C6TmjUyGNS3F5M/mFsozZtT5zL0jGA06oEn9+SDGQsxMyZ/MJdBfXtRq2YNOrbNZL/2mcyatwRJu2tnObl55OTm7f6rXxDgzIwdO3OSvjZQmk5N67L++11s+D6HPDNmrdhCjzYN90r3s6yWvPP1enLyC/+M9GjTkA0/7GL11h+rqshxI4n0tOiWRCkxyEnqVXQBmgIZ4efy2j1JhZntIni4eGDFih0/mfVrsi7iL+76bTvJrL93s6lPl0we+3VvRp+ZRfMGwf52Teuw/cdcbhqQxSPn9+J3ffZL6OsrsVi9fgttWzbZvd6mZRNWr99SKM2qdXvSZGSk07B+HTZu+X7vY1vsOTYvL5/jz/07XU+7nhOPOojDD+m0O91lN43lwH5/4pslaxn2yxMq8eoqV+M6GWz6IWf3+uYdOTSuU6NQmvaNa9Okbg3mrdlWaHut9DT6dm3O5PnrqqSslaHavvEA3F3KPqOMcdWLUeYkFZKGAcMAajZuWc7sK8/0bzfw/lfryMkzzuzRmuv7Hcg1L84lXaJ7u0YMGzuLtVt/ZNTPsuh3cCsmf7km0UVOGunpaXz43A1s2fYDv7ruUeYvWkXWAW0AGDPqfPLy8hlx54u88s4szhtwTIJLWzkEDOrRmrEzV+y17/SsFvznm2x25lXPFgAk/+TNpT0MfFJVFiQ85yPAIwD12x1YJXe3srfvokWDPTW35g1qkb298L2UrT/m7v486YvVDOsTDMKyfvtOvl23ndVbgmbGR4uyyWrTEL6sgoLHWevmjVi5dtPu9VVrN9G6eeFO9DYtgjRtWzYhNzePrdt30LRRvb2PXbf3sY0a1OX43l15b/r83UEOgiA46LTePDB2SrUNcpt35NKk7p6aW+M6Ndi8Y0/NrlZGGm0a1uYP4c9Nw9oZ/O7Yjjw8bSmdmtalZ9tGnNW9FXVqpGMYufnGB99uqPLrqAiR/B0PVRmEy5ykIhG+WrOVto3r0KphbTLSxMkHtmBakR+wpvVq7v587P7NWBbeOF64Zhv1a2XQKGya9OzQhKXF3FSuDnpldeTbZetZujKbXTm5TJgym/59ehRK0+/47oyb9AkAr73/GX2O6Iok+vfpwYQps9m5K4elK7P5dtl6eh/ciexN29iyLfg+dvy4i/98+hVdOrXEzFi8POh9NDPemjqXrh2Tp+ZeXks3/UCL+rVoVrcG6RK92zXii1Vbd+//MTefP76xgJFvLWTkWwv5buMPPDxtKcs27+DeDxbv3v6fRdm8/dX6ahPgCsRzFBJJ/SQtlLQofGe+pHRnSzJJZc7+FdUbD3Gye5IKguA2BDi3Cs9frHyDB95fxB1ndyctTbz55RqWbPiB3x7biYVrtzHt2w0M6tmW4/ZvRl6+sfXHXG57+6vdx/5z6mLu/kUPBHy9djtvzF2d2AuqoIyMdO4YMZizh48hL884b8DRdNu/Nbf+6w0O69aB00/owfkDj+X3o56m189H06RhPR6/Jeh/6rZ/a846tSdHD76FjPQ07hwxmPT0NNZkb+XS0WPJy88nP9/4+am96Hd8d/Lz87lk9Fi2fb8DMzikS1vuvv6XCf4GKi7fYPycVVz2k86kCaYv2cTqbTs5I6sFyzbt4IvV28rOpJqS4vdaV0TnZF+C21kzJE00s/lF0jUgmIbhk6jytSp85kHS6QSzfxVMUnFLSWnrtzvQul/xSEm793n/uab63qivKpe9/EWii5D0nhjSY1Ysc6G26nKInX/vy1GlvevMg0o9l6RjgNFm9tNw/QYAM/t7kXT3AVOA64BrzWwmpYhmZGBJ+pWkkeF6B0lHlnVcccxsspl1NbP9SwtwzrnqoxxvPGRKmhmxDCuSVXGdk20Ln0u9gPZmNina8kXTXH0IyCfoTb0Z2Aa8DBwR7Umcc6mpYN7VKGXHUmuUlAbcQxTTEEaKJsgdZWa9JH0GYGabwolfnXMunr2XZXVONgAOAf4b9ui2AiZKGlBakzWaIJcT3hA0AEnNCWp2zjkXz9e6Su2cNLMtQOae8+q/xOOeHPAA8ArQQtItBMMs3Vre0jvnUk88X+sys1zgcuBtggnsx5vZPEk3SxpQ0TJGM+/qs5JmEQy3JOAsM1tQ0RM651JLPF9lNLPJFJlDxsxGlpD2xGjyjGbQzA7AD8DrkdvMbFk0J3DOpa5ydjwkRDT35CaxZ0Kb2kBnYCFwcCWWyzlXTSR5jIuqudo9cj18TuXSSiuRc676SPDE0dEo92tdZjZb0lFlp3TO7QuU5FPZRHNP7uqI1TSgF7Cq0krknKs2BGQk+VhL0dTkGkR8ziW4Rxfdy2rOuZSX7EMtlRrkwoeAG5jZtVVUHudcNRL0ria6FKUrMchJyjCzXEnHVWWBnHPVSIKnG4xGaTW5Twnuv82RNBF4Efi+YKeZTajksjnnqoFUeE6uNrCBYBSSguflDPAg59w+TkB6Ne54aBH2rH7JnuBWoJrOLuqciy+RVo0fIUkH6kOxV+BBzjkXTmST6FKUrrQgt9rMbq6ykjjnqp9q/sZDkhfdOZcMqnPHwylVVgrnXLVUrZurZraxKgvinKue4jUlYWWpynlXnXMpRlTtDPUVkezlc84lMwXvrkazRJWd1E/SQkmLJF1fzP7fS/pC0hxJH0nKKitPD3LOuZgoyqXMfIJ35ccA/YEsYGgxQew5M+tuZocBdxBMUVgqD3LOuQorGP48miUKRwKLzGyxme0CngcGRiYws60Rq/WI4pldvyfnnItJObodMiVFTh/4iJk9ErHeFlgesb4C2GuAXkmXAVcDNQleNy2VBznnXAxEWvS9q9lmdnisZzSzMcAYSecCNwK/KS29N1edcxVW0LsazRKFlUD7iPV24baSPA+cVVamHuScczGJY+/qDKCLpM6SagJDgIlFztUlYvUM4JuyMvXmqnMuJvF6FDgcpPdy4G2CAUKeMLN5km4GZprZROBySacCOcAmymiqQhIHuQ5N6/LQkJ6JLoarxp6768lEFyH1Kb5zPJjZZGBykW0jIz5fWd48kzbIOeeSn4D0JH951YOccy4myR3iPMg552KU5BU5D3LOuYoLHiFJ7ijnQc45FxOvyTnnUpiQ1+Scc6nKe1edc6lN3lx1zqU4D3LOuZTm9+SccykrGDQz0aUonQc551xMqvO8q845VyZvrjrnUpY3V51zKc4fBnbOpTJ/Ts45l+qSPMb5HA/OuYoreK0rmiWq/KR+khZKWiTp+mL2Xy1pvqS5kt6T1LGsPD3IOedioyiXsrKR0oExQH8gCxgqKatIss+Aw82sB/AScEdZ+XqQc87FRFH+i8KRwCIzW2xmuwimHBwYmcDM/mNmP4SrHxNMW1gqD3LOuZhI0S1ApqSZEcuwIlm1BZZHrK8It5XkIuDNssrnHQ/OuZiUo+Mh28wOj8s5pV8BhwMnlJXWg5xzLjbx615dCbSPWG8Xbit8umDe1T8DJ5jZzrIy9SDnnKswKa7vrs4AukjqTBDchgDnFj6fegIPA/3MbF00mfo9OedcTOLUuYqZ5QKXA28DC4DxZjZP0s2SBoTJ7gTqAy9KmiNpYln5ek3OORebOD4NbGaTgclFto2M+HxqefP0IOeci4G/u+qcS3H+7qpzLmUJD3LOuRTnzVXnXErzmlw18PHsr7nvsTfIy8/nzL5H8OuzCz9E/dm877j/8Ul8u2QNN137S04+tjsAs774lgce39MRtHTlem66ZggnHF30neLk9e60+dxw90vk5edz/sBjueqC0wrt37krh0tGjWXOV8to2qgeT9x6IR3aNAPgniff5pmJ00lPS+O2a8/hlGOyosrzj3e9yLMTp7Ni6j0A/G/2Iv50z0vMW7SKx2/5LQNP6VkFVx5fpxx9EH//w1mkp6cxduLH3Df2/UL727VszEN/OZdGDWqTnpbGTQ9NYsr0BZx4RFdGXXoGNWtksCsnl5H/eJ0PZy1K0FVUTJLHuKp7Tk7SE5LWSfqyqs4Zjby8fO56eCJ3j7yA5x78A+9++DnfLV9bKE2rzMbcOPxs+vY5tND23t3359/3XcG/77uCB/96EbVq1eCongdUZfFjkpeXz3V3jOfF+y/l4/E38vI7s/hq8epCaca+Np1GDesw+5XRXHLuSYx+8DUAvlq8mglTZjP9hT/z0gOXcu3t48nLyy8zz8/mL2Xz1h8KnaN9qyaMGXU+5/w0Lm/8VLm0NHHnNYP4xdWPcPTQ2zm7by8O7NSyUJprLujLq+/N4YTf3MNFfxnLXdedDcCGLd8z9LrHOe5Xd3LpX8fxr1HnJeISKi7ah+QSGAmr8mHgp4B+VXi+qMz/ZgXtWjejbaum1KiRwak/6cGHnywolKZ1yyYc0Kl1qU92vz/tS47p1ZXatWpWdpHjZta8JezXPpNO7TKpWSODQX17MfmDuYXSvDl1LkPPOAqAgSf35IMZCzEzJn8wl0F9e1GrZg06ts1kv/aZzJq3pNQ88/LyGfnAq9w0/KxC5+jQphmHdGmb9LM+laR3VgcWr8hm6aqN5OTmMeHdzzi9zyF7pWtQrzYADevXZk32FgC++Hola7K3ArBg8Rrq1KpBzRrpVVf4OIjjKCSVosqCnJlNBTZW1fmitX7jFlpmNtq93rxZI9Zv3FrufN79aC59jz+07IRJZPX6LbRt2WT3epuWTVi9fkuhNKvW7UmTkZFOw/p12Ljl+72PbREcW1qej47/gP59utMq4vtOBa2bN2Llus2711et20zr5oWv8bbH3mJwv958+dpIxt99MSPufmWvfAac1IPPF65gV05eZRc5bgomsolmSRR/rSsOsjduZfHSNRzVs0uii5K0Vq/fzKvvfcawwWUOGpGSzu7bi+cmfcohA29m8DWP8q9R56KImutBnVsy+tKfcdXtLyawlBXkzdXoSRpWMNbUpo0bquSczZs2Ym32ntrL+g1baN60YbnyeO9/X9DnqIPJyKhezYzWzRuxcu2m3eur1m7aqwbSpsWeNLm5eWzdvoOmjertfey64NiS8py7cAXfLV9Pr0E30WPASH74MYdePx9duRdYRVav30LbFo13r7dp0XivGvGvzjyKV9/7HIAZXy6lds0aNGtcL0jfvBFjb/stl/z1OZasrJqf+3jy5mo5mNkjZna4mR3epGmzKjlnty5tWbE6m1VrN5KTk8u7H83lJ0d2K1ce7344l759elRSCStPr6yOfLtsPUtXZrMrJ5cJU2bTv8h19Du+O+MmfQLAa+9/Rp8juiKJ/n16MGHKbHbuymHpymy+Xbae3gd3KjHPn/7kEBa+/XfmTryZuRNvpm7tGsx+ZXQCrjr+Zi9Yzv7tm9OhdVNqZKQz6NSevPlh4f61lWs30efwoKbftWMLatXMIHvTdhrWr80Ld1/MTQ9N4pO5SxJQ+tiVY9DMhNjnHyHJSE/n6osHcNVNT5KXZ/zs1N7s16Eljz43hYMOaMfxR3Zj/jcruOG2Z9i2fQcfzVzA4+Pe49kH/wDA6rWbWJu9hZ4Hd07shVRARkY6d4wYzNnDx5CXZ5w34Gi67d+aW//1Bod168DpJ/Tg/IHH8vtRT9Pr56Np0rAej9/yWwC67d+as07tydGDbyEjPY07RwwmPT34m1lcnqWZPW8p5494lM1bf+Ctj77gtocnMX38jZV+/fGSl5fPiLsn8PJ9w0hPS+PZNz7lq+/WcsPF/ZizYDlvfjSPGx+YyP03DObSISdgZlz2t3EAXHzOT+jcrhkjLjyNERcGj9oM+sPDZG/anshLKpdk7y6SmVXNiaRxwIlAJrAWGGVmj5eUPqtHT3v29Q+qpGzVUbe25WtS74uaHHN1oouQ9H6cee+sWEbrPeTQXjbhnY+iSntgq3oxnauiqqwmZ2ZDq+pczrmqEedBMyvFPt9cdc7FJrlDnAc551yskjzKJVXvqnOuuon2AZLoIqGkfpIWSlok6fpi9veRNFtSrqRzosnTg5xzLibxeoREUjowBugPZAFDJRUd7WIZcAHwXLTl8+aqc67C4jxo5pHAIjNbDCDpeWAgML8ggZktCfflR5up1+ScczEpR3M1s+CNpnAZViSrtsDyiPUV4baYeE3OOReTctTkslP6OTnnXGqKY+fqSqB9xHq7cFtMvLnqnKu4KDsdoqztzQC6SOosqSYwBChz8uiyeJBzzsUoPmMtmVkucDnwNrAAGG9m8yTdLGkAgKQjJK0AfgE8LGleWfl6c9U5V2EFg2bGi5lNBiYX2TYy4vMMgmZs1DzIOedikuSvrnqQc87Fxudddc6ltuSOcR7knHOxSfIY50HOOVdxiR7aPBoe5JxzMVGSRzkPcs65mCR3iPMg55yLUZJX5DzIOedikdg5VaPhQc45V2FxHk+uUniQc87FxIOccy6leXPVOZe6/Dk551wqi24QpcTyIOeci02SRzkPcs65mPg9OedcSovnoJmVwYOccy42HuScc6nMm6vOuZRVHd54kJklugzFkrQeWJrockTIBLITXYgk599R6ZLx++loZs0rerCktwiuKxrZZtavoueqqKQNcslG0sxEzP5dnfh3VDr/fhLD5111zqU0D3LOuZTmQS56jyS6ANWAf0el8+8nAfyenHMupXlNzjmX0jzIOedSmge5KEg6SNJ0STslXZvo8iQbSf0kLZS0SNL1iS5PspH0hKR1kr5MdFn2RR7korMRGA7cleiCJBtJ6cAYoD+QBQyVlJXYUiWdp4AqfwjWBTzIRcHM1pnZDCAn0WVJQkcCi8xssZntAp4HBia4TEnFzKYS/KF0CeBBzsWqLbA8Yn1FuM25pOBBzjmX0jzIlUDSZZLmhEubRJcnia0E2kestwu3OZcUPMiVwMzGmNlh4bIq0eVJYjOALpI6S6oJDAEmJrhMzu3mbzxEQVIrYCbQEMgHtgNZZrY1oQVLEpJOB+4D0oEnzOyWxJYouUgaB5xIMCTRWmCUmT2e0ELtQzzIOedSmjdXnXMpzYOccy6leZBzzqU0D3LOuZTmQc45l9I8yFVjkvLCh5W/lPSipLox5PWUpHPCz4+V9pK9pBMlHVuBcyyRtNfMTiVtL5JmeznPNdpHjHHgQa662xE+rHwIsAv4feROSRWaV9fM/s/M5peS5ESg3EHOuUTwIJc6PgQOCGtZH0qaCMyXlC7pTkkzJM2V9DsABf4RjgP3LtCiICNJ/5V0ePi5n6TZkj6X9J6kTgTB9KqwFnm8pOaSXg7PMUPSceGxzSS9I2mepMeg7KnWJb0qaVZ4zLAi++4Nt78nqXm4bX9Jb4XHfCjpoLh8my5lVOgvvUsuYY2tP/BWuKkXcIiZfRcGii1mdoSkWsD/JL0D9AQOJBgDriUwH3iiSL7NgUeBPmFeTc1so6R/AdvN7K4w3XPAvWb2kaQOwNtAN2AU8JGZ3SzpDOCiKC7nwvAcdYAZkl42sw1APWCmmV0laWSY9+UEk8P83sy+kXQU8BBwcgW+RpeiPMhVb3UkzQk/fwg8TtCM/NTMvgu3nwb0KLjfBjQCugB9gHFmlgeskvR+MfkfDUwtyMvMShoT7VQgS9pdUWsoqX54jkHhsZMkbYrimoZL+nn4uX1Y1g0Er9O9EG5/BpgQnuNY4MWIc9eK4hxuH+JBrnrbYWaHRW4If9m/j9wEXGFmbxdJd3ocy5EGHG1mPxZTlqhJOpEgYB5jZj9I+i9Qu4TkFp53c9HvwLlIfk8u9b0NXCKpBoCkrpLqAVOBX4b37FoDJxVz7MdAH0mdw2Obhtu3AQ0i0r0DXFGwIumw8ONU4NxwW3+gSRllbQRsCgPcQQQ1yQJpQEFt9FyCZvBW4DtJvwjPIUmHlnEOt4/xIJf6HiO43zY7nEjlYYIa/CvAN+G+p4HpRQ80s/XAMIKm4efsaS6+Dvy8oOOBYP6Lw8OOjfns6eW9iSBIziNoti4ro6xvARmSFgC3EQTZAt8DR4bXcDJwc7j9POCisHzz8KHXXRE+ColzLqV5Tc45l9I8yDnnUpoHOedcSvMg55xLaR7knHMpzYOccy6leZBzzqW0/wdXHFUXdtH2ogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time \n",
    "# Starting with Sklearn MLP\n",
    "# Training model\n",
    "mlp_clf = MLPClassifier(random_state=random_state, max_iter=300, verbose=10)\n",
    "mlp_clf.fit(training_df_scaled_X,training_df[y_col])\n",
    "\n",
    "# Testing model\n",
    "mlp_acc,mlp_f1 = test_model_metrics(mlp_clf,\"MLP-scaled\",testing_df_scaled_X,testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    mlp_clf,\n",
    "    testing_df_scaled_X,\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"MLP Confusion Matrix-scaled\")\n",
    "\n",
    "print(\"MLP Confusion Matrix-scaled\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc2744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63144775\n",
      "Iteration 2, loss = 0.54512569\n",
      "Iteration 3, loss = 0.45330666\n",
      "Iteration 4, loss = 0.39835150\n",
      "Iteration 5, loss = 0.37696999\n",
      "Iteration 6, loss = 0.36801918\n",
      "Iteration 7, loss = 0.36305321\n",
      "Iteration 8, loss = 0.35949449\n",
      "Iteration 9, loss = 0.35678872\n",
      "Iteration 10, loss = 0.35473460\n",
      "Iteration 11, loss = 0.35340710\n",
      "Iteration 12, loss = 0.35176888\n",
      "Iteration 13, loss = 0.35076742\n",
      "Iteration 14, loss = 0.34945714\n",
      "Iteration 15, loss = 0.34864483\n",
      "Iteration 16, loss = 0.34731462\n",
      "Iteration 17, loss = 0.34642831\n",
      "Iteration 18, loss = 0.34489974\n",
      "Iteration 19, loss = 0.34420313\n",
      "Iteration 20, loss = 0.34346078\n",
      "Iteration 21, loss = 0.34273646\n",
      "Iteration 22, loss = 0.34168430\n",
      "Iteration 23, loss = 0.34086170\n",
      "Iteration 24, loss = 0.34002766\n",
      "Iteration 25, loss = 0.33885551\n",
      "Iteration 26, loss = 0.33844845\n",
      "Iteration 27, loss = 0.33781648\n",
      "Iteration 28, loss = 0.33645535\n",
      "Iteration 29, loss = 0.33637092\n",
      "Iteration 30, loss = 0.33535660\n",
      "Iteration 31, loss = 0.33466422\n",
      "Iteration 32, loss = 0.33391219\n",
      "Iteration 33, loss = 0.33352328\n",
      "Iteration 34, loss = 0.33230026\n",
      "Iteration 35, loss = 0.33184326\n",
      "Iteration 36, loss = 0.33130708\n",
      "Iteration 37, loss = 0.33060744\n",
      "Iteration 38, loss = 0.32966232\n",
      "Iteration 39, loss = 0.32915105\n",
      "Iteration 40, loss = 0.32887033\n",
      "Iteration 41, loss = 0.32811926\n",
      "Iteration 42, loss = 0.32739097\n",
      "Iteration 43, loss = 0.32726783\n",
      "Iteration 44, loss = 0.32608094\n",
      "Iteration 45, loss = 0.32587837\n",
      "Iteration 46, loss = 0.32523841\n",
      "Iteration 47, loss = 0.32418736\n",
      "Iteration 48, loss = 0.32402791\n",
      "Iteration 49, loss = 0.32309551\n",
      "Iteration 50, loss = 0.32271585\n",
      "Iteration 51, loss = 0.32251330\n",
      "Iteration 52, loss = 0.32154602\n",
      "Iteration 53, loss = 0.32105296\n",
      "Iteration 54, loss = 0.32043285\n",
      "Iteration 55, loss = 0.32013845\n",
      "Iteration 56, loss = 0.31956859\n",
      "Iteration 57, loss = 0.31850971\n",
      "Iteration 58, loss = 0.31890584\n",
      "Iteration 59, loss = 0.31749493\n",
      "Iteration 60, loss = 0.31732636\n",
      "Iteration 61, loss = 0.31679501\n",
      "Iteration 62, loss = 0.31616851\n",
      "Iteration 63, loss = 0.31542681\n",
      "Iteration 64, loss = 0.31525847\n",
      "Iteration 65, loss = 0.31478356\n",
      "Iteration 66, loss = 0.31431514\n",
      "Iteration 67, loss = 0.31367166\n",
      "Iteration 68, loss = 0.31358164\n",
      "Iteration 69, loss = 0.31281227\n",
      "Iteration 70, loss = 0.31324376\n",
      "Iteration 71, loss = 0.31195901\n",
      "Iteration 72, loss = 0.31142718\n",
      "Iteration 73, loss = 0.31119483\n",
      "Iteration 74, loss = 0.31097614\n",
      "Iteration 75, loss = 0.31031930\n",
      "Iteration 76, loss = 0.31083529\n",
      "Iteration 77, loss = 0.30999194\n",
      "Iteration 78, loss = 0.30966036\n",
      "Iteration 79, loss = 0.30959750\n",
      "Iteration 80, loss = 0.30863623\n",
      "Iteration 81, loss = 0.30834779\n",
      "Iteration 82, loss = 0.30824483\n",
      "Iteration 83, loss = 0.30820116\n",
      "Iteration 84, loss = 0.30748235\n",
      "Iteration 85, loss = 0.30695496\n",
      "Iteration 86, loss = 0.30690568\n",
      "Iteration 87, loss = 0.30705986\n",
      "Iteration 88, loss = 0.30638097\n",
      "Iteration 89, loss = 0.30587025\n",
      "Iteration 90, loss = 0.30552554\n",
      "Iteration 91, loss = 0.30541545\n",
      "Iteration 92, loss = 0.30507212\n",
      "Iteration 93, loss = 0.30441268\n",
      "Iteration 94, loss = 0.30465300\n",
      "Iteration 95, loss = 0.30388279\n",
      "Iteration 96, loss = 0.30380872\n",
      "Iteration 97, loss = 0.30390268\n",
      "Iteration 98, loss = 0.30333227\n",
      "Iteration 99, loss = 0.30301097\n",
      "Iteration 100, loss = 0.30335320\n",
      "Iteration 101, loss = 0.30255069\n",
      "Iteration 102, loss = 0.30251895\n",
      "Iteration 103, loss = 0.30268968\n",
      "Iteration 104, loss = 0.30182213\n",
      "Iteration 105, loss = 0.30237697\n",
      "Iteration 106, loss = 0.30082686\n",
      "Iteration 107, loss = 0.30094757\n",
      "Iteration 108, loss = 0.30098382\n",
      "Iteration 109, loss = 0.30039238\n",
      "Iteration 110, loss = 0.30060634\n",
      "Iteration 111, loss = 0.29995948\n",
      "Iteration 112, loss = 0.30010468\n",
      "Iteration 113, loss = 0.29962926\n",
      "Iteration 114, loss = 0.29977416\n",
      "Iteration 115, loss = 0.29945543\n",
      "Iteration 116, loss = 0.29902409\n",
      "Iteration 117, loss = 0.29826854\n",
      "Iteration 118, loss = 0.29835666\n",
      "Iteration 119, loss = 0.29871295\n",
      "Iteration 120, loss = 0.29869222\n",
      "Iteration 121, loss = 0.29827699\n",
      "Iteration 122, loss = 0.29768980\n",
      "Iteration 123, loss = 0.29769454\n",
      "Iteration 124, loss = 0.29720840\n",
      "Iteration 125, loss = 0.29700672\n",
      "Iteration 126, loss = 0.29748893\n",
      "Iteration 127, loss = 0.29660135\n",
      "Iteration 128, loss = 0.29614567\n",
      "Iteration 129, loss = 0.29660061\n",
      "Iteration 130, loss = 0.29656151\n",
      "Iteration 131, loss = 0.29619715\n",
      "Iteration 132, loss = 0.29570027\n",
      "Iteration 133, loss = 0.29538223\n",
      "Iteration 134, loss = 0.29524257\n",
      "Iteration 135, loss = 0.29509102\n",
      "Iteration 136, loss = 0.29513802\n",
      "Iteration 137, loss = 0.29482788\n",
      "Iteration 138, loss = 0.29538345\n",
      "Iteration 139, loss = 0.29480470\n",
      "Iteration 140, loss = 0.29455039\n",
      "Iteration 141, loss = 0.29401162\n",
      "Iteration 142, loss = 0.29457443\n",
      "Iteration 143, loss = 0.29417841\n",
      "Iteration 144, loss = 0.29425518\n",
      "Iteration 145, loss = 0.29354811\n",
      "Iteration 146, loss = 0.29325041\n",
      "Iteration 147, loss = 0.29328806\n",
      "Iteration 148, loss = 0.29318345\n",
      "Iteration 149, loss = 0.29343368\n",
      "Iteration 150, loss = 0.29269798\n",
      "Iteration 151, loss = 0.29286101\n",
      "Iteration 152, loss = 0.29267144\n",
      "Iteration 153, loss = 0.29256926\n",
      "Iteration 154, loss = 0.29176735\n",
      "Iteration 155, loss = 0.29243941\n",
      "Iteration 156, loss = 0.29185493\n",
      "Iteration 157, loss = 0.29213026\n",
      "Iteration 158, loss = 0.29131278\n",
      "Iteration 159, loss = 0.29140362\n",
      "Iteration 160, loss = 0.29185796\n",
      "Iteration 161, loss = 0.29156804\n",
      "Iteration 162, loss = 0.29127174\n",
      "Iteration 163, loss = 0.29107876\n",
      "Iteration 164, loss = 0.29084192\n",
      "Iteration 165, loss = 0.29073523\n",
      "Iteration 166, loss = 0.29082228\n",
      "Iteration 167, loss = 0.29091034\n",
      "Iteration 168, loss = 0.29042557\n",
      "Iteration 169, loss = 0.29020325\n",
      "Iteration 170, loss = 0.29009096\n",
      "Iteration 171, loss = 0.28955046\n",
      "Iteration 172, loss = 0.28964656\n",
      "Iteration 173, loss = 0.29015070\n",
      "Iteration 174, loss = 0.28970656\n",
      "Iteration 175, loss = 0.28957431\n",
      "Iteration 176, loss = 0.28880517\n",
      "Iteration 177, loss = 0.28924621\n",
      "Iteration 178, loss = 0.28889715\n",
      "Iteration 179, loss = 0.28889543\n",
      "Iteration 180, loss = 0.28892613\n",
      "Iteration 181, loss = 0.28863282\n",
      "Iteration 182, loss = 0.28816658\n",
      "Iteration 183, loss = 0.28876176\n",
      "Iteration 184, loss = 0.28892402\n",
      "Iteration 185, loss = 0.28864684\n",
      "Iteration 186, loss = 0.28815646\n",
      "Iteration 187, loss = 0.28793082\n",
      "Iteration 188, loss = 0.28807789\n",
      "Iteration 189, loss = 0.28784414\n",
      "Iteration 190, loss = 0.28715480\n",
      "Iteration 191, loss = 0.28702057\n",
      "Iteration 192, loss = 0.28785169\n",
      "Iteration 193, loss = 0.28751356\n",
      "Iteration 194, loss = 0.28694426\n",
      "Iteration 195, loss = 0.28736989\n",
      "Iteration 196, loss = 0.28656958\n",
      "Iteration 197, loss = 0.28662808\n",
      "Iteration 198, loss = 0.28641069\n",
      "Iteration 199, loss = 0.28695853\n",
      "Iteration 200, loss = 0.28693735\n",
      "Iteration 201, loss = 0.28614122\n",
      "Iteration 202, loss = 0.28680115\n",
      "Iteration 203, loss = 0.28584496\n",
      "Iteration 204, loss = 0.28573309\n",
      "Iteration 205, loss = 0.28600176\n",
      "Iteration 206, loss = 0.28552325\n",
      "Iteration 207, loss = 0.28553672\n",
      "Iteration 208, loss = 0.28504088\n",
      "Iteration 209, loss = 0.28569601\n",
      "Iteration 210, loss = 0.28478293\n",
      "Iteration 211, loss = 0.28540210\n",
      "Iteration 212, loss = 0.28577435\n",
      "Iteration 213, loss = 0.28462108\n",
      "Iteration 214, loss = 0.28505474\n",
      "Iteration 215, loss = 0.28494195\n",
      "Iteration 216, loss = 0.28405666\n",
      "Iteration 217, loss = 0.28464948\n",
      "Iteration 218, loss = 0.28404417\n",
      "Iteration 219, loss = 0.28441938\n",
      "Iteration 220, loss = 0.28348706\n",
      "Iteration 221, loss = 0.28337570\n",
      "Iteration 222, loss = 0.28359122\n",
      "Iteration 223, loss = 0.28386651\n",
      "Iteration 224, loss = 0.28384915\n",
      "Iteration 225, loss = 0.28337675\n",
      "Iteration 226, loss = 0.28329779\n",
      "Iteration 227, loss = 0.28347766\n",
      "Iteration 228, loss = 0.28323321\n",
      "Iteration 229, loss = 0.28249169\n",
      "Iteration 230, loss = 0.28282186\n",
      "Iteration 231, loss = 0.28311546\n",
      "Iteration 232, loss = 0.28309071\n",
      "Iteration 233, loss = 0.28306444\n",
      "Iteration 234, loss = 0.28275691\n",
      "Iteration 235, loss = 0.28242638\n",
      "Iteration 236, loss = 0.28200700\n",
      "Iteration 237, loss = 0.28231406\n",
      "Iteration 238, loss = 0.28261226\n",
      "Iteration 239, loss = 0.28217544\n",
      "Iteration 240, loss = 0.28197570\n",
      "Iteration 241, loss = 0.28172706\n",
      "Iteration 242, loss = 0.28134618\n",
      "Iteration 243, loss = 0.28138601\n",
      "Iteration 244, loss = 0.28178218\n",
      "Iteration 245, loss = 0.28112446\n",
      "Iteration 246, loss = 0.28089091\n",
      "Iteration 247, loss = 0.28082341\n",
      "Iteration 248, loss = 0.28097059\n",
      "Iteration 249, loss = 0.28084434\n",
      "Iteration 250, loss = 0.28099476\n",
      "Iteration 251, loss = 0.28058195\n",
      "Iteration 1, loss = 0.63163798\n",
      "Iteration 2, loss = 0.54502261\n",
      "Iteration 3, loss = 0.45317602\n",
      "Iteration 4, loss = 0.39880882\n",
      "Iteration 5, loss = 0.37797192\n",
      "Iteration 6, loss = 0.36890480\n",
      "Iteration 7, loss = 0.36379702\n",
      "Iteration 8, loss = 0.36049693\n",
      "Iteration 9, loss = 0.35745959\n",
      "Iteration 10, loss = 0.35575697\n",
      "Iteration 11, loss = 0.35416229\n",
      "Iteration 12, loss = 0.35245420\n",
      "Iteration 13, loss = 0.35108429\n",
      "Iteration 14, loss = 0.35018897\n",
      "Iteration 15, loss = 0.34868728\n",
      "Iteration 16, loss = 0.34807040\n",
      "Iteration 17, loss = 0.34667372\n",
      "Iteration 18, loss = 0.34543841\n",
      "Iteration 19, loss = 0.34456562\n",
      "Iteration 20, loss = 0.34381433\n",
      "Iteration 21, loss = 0.34295529\n",
      "Iteration 22, loss = 0.34196955\n",
      "Iteration 23, loss = 0.34136042\n",
      "Iteration 24, loss = 0.34044179\n",
      "Iteration 25, loss = 0.33953511\n",
      "Iteration 26, loss = 0.33852337\n",
      "Iteration 27, loss = 0.33792050\n",
      "Iteration 28, loss = 0.33731875\n",
      "Iteration 29, loss = 0.33622686\n",
      "Iteration 30, loss = 0.33586908\n",
      "Iteration 31, loss = 0.33473257\n",
      "Iteration 32, loss = 0.33445565\n",
      "Iteration 33, loss = 0.33402112\n",
      "Iteration 34, loss = 0.33327360\n",
      "Iteration 35, loss = 0.33245732\n",
      "Iteration 36, loss = 0.33166730\n",
      "Iteration 37, loss = 0.33109904\n",
      "Iteration 38, loss = 0.33037710\n",
      "Iteration 39, loss = 0.32967904\n",
      "Iteration 40, loss = 0.32966197\n",
      "Iteration 41, loss = 0.32878556\n",
      "Iteration 42, loss = 0.32837868\n",
      "Iteration 43, loss = 0.32789917\n",
      "Iteration 44, loss = 0.32743595\n",
      "Iteration 45, loss = 0.32683075\n",
      "Iteration 46, loss = 0.32597199\n",
      "Iteration 47, loss = 0.32531597\n",
      "Iteration 48, loss = 0.32483242\n",
      "Iteration 49, loss = 0.32406442\n",
      "Iteration 50, loss = 0.32394817\n",
      "Iteration 51, loss = 0.32315396\n",
      "Iteration 52, loss = 0.32291955\n",
      "Iteration 53, loss = 0.32241097\n",
      "Iteration 54, loss = 0.32192204\n",
      "Iteration 55, loss = 0.32136123\n",
      "Iteration 56, loss = 0.32100925\n",
      "Iteration 57, loss = 0.32024481\n",
      "Iteration 58, loss = 0.32000839\n",
      "Iteration 59, loss = 0.31936364\n",
      "Iteration 60, loss = 0.31896540\n",
      "Iteration 61, loss = 0.31810624\n",
      "Iteration 62, loss = 0.31758106\n",
      "Iteration 63, loss = 0.31698392\n",
      "Iteration 64, loss = 0.31687739\n",
      "Iteration 65, loss = 0.31616849\n",
      "Iteration 66, loss = 0.31609052\n",
      "Iteration 67, loss = 0.31543495\n",
      "Iteration 68, loss = 0.31465378\n",
      "Iteration 69, loss = 0.31473561\n",
      "Iteration 70, loss = 0.31422005\n",
      "Iteration 71, loss = 0.31360273\n",
      "Iteration 72, loss = 0.31308339\n",
      "Iteration 73, loss = 0.31266935\n",
      "Iteration 74, loss = 0.31250154\n",
      "Iteration 75, loss = 0.31162703\n",
      "Iteration 76, loss = 0.31187317\n",
      "Iteration 77, loss = 0.31065323\n",
      "Iteration 78, loss = 0.31106111\n",
      "Iteration 79, loss = 0.31045105\n",
      "Iteration 80, loss = 0.31005082\n",
      "Iteration 81, loss = 0.30980720\n",
      "Iteration 82, loss = 0.30993564\n",
      "Iteration 83, loss = 0.30977536\n",
      "Iteration 84, loss = 0.30872082\n",
      "Iteration 85, loss = 0.30907066\n",
      "Iteration 86, loss = 0.30806781\n",
      "Iteration 87, loss = 0.30776472\n",
      "Iteration 88, loss = 0.30788833\n",
      "Iteration 89, loss = 0.30733314\n",
      "Iteration 90, loss = 0.30713527\n",
      "Iteration 91, loss = 0.30637756\n",
      "Iteration 92, loss = 0.30681060\n",
      "Iteration 93, loss = 0.30644383\n",
      "Iteration 94, loss = 0.30584421\n",
      "Iteration 95, loss = 0.30554752\n",
      "Iteration 96, loss = 0.30486469\n",
      "Iteration 97, loss = 0.30453757\n",
      "Iteration 98, loss = 0.30452326\n",
      "Iteration 99, loss = 0.30495797\n",
      "Iteration 100, loss = 0.30392929\n",
      "Iteration 101, loss = 0.30442687\n",
      "Iteration 102, loss = 0.30409773\n",
      "Iteration 103, loss = 0.30319973\n",
      "Iteration 104, loss = 0.30307754\n",
      "Iteration 105, loss = 0.30280427\n",
      "Iteration 106, loss = 0.30225676\n",
      "Iteration 107, loss = 0.30222287\n",
      "Iteration 108, loss = 0.30217216\n",
      "Iteration 109, loss = 0.30164511\n",
      "Iteration 110, loss = 0.30157336\n",
      "Iteration 111, loss = 0.30135682\n",
      "Iteration 112, loss = 0.30082547\n",
      "Iteration 113, loss = 0.30123493\n",
      "Iteration 114, loss = 0.30050270\n",
      "Iteration 115, loss = 0.30058782\n",
      "Iteration 116, loss = 0.30083284\n",
      "Iteration 117, loss = 0.29955545\n",
      "Iteration 118, loss = 0.29968788\n",
      "Iteration 119, loss = 0.29977843\n",
      "Iteration 120, loss = 0.29965134\n",
      "Iteration 121, loss = 0.29911176\n",
      "Iteration 122, loss = 0.29914395\n",
      "Iteration 123, loss = 0.29892772\n",
      "Iteration 124, loss = 0.29817146\n",
      "Iteration 125, loss = 0.29800822\n",
      "Iteration 126, loss = 0.29794743\n",
      "Iteration 127, loss = 0.29812438\n",
      "Iteration 128, loss = 0.29742294\n",
      "Iteration 129, loss = 0.29718728\n",
      "Iteration 130, loss = 0.29709141\n",
      "Iteration 131, loss = 0.29758424\n",
      "Iteration 132, loss = 0.29734478\n",
      "Iteration 133, loss = 0.29695247\n",
      "Iteration 134, loss = 0.29643326\n",
      "Iteration 135, loss = 0.29613464\n",
      "Iteration 136, loss = 0.29547843\n",
      "Iteration 137, loss = 0.29603601\n",
      "Iteration 138, loss = 0.29571571\n",
      "Iteration 139, loss = 0.29594371\n",
      "Iteration 140, loss = 0.29580031\n",
      "Iteration 141, loss = 0.29485632\n",
      "Iteration 142, loss = 0.29502756\n",
      "Iteration 143, loss = 0.29497623\n",
      "Iteration 144, loss = 0.29463648\n",
      "Iteration 145, loss = 0.29444961\n",
      "Iteration 146, loss = 0.29469324\n",
      "Iteration 147, loss = 0.29394837\n",
      "Iteration 148, loss = 0.29362736\n",
      "Iteration 149, loss = 0.29450420\n",
      "Iteration 150, loss = 0.29376516\n",
      "Iteration 151, loss = 0.29336841\n",
      "Iteration 152, loss = 0.29330666\n",
      "Iteration 153, loss = 0.29329564\n",
      "Iteration 154, loss = 0.29343550\n",
      "Iteration 155, loss = 0.29372684\n",
      "Iteration 156, loss = 0.29275314\n",
      "Iteration 157, loss = 0.29302391\n",
      "Iteration 158, loss = 0.29192266\n",
      "Iteration 159, loss = 0.29213936\n",
      "Iteration 160, loss = 0.29290043\n",
      "Iteration 161, loss = 0.29188592\n",
      "Iteration 162, loss = 0.29172974\n",
      "Iteration 163, loss = 0.29178926\n",
      "Iteration 164, loss = 0.29161259\n",
      "Iteration 165, loss = 0.29142630\n",
      "Iteration 166, loss = 0.29087965\n",
      "Iteration 167, loss = 0.29106331\n",
      "Iteration 168, loss = 0.29102796\n",
      "Iteration 169, loss = 0.29139432\n",
      "Iteration 170, loss = 0.29089547\n",
      "Iteration 171, loss = 0.29101111\n",
      "Iteration 172, loss = 0.29093785\n",
      "Iteration 173, loss = 0.29013448\n",
      "Iteration 174, loss = 0.29042373\n",
      "Iteration 175, loss = 0.29067207\n",
      "Iteration 176, loss = 0.29007542\n",
      "Iteration 177, loss = 0.29008069\n",
      "Iteration 178, loss = 0.29017134\n",
      "Iteration 179, loss = 0.28940084\n",
      "Iteration 180, loss = 0.28986272\n",
      "Iteration 181, loss = 0.28952160\n",
      "Iteration 182, loss = 0.28948002\n",
      "Iteration 183, loss = 0.28919414\n",
      "Iteration 184, loss = 0.28873929\n",
      "Iteration 185, loss = 0.28935579\n",
      "Iteration 186, loss = 0.28889306\n",
      "Iteration 187, loss = 0.28850872\n",
      "Iteration 188, loss = 0.28864533\n",
      "Iteration 189, loss = 0.28847318\n",
      "Iteration 190, loss = 0.28784289\n",
      "Iteration 191, loss = 0.28814304\n",
      "Iteration 192, loss = 0.28840016\n",
      "Iteration 193, loss = 0.28780044\n",
      "Iteration 194, loss = 0.28751745\n",
      "Iteration 195, loss = 0.28794203\n",
      "Iteration 196, loss = 0.28703567\n",
      "Iteration 197, loss = 0.28689944\n",
      "Iteration 198, loss = 0.28684928\n",
      "Iteration 199, loss = 0.28689329\n",
      "Iteration 200, loss = 0.28671224\n",
      "Iteration 201, loss = 0.28673360\n",
      "Iteration 202, loss = 0.28686069\n",
      "Iteration 203, loss = 0.28584036\n",
      "Iteration 204, loss = 0.28603774\n",
      "Iteration 205, loss = 0.28624484\n",
      "Iteration 206, loss = 0.28668848\n",
      "Iteration 207, loss = 0.28581602\n",
      "Iteration 208, loss = 0.28635199\n",
      "Iteration 209, loss = 0.28581144\n",
      "Iteration 210, loss = 0.28537775\n",
      "Iteration 211, loss = 0.28525664\n",
      "Iteration 212, loss = 0.28578784\n",
      "Iteration 213, loss = 0.28552089\n",
      "Iteration 214, loss = 0.28554781\n",
      "Iteration 215, loss = 0.28487218\n",
      "Iteration 216, loss = 0.28446633\n",
      "Iteration 217, loss = 0.28453824\n",
      "Iteration 218, loss = 0.28443601\n",
      "Iteration 219, loss = 0.28474106\n",
      "Iteration 220, loss = 0.28425175\n",
      "Iteration 221, loss = 0.28443789\n",
      "Iteration 222, loss = 0.28476140\n",
      "Iteration 223, loss = 0.28428988\n",
      "Iteration 224, loss = 0.28373552\n",
      "Iteration 225, loss = 0.28398097\n",
      "Iteration 226, loss = 0.28374752\n",
      "Iteration 227, loss = 0.28398831\n",
      "Iteration 228, loss = 0.28351474\n",
      "Iteration 229, loss = 0.28272775\n",
      "Iteration 230, loss = 0.28246097\n",
      "Iteration 231, loss = 0.28329118\n",
      "Iteration 232, loss = 0.28286595\n",
      "Iteration 233, loss = 0.28280995\n",
      "Iteration 234, loss = 0.28324616\n",
      "Iteration 235, loss = 0.28292343\n",
      "Iteration 236, loss = 0.28242716\n",
      "Iteration 237, loss = 0.28224495\n",
      "Iteration 238, loss = 0.28247430\n",
      "Iteration 239, loss = 0.28236788\n",
      "Iteration 240, loss = 0.28182755\n",
      "Iteration 241, loss = 0.28197373\n",
      "Iteration 242, loss = 0.28207034\n",
      "Iteration 243, loss = 0.28173198\n",
      "Iteration 244, loss = 0.28183564\n",
      "Iteration 245, loss = 0.28140909\n",
      "Iteration 246, loss = 0.28111272\n",
      "Iteration 247, loss = 0.28077948\n",
      "Iteration 248, loss = 0.28138282\n",
      "Iteration 249, loss = 0.28112170\n",
      "Iteration 250, loss = 0.28094810\n",
      "Iteration 251, loss = 0.28035402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63169453\n",
      "Iteration 2, loss = 0.54499864\n",
      "Iteration 3, loss = 0.45303484\n",
      "Iteration 4, loss = 0.39845655\n",
      "Iteration 5, loss = 0.37696366\n",
      "Iteration 6, loss = 0.36851589\n",
      "Iteration 7, loss = 0.36332825\n",
      "Iteration 8, loss = 0.35977827\n",
      "Iteration 9, loss = 0.35740088\n",
      "Iteration 10, loss = 0.35586347\n",
      "Iteration 11, loss = 0.35349730\n",
      "Iteration 12, loss = 0.35233248\n",
      "Iteration 13, loss = 0.35077033\n",
      "Iteration 14, loss = 0.34964713\n",
      "Iteration 15, loss = 0.34838108\n",
      "Iteration 16, loss = 0.34755449\n",
      "Iteration 17, loss = 0.34658101\n",
      "Iteration 18, loss = 0.34549165\n",
      "Iteration 19, loss = 0.34444446\n",
      "Iteration 20, loss = 0.34348907\n",
      "Iteration 21, loss = 0.34289409\n",
      "Iteration 22, loss = 0.34177000\n",
      "Iteration 23, loss = 0.34117601\n",
      "Iteration 24, loss = 0.34027755\n",
      "Iteration 25, loss = 0.33879621\n",
      "Iteration 26, loss = 0.33853807\n",
      "Iteration 27, loss = 0.33794820\n",
      "Iteration 28, loss = 0.33708337\n",
      "Iteration 29, loss = 0.33594086\n",
      "Iteration 30, loss = 0.33515553\n",
      "Iteration 31, loss = 0.33445560\n",
      "Iteration 32, loss = 0.33384722\n",
      "Iteration 33, loss = 0.33330869\n",
      "Iteration 34, loss = 0.33219018\n",
      "Iteration 35, loss = 0.33184526\n",
      "Iteration 36, loss = 0.33121528\n",
      "Iteration 37, loss = 0.33079830\n",
      "Iteration 38, loss = 0.33010347\n",
      "Iteration 39, loss = 0.32945872\n",
      "Iteration 40, loss = 0.32900750\n",
      "Iteration 41, loss = 0.32857551\n",
      "Iteration 42, loss = 0.32767781\n",
      "Iteration 43, loss = 0.32701374\n",
      "Iteration 44, loss = 0.32633379\n",
      "Iteration 45, loss = 0.32600281\n",
      "Iteration 46, loss = 0.32531362\n",
      "Iteration 47, loss = 0.32449923\n",
      "Iteration 48, loss = 0.32403301\n",
      "Iteration 49, loss = 0.32385763\n",
      "Iteration 50, loss = 0.32289710\n",
      "Iteration 51, loss = 0.32264076\n",
      "Iteration 52, loss = 0.32197584\n",
      "Iteration 53, loss = 0.32165072\n",
      "Iteration 54, loss = 0.32093951\n",
      "Iteration 55, loss = 0.32032909\n",
      "Iteration 56, loss = 0.31983258\n",
      "Iteration 57, loss = 0.31921163\n",
      "Iteration 58, loss = 0.31922763\n",
      "Iteration 59, loss = 0.31865579\n",
      "Iteration 60, loss = 0.31751234\n",
      "Iteration 61, loss = 0.31742319\n",
      "Iteration 62, loss = 0.31724499\n",
      "Iteration 63, loss = 0.31637482\n",
      "Iteration 64, loss = 0.31620898\n",
      "Iteration 65, loss = 0.31501804\n",
      "Iteration 66, loss = 0.31482901\n",
      "Iteration 67, loss = 0.31461701\n",
      "Iteration 68, loss = 0.31423830\n",
      "Iteration 69, loss = 0.31354203\n",
      "Iteration 70, loss = 0.31324455\n",
      "Iteration 71, loss = 0.31261148\n",
      "Iteration 72, loss = 0.31255140\n",
      "Iteration 73, loss = 0.31222136\n",
      "Iteration 74, loss = 0.31131148\n",
      "Iteration 75, loss = 0.31156756\n",
      "Iteration 76, loss = 0.31077984\n",
      "Iteration 77, loss = 0.31055144\n",
      "Iteration 78, loss = 0.31025621\n",
      "Iteration 79, loss = 0.30997549\n",
      "Iteration 80, loss = 0.30931765\n",
      "Iteration 81, loss = 0.30916641\n",
      "Iteration 82, loss = 0.30878050\n",
      "Iteration 83, loss = 0.30847858\n",
      "Iteration 84, loss = 0.30843446\n",
      "Iteration 85, loss = 0.30786815\n",
      "Iteration 86, loss = 0.30744528\n",
      "Iteration 87, loss = 0.30773636\n",
      "Iteration 88, loss = 0.30750141\n",
      "Iteration 89, loss = 0.30672903\n",
      "Iteration 90, loss = 0.30657234\n",
      "Iteration 91, loss = 0.30615987\n",
      "Iteration 92, loss = 0.30579517\n",
      "Iteration 93, loss = 0.30494694\n",
      "Iteration 94, loss = 0.30500543\n",
      "Iteration 95, loss = 0.30519277\n",
      "Iteration 96, loss = 0.30510853\n",
      "Iteration 97, loss = 0.30439523\n",
      "Iteration 98, loss = 0.30431022\n",
      "Iteration 99, loss = 0.30361143\n",
      "Iteration 100, loss = 0.30360649\n",
      "Iteration 101, loss = 0.30331385\n",
      "Iteration 102, loss = 0.30336481\n",
      "Iteration 103, loss = 0.30292127\n",
      "Iteration 104, loss = 0.30235396\n",
      "Iteration 105, loss = 0.30254660\n",
      "Iteration 106, loss = 0.30181696\n",
      "Iteration 107, loss = 0.30167307\n",
      "Iteration 108, loss = 0.30156188\n",
      "Iteration 109, loss = 0.30152852\n",
      "Iteration 110, loss = 0.30089170\n",
      "Iteration 111, loss = 0.30081610\n",
      "Iteration 112, loss = 0.30051150\n",
      "Iteration 113, loss = 0.30076759\n",
      "Iteration 114, loss = 0.30021331\n",
      "Iteration 115, loss = 0.30005326\n",
      "Iteration 116, loss = 0.29967005\n",
      "Iteration 117, loss = 0.29968276\n",
      "Iteration 118, loss = 0.29961351\n",
      "Iteration 119, loss = 0.29948805\n",
      "Iteration 120, loss = 0.29883679\n",
      "Iteration 121, loss = 0.29868506\n",
      "Iteration 122, loss = 0.29832117\n",
      "Iteration 123, loss = 0.29839729\n",
      "Iteration 124, loss = 0.29825711\n",
      "Iteration 125, loss = 0.29739510\n",
      "Iteration 126, loss = 0.29807106\n",
      "Iteration 127, loss = 0.29752330\n",
      "Iteration 128, loss = 0.29703978\n",
      "Iteration 129, loss = 0.29712404\n",
      "Iteration 130, loss = 0.29769185\n",
      "Iteration 131, loss = 0.29663639\n",
      "Iteration 132, loss = 0.29629581\n",
      "Iteration 133, loss = 0.29615371\n",
      "Iteration 134, loss = 0.29606582\n",
      "Iteration 135, loss = 0.29574438\n",
      "Iteration 136, loss = 0.29595586\n",
      "Iteration 137, loss = 0.29534381\n",
      "Iteration 138, loss = 0.29565833\n",
      "Iteration 139, loss = 0.29509506\n",
      "Iteration 140, loss = 0.29556006\n",
      "Iteration 141, loss = 0.29493907\n",
      "Iteration 142, loss = 0.29438121\n",
      "Iteration 143, loss = 0.29505139\n",
      "Iteration 144, loss = 0.29412545\n",
      "Iteration 145, loss = 0.29481779\n",
      "Iteration 146, loss = 0.29406015\n",
      "Iteration 147, loss = 0.29341203\n",
      "Iteration 148, loss = 0.29352218\n",
      "Iteration 149, loss = 0.29406582\n",
      "Iteration 150, loss = 0.29310054\n",
      "Iteration 151, loss = 0.29341726\n",
      "Iteration 152, loss = 0.29290322\n",
      "Iteration 153, loss = 0.29266253\n",
      "Iteration 154, loss = 0.29300081\n",
      "Iteration 155, loss = 0.29258953\n",
      "Iteration 156, loss = 0.29278889\n",
      "Iteration 157, loss = 0.29297818\n",
      "Iteration 158, loss = 0.29240653\n",
      "Iteration 159, loss = 0.29246897\n",
      "Iteration 160, loss = 0.29212211\n",
      "Iteration 161, loss = 0.29186061\n",
      "Iteration 162, loss = 0.29205065\n",
      "Iteration 163, loss = 0.29127217\n",
      "Iteration 164, loss = 0.29166442\n",
      "Iteration 165, loss = 0.29100021\n",
      "Iteration 166, loss = 0.29114329\n",
      "Iteration 167, loss = 0.29121374\n",
      "Iteration 168, loss = 0.29070258\n",
      "Iteration 169, loss = 0.29062508\n",
      "Iteration 170, loss = 0.29096120\n",
      "Iteration 171, loss = 0.29014424\n",
      "Iteration 172, loss = 0.29023191\n",
      "Iteration 173, loss = 0.28976043\n",
      "Iteration 174, loss = 0.28980496\n",
      "Iteration 175, loss = 0.29014621\n",
      "Iteration 176, loss = 0.28991846\n",
      "Iteration 177, loss = 0.28951488\n",
      "Iteration 178, loss = 0.28962667\n",
      "Iteration 179, loss = 0.28947284\n",
      "Iteration 180, loss = 0.28896819\n",
      "Iteration 181, loss = 0.28887102\n",
      "Iteration 182, loss = 0.28900605\n",
      "Iteration 183, loss = 0.28908407\n",
      "Iteration 184, loss = 0.28915798\n",
      "Iteration 185, loss = 0.28851397\n",
      "Iteration 186, loss = 0.28861741\n",
      "Iteration 187, loss = 0.28813552\n",
      "Iteration 188, loss = 0.28865427\n",
      "Iteration 189, loss = 0.28807103\n",
      "Iteration 190, loss = 0.28812434\n",
      "Iteration 191, loss = 0.28796982\n",
      "Iteration 192, loss = 0.28772678\n",
      "Iteration 193, loss = 0.28739442\n",
      "Iteration 194, loss = 0.28723532\n",
      "Iteration 195, loss = 0.28732348\n",
      "Iteration 196, loss = 0.28675017\n",
      "Iteration 197, loss = 0.28672759\n",
      "Iteration 198, loss = 0.28684126\n",
      "Iteration 199, loss = 0.28662062\n",
      "Iteration 200, loss = 0.28634975\n",
      "Iteration 201, loss = 0.28634293\n",
      "Iteration 202, loss = 0.28638921\n",
      "Iteration 203, loss = 0.28709047\n",
      "Iteration 204, loss = 0.28597161\n",
      "Iteration 205, loss = 0.28629887\n",
      "Iteration 206, loss = 0.28599961\n",
      "Iteration 207, loss = 0.28544301\n",
      "Iteration 208, loss = 0.28607175\n",
      "Iteration 209, loss = 0.28583065\n",
      "Iteration 210, loss = 0.28563448\n",
      "Iteration 211, loss = 0.28529059\n",
      "Iteration 212, loss = 0.28658706\n",
      "Iteration 213, loss = 0.28510561\n",
      "Iteration 214, loss = 0.28482407\n",
      "Iteration 215, loss = 0.28532457\n",
      "Iteration 216, loss = 0.28515910\n",
      "Iteration 217, loss = 0.28460383\n",
      "Iteration 218, loss = 0.28440465\n",
      "Iteration 219, loss = 0.28412939\n",
      "Iteration 220, loss = 0.28461793\n",
      "Iteration 221, loss = 0.28396846\n",
      "Iteration 222, loss = 0.28380977\n",
      "Iteration 223, loss = 0.28386694\n",
      "Iteration 224, loss = 0.28352825\n",
      "Iteration 225, loss = 0.28382950\n",
      "Iteration 226, loss = 0.28336142\n",
      "Iteration 227, loss = 0.28354790\n",
      "Iteration 228, loss = 0.28359812\n",
      "Iteration 229, loss = 0.28294729\n",
      "Iteration 230, loss = 0.28295167\n",
      "Iteration 231, loss = 0.28352574\n",
      "Iteration 232, loss = 0.28243689\n",
      "Iteration 233, loss = 0.28343231\n",
      "Iteration 234, loss = 0.28228663\n",
      "Iteration 235, loss = 0.28264886\n",
      "Iteration 236, loss = 0.28221709\n",
      "Iteration 237, loss = 0.28262424\n",
      "Iteration 238, loss = 0.28203535\n",
      "Iteration 239, loss = 0.28178918\n",
      "Iteration 240, loss = 0.28175185\n",
      "Iteration 241, loss = 0.28149824\n",
      "Iteration 242, loss = 0.28196829\n",
      "Iteration 243, loss = 0.28151837\n",
      "Iteration 244, loss = 0.28158436\n",
      "Iteration 245, loss = 0.28178470\n",
      "Iteration 246, loss = 0.28142759\n",
      "Iteration 247, loss = 0.28087824\n",
      "Iteration 248, loss = 0.28078130\n",
      "Iteration 249, loss = 0.28054280\n",
      "Iteration 250, loss = 0.28145568\n",
      "Iteration 251, loss = 0.28068680\n",
      "Iteration 1, loss = 0.63163798\n",
      "Iteration 2, loss = 0.54502261\n",
      "Iteration 3, loss = 0.45317602\n",
      "Iteration 4, loss = 0.39880882\n",
      "Iteration 5, loss = 0.37797192\n",
      "Iteration 6, loss = 0.36890480\n",
      "Iteration 7, loss = 0.36379702\n",
      "Iteration 8, loss = 0.36049693\n",
      "Iteration 9, loss = 0.35745959\n",
      "Iteration 10, loss = 0.35575697\n",
      "Iteration 11, loss = 0.35416229\n",
      "Iteration 12, loss = 0.35245420\n",
      "Iteration 13, loss = 0.35108429\n",
      "Iteration 14, loss = 0.35018897\n",
      "Iteration 15, loss = 0.34868728\n",
      "Iteration 16, loss = 0.34807040\n",
      "Iteration 17, loss = 0.34667372\n",
      "Iteration 18, loss = 0.34543841\n",
      "Iteration 19, loss = 0.34456562\n",
      "Iteration 20, loss = 0.34381433\n",
      "Iteration 21, loss = 0.34295529\n",
      "Iteration 22, loss = 0.34196955\n",
      "Iteration 23, loss = 0.34136042\n",
      "Iteration 24, loss = 0.34044179\n",
      "Iteration 25, loss = 0.33953511\n",
      "Iteration 26, loss = 0.33852337\n",
      "Iteration 27, loss = 0.33792050\n",
      "Iteration 28, loss = 0.33731875\n",
      "Iteration 29, loss = 0.33622686\n",
      "Iteration 30, loss = 0.33586908\n",
      "Iteration 31, loss = 0.33473257\n",
      "Iteration 32, loss = 0.33445565\n",
      "Iteration 33, loss = 0.33402112\n",
      "Iteration 34, loss = 0.33327360\n",
      "Iteration 35, loss = 0.33245732\n",
      "Iteration 36, loss = 0.33166730\n",
      "Iteration 37, loss = 0.33109904\n",
      "Iteration 38, loss = 0.33037710\n",
      "Iteration 39, loss = 0.32967904\n",
      "Iteration 40, loss = 0.32966197\n",
      "Iteration 41, loss = 0.32878556\n",
      "Iteration 42, loss = 0.32837868\n",
      "Iteration 43, loss = 0.32789917\n",
      "Iteration 44, loss = 0.32743595\n",
      "Iteration 45, loss = 0.32683075\n",
      "Iteration 46, loss = 0.32597199\n",
      "Iteration 47, loss = 0.32531597\n",
      "Iteration 48, loss = 0.32483242\n",
      "Iteration 49, loss = 0.32406442\n",
      "Iteration 50, loss = 0.32394817\n",
      "Iteration 51, loss = 0.32315396\n",
      "Iteration 52, loss = 0.32291955\n",
      "Iteration 53, loss = 0.32241097\n",
      "Iteration 54, loss = 0.32192204\n",
      "Iteration 55, loss = 0.32136123\n",
      "Iteration 56, loss = 0.32100925\n",
      "Iteration 57, loss = 0.32024481\n",
      "Iteration 58, loss = 0.32000839\n",
      "Iteration 59, loss = 0.31936364\n",
      "Iteration 60, loss = 0.31896540\n",
      "Iteration 61, loss = 0.31810624\n",
      "Iteration 62, loss = 0.31758106\n",
      "Iteration 63, loss = 0.31698392\n",
      "Iteration 64, loss = 0.31687739\n",
      "Iteration 65, loss = 0.31616849\n",
      "Iteration 66, loss = 0.31609052\n",
      "Iteration 67, loss = 0.31543495\n",
      "Iteration 68, loss = 0.31465378\n",
      "Iteration 69, loss = 0.31473561\n",
      "Iteration 70, loss = 0.31422005\n",
      "Iteration 71, loss = 0.31360273\n",
      "Iteration 72, loss = 0.31308339\n",
      "Iteration 73, loss = 0.31266935\n",
      "Iteration 74, loss = 0.31250154\n",
      "Iteration 75, loss = 0.31162703\n",
      "Iteration 76, loss = 0.31187317\n",
      "Iteration 77, loss = 0.31065323\n",
      "Iteration 78, loss = 0.31106111\n",
      "Iteration 79, loss = 0.31045105\n",
      "Iteration 80, loss = 0.31005082\n",
      "Iteration 81, loss = 0.30980720\n",
      "Iteration 82, loss = 0.30993564\n",
      "Iteration 83, loss = 0.30977536\n",
      "Iteration 84, loss = 0.30872082\n",
      "Iteration 85, loss = 0.30907066\n",
      "Iteration 86, loss = 0.30806781\n",
      "Iteration 87, loss = 0.30776472\n",
      "Iteration 88, loss = 0.30788833\n",
      "Iteration 89, loss = 0.30733314\n",
      "Iteration 90, loss = 0.30713527\n",
      "Iteration 91, loss = 0.30637756\n",
      "Iteration 92, loss = 0.30681060\n",
      "Iteration 93, loss = 0.30644383\n",
      "Iteration 94, loss = 0.30584421\n",
      "Iteration 95, loss = 0.30554752\n",
      "Iteration 96, loss = 0.30486469\n",
      "Iteration 97, loss = 0.30453757\n",
      "Iteration 98, loss = 0.30452326\n",
      "Iteration 99, loss = 0.30495797\n",
      "Iteration 100, loss = 0.30392929\n",
      "Iteration 101, loss = 0.30442687\n",
      "Iteration 102, loss = 0.30409773\n",
      "Iteration 103, loss = 0.30319973\n",
      "Iteration 104, loss = 0.30307754\n",
      "Iteration 105, loss = 0.30280427\n",
      "Iteration 106, loss = 0.30225676\n",
      "Iteration 107, loss = 0.30222287\n",
      "Iteration 108, loss = 0.30217216\n",
      "Iteration 109, loss = 0.30164511\n",
      "Iteration 110, loss = 0.30157336\n",
      "Iteration 111, loss = 0.30135682\n",
      "Iteration 112, loss = 0.30082547\n",
      "Iteration 113, loss = 0.30123493\n",
      "Iteration 114, loss = 0.30050270\n",
      "Iteration 115, loss = 0.30058782\n",
      "Iteration 116, loss = 0.30083284\n",
      "Iteration 117, loss = 0.29955545\n",
      "Iteration 118, loss = 0.29968788\n",
      "Iteration 119, loss = 0.29977843\n",
      "Iteration 120, loss = 0.29965134\n",
      "Iteration 121, loss = 0.29911176\n",
      "Iteration 122, loss = 0.29914395\n",
      "Iteration 123, loss = 0.29892772\n",
      "Iteration 124, loss = 0.29817146\n",
      "Iteration 125, loss = 0.29800822\n",
      "Iteration 126, loss = 0.29794743\n",
      "Iteration 127, loss = 0.29812438\n",
      "Iteration 128, loss = 0.29742294\n",
      "Iteration 129, loss = 0.29718728\n",
      "Iteration 130, loss = 0.29709141\n",
      "Iteration 131, loss = 0.29758424\n",
      "Iteration 132, loss = 0.29734478\n",
      "Iteration 133, loss = 0.29695247\n",
      "Iteration 134, loss = 0.29643326\n",
      "Iteration 135, loss = 0.29613464\n",
      "Iteration 136, loss = 0.29547843\n",
      "Iteration 137, loss = 0.29603601\n",
      "Iteration 138, loss = 0.29571571\n",
      "Iteration 139, loss = 0.29594371\n",
      "Iteration 140, loss = 0.29580031\n",
      "Iteration 141, loss = 0.29485632\n",
      "Iteration 142, loss = 0.29502756\n",
      "Iteration 143, loss = 0.29497623\n",
      "Iteration 144, loss = 0.29463648\n",
      "Iteration 145, loss = 0.29444961\n",
      "Iteration 146, loss = 0.29469324\n",
      "Iteration 147, loss = 0.29394837\n",
      "Iteration 148, loss = 0.29362736\n",
      "Iteration 149, loss = 0.29450420\n",
      "Iteration 150, loss = 0.29376516\n",
      "Iteration 151, loss = 0.29336841\n",
      "Iteration 152, loss = 0.29330666\n",
      "Iteration 153, loss = 0.29329564\n",
      "Iteration 154, loss = 0.29343550\n",
      "Iteration 155, loss = 0.29372684\n",
      "Iteration 156, loss = 0.29275314\n",
      "Iteration 157, loss = 0.29302391\n",
      "Iteration 158, loss = 0.29192266\n",
      "Iteration 159, loss = 0.29213936\n",
      "Iteration 160, loss = 0.29290043\n",
      "Iteration 161, loss = 0.29188592\n",
      "Iteration 162, loss = 0.29172974\n",
      "Iteration 163, loss = 0.29178926\n",
      "Iteration 164, loss = 0.29161259\n",
      "Iteration 165, loss = 0.29142630\n",
      "Iteration 166, loss = 0.29087965\n",
      "Iteration 167, loss = 0.29106331\n",
      "Iteration 168, loss = 0.29102796\n",
      "Iteration 169, loss = 0.29139432\n",
      "Iteration 170, loss = 0.29089547\n",
      "Iteration 171, loss = 0.29101111\n",
      "Iteration 172, loss = 0.29093785\n",
      "Iteration 173, loss = 0.29013448\n",
      "Iteration 174, loss = 0.29042373\n",
      "Iteration 175, loss = 0.29067207\n",
      "Iteration 176, loss = 0.29007542\n",
      "Iteration 177, loss = 0.29008069\n",
      "Iteration 178, loss = 0.29017134\n",
      "Iteration 179, loss = 0.28940084\n",
      "Iteration 180, loss = 0.28986272\n",
      "Iteration 181, loss = 0.28952160\n",
      "Iteration 182, loss = 0.28948002\n",
      "Iteration 183, loss = 0.28919414\n",
      "Iteration 184, loss = 0.28873929\n",
      "Iteration 185, loss = 0.28935579\n",
      "Iteration 186, loss = 0.28889306\n",
      "Iteration 187, loss = 0.28850872\n",
      "Iteration 188, loss = 0.28864533\n",
      "Iteration 189, loss = 0.28847318\n",
      "Iteration 190, loss = 0.28784289\n",
      "Iteration 191, loss = 0.28814304\n",
      "Iteration 192, loss = 0.28840016\n",
      "Iteration 193, loss = 0.28780044\n",
      "Iteration 194, loss = 0.28751745\n",
      "Iteration 195, loss = 0.28794203\n",
      "Iteration 196, loss = 0.28703567\n",
      "Iteration 197, loss = 0.28689944\n",
      "Iteration 198, loss = 0.28684928\n",
      "Iteration 199, loss = 0.28689329\n",
      "Iteration 200, loss = 0.28671224\n",
      "Iteration 201, loss = 0.28673360\n",
      "Iteration 202, loss = 0.28686069\n",
      "Iteration 203, loss = 0.28584036\n",
      "Iteration 204, loss = 0.28603774\n",
      "Iteration 205, loss = 0.28624484\n",
      "Iteration 206, loss = 0.28668848\n",
      "Iteration 207, loss = 0.28581602\n",
      "Iteration 208, loss = 0.28635199\n",
      "Iteration 209, loss = 0.28581144\n",
      "Iteration 210, loss = 0.28537775\n",
      "Iteration 211, loss = 0.28525664\n",
      "Iteration 212, loss = 0.28578784\n",
      "Iteration 213, loss = 0.28552089\n",
      "Iteration 214, loss = 0.28554781\n",
      "Iteration 215, loss = 0.28487218\n",
      "Iteration 216, loss = 0.28446633\n",
      "Iteration 217, loss = 0.28453824\n",
      "Iteration 218, loss = 0.28443601\n",
      "Iteration 219, loss = 0.28474106\n",
      "Iteration 220, loss = 0.28425175\n",
      "Iteration 221, loss = 0.28443789\n",
      "Iteration 222, loss = 0.28476140\n",
      "Iteration 223, loss = 0.28428988\n",
      "Iteration 224, loss = 0.28373552\n",
      "Iteration 225, loss = 0.28398097\n",
      "Iteration 226, loss = 0.28374752\n",
      "Iteration 227, loss = 0.28398831\n",
      "Iteration 228, loss = 0.28351474\n",
      "Iteration 229, loss = 0.28272775\n",
      "Iteration 230, loss = 0.28246097\n",
      "Iteration 231, loss = 0.28329118\n",
      "Iteration 232, loss = 0.28286595\n",
      "Iteration 233, loss = 0.28280995\n",
      "Iteration 234, loss = 0.28324616\n",
      "Iteration 235, loss = 0.28292343\n",
      "Iteration 236, loss = 0.28242716\n",
      "Iteration 237, loss = 0.28224495\n",
      "Iteration 238, loss = 0.28247430\n",
      "Iteration 239, loss = 0.28236788\n",
      "Iteration 240, loss = 0.28182755\n",
      "Iteration 241, loss = 0.28197373\n",
      "Iteration 242, loss = 0.28207034\n",
      "Iteration 243, loss = 0.28173198\n",
      "Iteration 244, loss = 0.28183564\n",
      "Iteration 245, loss = 0.28140909\n",
      "Iteration 246, loss = 0.28111272\n",
      "Iteration 247, loss = 0.28077948\n",
      "Iteration 248, loss = 0.28138282\n",
      "Iteration 249, loss = 0.28112170\n",
      "Iteration 250, loss = 0.28094810\n",
      "Iteration 251, loss = 0.28035402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63144775\n",
      "Iteration 2, loss = 0.54512569\n",
      "Iteration 3, loss = 0.45330666\n",
      "Iteration 4, loss = 0.39835150\n",
      "Iteration 5, loss = 0.37696999\n",
      "Iteration 6, loss = 0.36801918\n",
      "Iteration 7, loss = 0.36305321\n",
      "Iteration 8, loss = 0.35949449\n",
      "Iteration 9, loss = 0.35678872\n",
      "Iteration 10, loss = 0.35473460\n",
      "Iteration 11, loss = 0.35340710\n",
      "Iteration 12, loss = 0.35176888\n",
      "Iteration 13, loss = 0.35076742\n",
      "Iteration 14, loss = 0.34945714\n",
      "Iteration 15, loss = 0.34864483\n",
      "Iteration 16, loss = 0.34731462\n",
      "Iteration 17, loss = 0.34642831\n",
      "Iteration 18, loss = 0.34489974\n",
      "Iteration 19, loss = 0.34420313\n",
      "Iteration 20, loss = 0.34346078\n",
      "Iteration 21, loss = 0.34273646\n",
      "Iteration 22, loss = 0.34168430\n",
      "Iteration 23, loss = 0.34086170\n",
      "Iteration 24, loss = 0.34002766\n",
      "Iteration 25, loss = 0.33885551\n",
      "Iteration 26, loss = 0.33844845\n",
      "Iteration 27, loss = 0.33781648\n",
      "Iteration 28, loss = 0.33645535\n",
      "Iteration 29, loss = 0.33637092\n",
      "Iteration 30, loss = 0.33535660\n",
      "Iteration 31, loss = 0.33466422\n",
      "Iteration 32, loss = 0.33391219\n",
      "Iteration 33, loss = 0.33352328\n",
      "Iteration 34, loss = 0.33230026\n",
      "Iteration 35, loss = 0.33184326\n",
      "Iteration 36, loss = 0.33130708\n",
      "Iteration 37, loss = 0.33060744\n",
      "Iteration 38, loss = 0.32966232\n",
      "Iteration 39, loss = 0.32915105\n",
      "Iteration 40, loss = 0.32887033\n",
      "Iteration 41, loss = 0.32811926\n",
      "Iteration 42, loss = 0.32739097\n",
      "Iteration 43, loss = 0.32726783\n",
      "Iteration 44, loss = 0.32608094\n",
      "Iteration 45, loss = 0.32587837\n",
      "Iteration 46, loss = 0.32523841\n",
      "Iteration 47, loss = 0.32418736\n",
      "Iteration 48, loss = 0.32402791\n",
      "Iteration 49, loss = 0.32309551\n",
      "Iteration 50, loss = 0.32271585\n",
      "Iteration 51, loss = 0.32251330\n",
      "Iteration 52, loss = 0.32154602\n",
      "Iteration 53, loss = 0.32105296\n",
      "Iteration 54, loss = 0.32043285\n",
      "Iteration 55, loss = 0.32013845\n",
      "Iteration 56, loss = 0.31956859\n",
      "Iteration 57, loss = 0.31850971\n",
      "Iteration 58, loss = 0.31890584\n",
      "Iteration 59, loss = 0.31749493\n",
      "Iteration 60, loss = 0.31732636\n",
      "Iteration 61, loss = 0.31679501\n",
      "Iteration 62, loss = 0.31616851\n",
      "Iteration 63, loss = 0.31542681\n",
      "Iteration 64, loss = 0.31525847\n",
      "Iteration 65, loss = 0.31478356\n",
      "Iteration 66, loss = 0.31431514\n",
      "Iteration 67, loss = 0.31367166\n",
      "Iteration 68, loss = 0.31358164\n",
      "Iteration 69, loss = 0.31281227\n",
      "Iteration 70, loss = 0.31324376\n",
      "Iteration 71, loss = 0.31195901\n",
      "Iteration 72, loss = 0.31142718\n",
      "Iteration 73, loss = 0.31119483\n",
      "Iteration 74, loss = 0.31097614\n",
      "Iteration 75, loss = 0.31031930\n",
      "Iteration 76, loss = 0.31083529\n",
      "Iteration 77, loss = 0.30999194\n",
      "Iteration 78, loss = 0.30966036\n",
      "Iteration 79, loss = 0.30959750\n",
      "Iteration 80, loss = 0.30863623\n",
      "Iteration 81, loss = 0.30834779\n",
      "Iteration 82, loss = 0.30824483\n",
      "Iteration 83, loss = 0.30820116\n",
      "Iteration 84, loss = 0.30748235\n",
      "Iteration 85, loss = 0.30695496\n",
      "Iteration 86, loss = 0.30690568\n",
      "Iteration 87, loss = 0.30705986\n",
      "Iteration 88, loss = 0.30638097\n",
      "Iteration 89, loss = 0.30587025\n",
      "Iteration 90, loss = 0.30552554\n",
      "Iteration 91, loss = 0.30541545\n",
      "Iteration 92, loss = 0.30507212\n",
      "Iteration 93, loss = 0.30441268\n",
      "Iteration 94, loss = 0.30465300\n",
      "Iteration 95, loss = 0.30388279\n",
      "Iteration 96, loss = 0.30380872\n",
      "Iteration 97, loss = 0.30390268\n",
      "Iteration 98, loss = 0.30333227\n",
      "Iteration 99, loss = 0.30301097\n",
      "Iteration 100, loss = 0.30335320\n",
      "Iteration 101, loss = 0.30255069\n",
      "Iteration 102, loss = 0.30251895\n",
      "Iteration 103, loss = 0.30268968\n",
      "Iteration 104, loss = 0.30182213\n",
      "Iteration 105, loss = 0.30237697\n",
      "Iteration 106, loss = 0.30082686\n",
      "Iteration 107, loss = 0.30094757\n",
      "Iteration 108, loss = 0.30098382\n",
      "Iteration 109, loss = 0.30039238\n",
      "Iteration 110, loss = 0.30060634\n",
      "Iteration 111, loss = 0.29995948\n",
      "Iteration 112, loss = 0.30010468\n",
      "Iteration 113, loss = 0.29962926\n",
      "Iteration 114, loss = 0.29977416\n",
      "Iteration 115, loss = 0.29945543\n",
      "Iteration 116, loss = 0.29902409\n",
      "Iteration 117, loss = 0.29826854\n",
      "Iteration 118, loss = 0.29835666\n",
      "Iteration 119, loss = 0.29871295\n",
      "Iteration 120, loss = 0.29869222\n",
      "Iteration 121, loss = 0.29827699\n",
      "Iteration 122, loss = 0.29768980\n",
      "Iteration 123, loss = 0.29769454\n",
      "Iteration 124, loss = 0.29720840\n",
      "Iteration 125, loss = 0.29700672\n",
      "Iteration 126, loss = 0.29748893\n",
      "Iteration 127, loss = 0.29660135\n",
      "Iteration 128, loss = 0.29614567\n",
      "Iteration 129, loss = 0.29660061\n",
      "Iteration 130, loss = 0.29656151\n",
      "Iteration 131, loss = 0.29619715\n",
      "Iteration 132, loss = 0.29570027\n",
      "Iteration 133, loss = 0.29538223\n",
      "Iteration 134, loss = 0.29524257\n",
      "Iteration 135, loss = 0.29509102\n",
      "Iteration 136, loss = 0.29513802\n",
      "Iteration 137, loss = 0.29482788\n",
      "Iteration 138, loss = 0.29538345\n",
      "Iteration 139, loss = 0.29480470\n",
      "Iteration 140, loss = 0.29455039\n",
      "Iteration 141, loss = 0.29401162\n",
      "Iteration 142, loss = 0.29457443\n",
      "Iteration 143, loss = 0.29417841\n",
      "Iteration 144, loss = 0.29425518\n",
      "Iteration 145, loss = 0.29354811\n",
      "Iteration 146, loss = 0.29325041\n",
      "Iteration 147, loss = 0.29328806\n",
      "Iteration 148, loss = 0.29318345\n",
      "Iteration 149, loss = 0.29343368\n",
      "Iteration 150, loss = 0.29269798\n",
      "Iteration 151, loss = 0.29286101\n",
      "Iteration 152, loss = 0.29267144\n",
      "Iteration 153, loss = 0.29256926\n",
      "Iteration 154, loss = 0.29176735\n",
      "Iteration 155, loss = 0.29243941\n",
      "Iteration 156, loss = 0.29185493\n",
      "Iteration 157, loss = 0.29213026\n",
      "Iteration 158, loss = 0.29131278\n",
      "Iteration 159, loss = 0.29140362\n",
      "Iteration 160, loss = 0.29185796\n",
      "Iteration 161, loss = 0.29156804\n",
      "Iteration 162, loss = 0.29127174\n",
      "Iteration 163, loss = 0.29107876\n",
      "Iteration 164, loss = 0.29084192\n",
      "Iteration 165, loss = 0.29073523\n",
      "Iteration 166, loss = 0.29082228\n",
      "Iteration 167, loss = 0.29091034\n",
      "Iteration 168, loss = 0.29042557\n",
      "Iteration 169, loss = 0.29020325\n",
      "Iteration 170, loss = 0.29009096\n",
      "Iteration 171, loss = 0.28955046\n",
      "Iteration 172, loss = 0.28964656\n",
      "Iteration 173, loss = 0.29015070\n",
      "Iteration 174, loss = 0.28970656\n",
      "Iteration 175, loss = 0.28957431\n",
      "Iteration 176, loss = 0.28880517\n",
      "Iteration 177, loss = 0.28924621\n",
      "Iteration 178, loss = 0.28889715\n",
      "Iteration 179, loss = 0.28889543\n",
      "Iteration 180, loss = 0.28892613\n",
      "Iteration 181, loss = 0.28863282\n",
      "Iteration 182, loss = 0.28816658\n",
      "Iteration 183, loss = 0.28876176\n",
      "Iteration 184, loss = 0.28892402\n",
      "Iteration 185, loss = 0.28864684\n",
      "Iteration 186, loss = 0.28815646\n",
      "Iteration 187, loss = 0.28793082\n",
      "Iteration 188, loss = 0.28807789\n",
      "Iteration 189, loss = 0.28784414\n",
      "Iteration 190, loss = 0.28715480\n",
      "Iteration 191, loss = 0.28702057\n",
      "Iteration 192, loss = 0.28785169\n",
      "Iteration 193, loss = 0.28751356\n",
      "Iteration 194, loss = 0.28694426\n",
      "Iteration 195, loss = 0.28736989\n",
      "Iteration 196, loss = 0.28656958\n",
      "Iteration 197, loss = 0.28662808\n",
      "Iteration 198, loss = 0.28641069\n",
      "Iteration 199, loss = 0.28695853\n",
      "Iteration 200, loss = 0.28693735\n",
      "Iteration 201, loss = 0.28614122\n",
      "Iteration 202, loss = 0.28680115\n",
      "Iteration 203, loss = 0.28584496\n",
      "Iteration 204, loss = 0.28573309\n",
      "Iteration 205, loss = 0.28600176\n",
      "Iteration 206, loss = 0.28552325\n",
      "Iteration 207, loss = 0.28553672\n",
      "Iteration 208, loss = 0.28504088\n",
      "Iteration 209, loss = 0.28569601\n",
      "Iteration 210, loss = 0.28478293\n",
      "Iteration 211, loss = 0.28540210\n",
      "Iteration 212, loss = 0.28577435\n",
      "Iteration 213, loss = 0.28462108\n",
      "Iteration 214, loss = 0.28505474\n",
      "Iteration 215, loss = 0.28494195\n",
      "Iteration 216, loss = 0.28405666\n",
      "Iteration 217, loss = 0.28464948\n",
      "Iteration 218, loss = 0.28404417\n",
      "Iteration 219, loss = 0.28441938\n",
      "Iteration 220, loss = 0.28348706\n",
      "Iteration 221, loss = 0.28337570\n",
      "Iteration 222, loss = 0.28359122\n",
      "Iteration 223, loss = 0.28386651\n",
      "Iteration 224, loss = 0.28384915\n",
      "Iteration 225, loss = 0.28337675\n",
      "Iteration 226, loss = 0.28329779\n",
      "Iteration 227, loss = 0.28347766\n",
      "Iteration 228, loss = 0.28323321\n",
      "Iteration 229, loss = 0.28249169\n",
      "Iteration 230, loss = 0.28282186\n",
      "Iteration 231, loss = 0.28311546\n",
      "Iteration 232, loss = 0.28309071\n",
      "Iteration 233, loss = 0.28306444\n",
      "Iteration 234, loss = 0.28275691\n",
      "Iteration 235, loss = 0.28242638\n",
      "Iteration 236, loss = 0.28200700\n",
      "Iteration 237, loss = 0.28231406\n",
      "Iteration 238, loss = 0.28261226\n",
      "Iteration 239, loss = 0.28217544\n",
      "Iteration 240, loss = 0.28197570\n",
      "Iteration 241, loss = 0.28172706\n",
      "Iteration 242, loss = 0.28134618\n",
      "Iteration 243, loss = 0.28138601\n",
      "Iteration 244, loss = 0.28178218\n",
      "Iteration 245, loss = 0.28112446\n",
      "Iteration 246, loss = 0.28089091\n",
      "Iteration 247, loss = 0.28082341\n",
      "Iteration 248, loss = 0.28097059\n",
      "Iteration 249, loss = 0.28084434\n",
      "Iteration 250, loss = 0.28099476\n",
      "Iteration 251, loss = 0.28058195\n",
      "Iteration 1, loss = 0.63169453\n",
      "Iteration 2, loss = 0.54499864\n",
      "Iteration 3, loss = 0.45303484\n",
      "Iteration 4, loss = 0.39845655\n",
      "Iteration 5, loss = 0.37696366\n",
      "Iteration 6, loss = 0.36851589\n",
      "Iteration 7, loss = 0.36332825\n",
      "Iteration 8, loss = 0.35977827\n",
      "Iteration 9, loss = 0.35740088\n",
      "Iteration 10, loss = 0.35586347\n",
      "Iteration 11, loss = 0.35349730\n",
      "Iteration 12, loss = 0.35233248\n",
      "Iteration 13, loss = 0.35077033\n",
      "Iteration 14, loss = 0.34964713\n",
      "Iteration 15, loss = 0.34838108\n",
      "Iteration 16, loss = 0.34755449\n",
      "Iteration 17, loss = 0.34658101\n",
      "Iteration 18, loss = 0.34549165\n",
      "Iteration 19, loss = 0.34444446\n",
      "Iteration 20, loss = 0.34348907\n",
      "Iteration 21, loss = 0.34289409\n",
      "Iteration 22, loss = 0.34177000\n",
      "Iteration 23, loss = 0.34117601\n",
      "Iteration 24, loss = 0.34027755\n",
      "Iteration 25, loss = 0.33879621\n",
      "Iteration 26, loss = 0.33853807\n",
      "Iteration 27, loss = 0.33794820\n",
      "Iteration 28, loss = 0.33708337\n",
      "Iteration 29, loss = 0.33594086\n",
      "Iteration 30, loss = 0.33515553\n",
      "Iteration 31, loss = 0.33445560\n",
      "Iteration 32, loss = 0.33384722\n",
      "Iteration 33, loss = 0.33330869\n",
      "Iteration 34, loss = 0.33219018\n",
      "Iteration 35, loss = 0.33184526\n",
      "Iteration 36, loss = 0.33121528\n",
      "Iteration 37, loss = 0.33079830\n",
      "Iteration 38, loss = 0.33010347\n",
      "Iteration 39, loss = 0.32945872\n",
      "Iteration 40, loss = 0.32900750\n",
      "Iteration 41, loss = 0.32857551\n",
      "Iteration 42, loss = 0.32767781\n",
      "Iteration 43, loss = 0.32701374\n",
      "Iteration 44, loss = 0.32633379\n",
      "Iteration 45, loss = 0.32600281\n",
      "Iteration 46, loss = 0.32531362\n",
      "Iteration 47, loss = 0.32449923\n",
      "Iteration 48, loss = 0.32403301\n",
      "Iteration 49, loss = 0.32385763\n",
      "Iteration 50, loss = 0.32289710\n",
      "Iteration 51, loss = 0.32264076\n",
      "Iteration 52, loss = 0.32197584\n",
      "Iteration 53, loss = 0.32165072\n",
      "Iteration 54, loss = 0.32093951\n",
      "Iteration 55, loss = 0.32032909\n",
      "Iteration 56, loss = 0.31983258\n",
      "Iteration 57, loss = 0.31921163\n",
      "Iteration 58, loss = 0.31922763\n",
      "Iteration 59, loss = 0.31865579\n",
      "Iteration 60, loss = 0.31751234\n",
      "Iteration 61, loss = 0.31742319\n",
      "Iteration 62, loss = 0.31724499\n",
      "Iteration 63, loss = 0.31637482\n",
      "Iteration 64, loss = 0.31620898\n",
      "Iteration 65, loss = 0.31501804\n",
      "Iteration 66, loss = 0.31482901\n",
      "Iteration 67, loss = 0.31461701\n",
      "Iteration 68, loss = 0.31423830\n",
      "Iteration 69, loss = 0.31354203\n",
      "Iteration 70, loss = 0.31324455\n",
      "Iteration 71, loss = 0.31261148\n",
      "Iteration 72, loss = 0.31255140\n",
      "Iteration 73, loss = 0.31222136\n",
      "Iteration 74, loss = 0.31131148\n",
      "Iteration 75, loss = 0.31156756\n",
      "Iteration 76, loss = 0.31077984\n",
      "Iteration 77, loss = 0.31055144\n",
      "Iteration 78, loss = 0.31025621\n",
      "Iteration 79, loss = 0.30997549\n",
      "Iteration 80, loss = 0.30931765\n",
      "Iteration 81, loss = 0.30916641\n",
      "Iteration 82, loss = 0.30878050\n",
      "Iteration 83, loss = 0.30847858\n",
      "Iteration 84, loss = 0.30843446\n",
      "Iteration 85, loss = 0.30786815\n",
      "Iteration 86, loss = 0.30744528\n",
      "Iteration 87, loss = 0.30773636\n",
      "Iteration 88, loss = 0.30750141\n",
      "Iteration 89, loss = 0.30672903\n",
      "Iteration 90, loss = 0.30657234\n",
      "Iteration 91, loss = 0.30615987\n",
      "Iteration 92, loss = 0.30579517\n",
      "Iteration 93, loss = 0.30494694\n",
      "Iteration 94, loss = 0.30500543\n",
      "Iteration 95, loss = 0.30519277\n",
      "Iteration 96, loss = 0.30510853\n",
      "Iteration 97, loss = 0.30439523\n",
      "Iteration 98, loss = 0.30431022\n",
      "Iteration 99, loss = 0.30361143\n",
      "Iteration 100, loss = 0.30360649\n",
      "Iteration 101, loss = 0.30331385\n",
      "Iteration 102, loss = 0.30336481\n",
      "Iteration 103, loss = 0.30292127\n",
      "Iteration 104, loss = 0.30235396\n",
      "Iteration 105, loss = 0.30254660\n",
      "Iteration 106, loss = 0.30181696\n",
      "Iteration 107, loss = 0.30167307\n",
      "Iteration 108, loss = 0.30156188\n",
      "Iteration 109, loss = 0.30152852\n",
      "Iteration 110, loss = 0.30089170\n",
      "Iteration 111, loss = 0.30081610\n",
      "Iteration 112, loss = 0.30051150\n",
      "Iteration 113, loss = 0.30076759\n",
      "Iteration 114, loss = 0.30021331\n",
      "Iteration 115, loss = 0.30005326\n",
      "Iteration 116, loss = 0.29967005\n",
      "Iteration 117, loss = 0.29968276\n",
      "Iteration 118, loss = 0.29961351\n",
      "Iteration 119, loss = 0.29948805\n",
      "Iteration 120, loss = 0.29883679\n",
      "Iteration 121, loss = 0.29868506\n",
      "Iteration 122, loss = 0.29832117\n",
      "Iteration 123, loss = 0.29839729\n",
      "Iteration 124, loss = 0.29825711\n",
      "Iteration 125, loss = 0.29739510\n",
      "Iteration 126, loss = 0.29807106\n",
      "Iteration 127, loss = 0.29752330\n",
      "Iteration 128, loss = 0.29703978\n",
      "Iteration 129, loss = 0.29712404\n",
      "Iteration 130, loss = 0.29769185\n",
      "Iteration 131, loss = 0.29663639\n",
      "Iteration 132, loss = 0.29629581\n",
      "Iteration 133, loss = 0.29615371\n",
      "Iteration 134, loss = 0.29606582\n",
      "Iteration 135, loss = 0.29574438\n",
      "Iteration 136, loss = 0.29595586\n",
      "Iteration 137, loss = 0.29534381\n",
      "Iteration 138, loss = 0.29565833\n",
      "Iteration 139, loss = 0.29509506\n",
      "Iteration 140, loss = 0.29556006\n",
      "Iteration 141, loss = 0.29493907\n",
      "Iteration 142, loss = 0.29438121\n",
      "Iteration 143, loss = 0.29505139\n",
      "Iteration 144, loss = 0.29412545\n",
      "Iteration 145, loss = 0.29481779\n",
      "Iteration 146, loss = 0.29406015\n",
      "Iteration 147, loss = 0.29341203\n",
      "Iteration 148, loss = 0.29352218\n",
      "Iteration 149, loss = 0.29406582\n",
      "Iteration 150, loss = 0.29310054\n",
      "Iteration 151, loss = 0.29341726\n",
      "Iteration 152, loss = 0.29290322\n",
      "Iteration 153, loss = 0.29266253\n",
      "Iteration 154, loss = 0.29300081\n",
      "Iteration 155, loss = 0.29258953\n",
      "Iteration 156, loss = 0.29278889\n",
      "Iteration 157, loss = 0.29297818\n",
      "Iteration 158, loss = 0.29240653\n",
      "Iteration 159, loss = 0.29246897\n",
      "Iteration 160, loss = 0.29212211\n",
      "Iteration 161, loss = 0.29186061\n",
      "Iteration 162, loss = 0.29205065\n",
      "Iteration 163, loss = 0.29127217\n",
      "Iteration 164, loss = 0.29166442\n",
      "Iteration 165, loss = 0.29100021\n",
      "Iteration 166, loss = 0.29114329\n",
      "Iteration 167, loss = 0.29121374\n",
      "Iteration 168, loss = 0.29070258\n",
      "Iteration 169, loss = 0.29062508\n",
      "Iteration 170, loss = 0.29096120\n",
      "Iteration 171, loss = 0.29014424\n",
      "Iteration 172, loss = 0.29023191\n",
      "Iteration 173, loss = 0.28976043\n",
      "Iteration 174, loss = 0.28980496\n",
      "Iteration 175, loss = 0.29014621\n",
      "Iteration 176, loss = 0.28991846\n",
      "Iteration 177, loss = 0.28951488\n",
      "Iteration 178, loss = 0.28962667\n",
      "Iteration 179, loss = 0.28947284\n",
      "Iteration 180, loss = 0.28896819\n",
      "Iteration 181, loss = 0.28887102\n",
      "Iteration 182, loss = 0.28900605\n",
      "Iteration 183, loss = 0.28908407\n",
      "Iteration 184, loss = 0.28915798\n",
      "Iteration 185, loss = 0.28851397\n",
      "Iteration 186, loss = 0.28861741\n",
      "Iteration 187, loss = 0.28813552\n",
      "Iteration 188, loss = 0.28865427\n",
      "Iteration 189, loss = 0.28807103\n",
      "Iteration 190, loss = 0.28812434\n",
      "Iteration 191, loss = 0.28796982\n",
      "Iteration 192, loss = 0.28772678\n",
      "Iteration 193, loss = 0.28739442\n",
      "Iteration 194, loss = 0.28723532\n",
      "Iteration 195, loss = 0.28732348\n",
      "Iteration 196, loss = 0.28675017\n",
      "Iteration 197, loss = 0.28672759\n",
      "Iteration 198, loss = 0.28684126\n",
      "Iteration 199, loss = 0.28662062\n",
      "Iteration 200, loss = 0.28634975\n",
      "Iteration 201, loss = 0.28634293\n",
      "Iteration 202, loss = 0.28638921\n",
      "Iteration 203, loss = 0.28709047\n",
      "Iteration 204, loss = 0.28597161\n",
      "Iteration 205, loss = 0.28629887\n",
      "Iteration 206, loss = 0.28599961\n",
      "Iteration 207, loss = 0.28544301\n",
      "Iteration 208, loss = 0.28607175\n",
      "Iteration 209, loss = 0.28583065\n",
      "Iteration 210, loss = 0.28563448\n",
      "Iteration 211, loss = 0.28529059\n",
      "Iteration 212, loss = 0.28658706\n",
      "Iteration 213, loss = 0.28510561\n",
      "Iteration 214, loss = 0.28482407\n",
      "Iteration 215, loss = 0.28532457\n",
      "Iteration 216, loss = 0.28515910\n",
      "Iteration 217, loss = 0.28460383\n",
      "Iteration 218, loss = 0.28440465\n",
      "Iteration 219, loss = 0.28412939\n",
      "Iteration 220, loss = 0.28461793\n",
      "Iteration 221, loss = 0.28396846\n",
      "Iteration 222, loss = 0.28380977\n",
      "Iteration 223, loss = 0.28386694\n",
      "Iteration 224, loss = 0.28352825\n",
      "Iteration 225, loss = 0.28382950\n",
      "Iteration 226, loss = 0.28336142\n",
      "Iteration 227, loss = 0.28354790\n",
      "Iteration 228, loss = 0.28359812\n",
      "Iteration 229, loss = 0.28294729\n",
      "Iteration 230, loss = 0.28295167\n",
      "Iteration 231, loss = 0.28352574\n",
      "Iteration 232, loss = 0.28243689\n",
      "Iteration 233, loss = 0.28343231\n",
      "Iteration 234, loss = 0.28228663\n",
      "Iteration 235, loss = 0.28264886\n",
      "Iteration 236, loss = 0.28221709\n",
      "Iteration 237, loss = 0.28262424\n",
      "Iteration 238, loss = 0.28203535\n",
      "Iteration 239, loss = 0.28178918\n",
      "Iteration 240, loss = 0.28175185\n",
      "Iteration 241, loss = 0.28149824\n",
      "Iteration 242, loss = 0.28196829\n",
      "Iteration 243, loss = 0.28151837\n",
      "Iteration 244, loss = 0.28158436\n",
      "Iteration 245, loss = 0.28178470\n",
      "Iteration 246, loss = 0.28142759\n",
      "Iteration 247, loss = 0.28087824\n",
      "Iteration 248, loss = 0.28078130\n",
      "Iteration 249, loss = 0.28054280\n",
      "Iteration 250, loss = 0.28145568\n",
      "Iteration 251, loss = 0.28068680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.41950013\n",
      "Iteration 2, loss = 0.35947995\n",
      "Iteration 3, loss = 0.34715154\n",
      "Iteration 4, loss = 0.33853379\n",
      "Iteration 5, loss = 0.33076563\n",
      "Iteration 6, loss = 0.32602897\n",
      "Iteration 7, loss = 0.32260993\n",
      "Iteration 8, loss = 0.31890063\n",
      "Iteration 9, loss = 0.31278507\n",
      "Iteration 10, loss = 0.30712618\n",
      "Iteration 11, loss = 0.30342862\n",
      "Iteration 12, loss = 0.29909694\n",
      "Iteration 13, loss = 0.29573207\n",
      "Iteration 14, loss = 0.29169488\n",
      "Iteration 15, loss = 0.29041709\n",
      "Iteration 16, loss = 0.28756977\n",
      "Iteration 17, loss = 0.28731327\n",
      "Iteration 18, loss = 0.28466933\n",
      "Iteration 19, loss = 0.28307872\n",
      "Iteration 20, loss = 0.28144355\n",
      "Iteration 21, loss = 0.27824161\n",
      "Iteration 22, loss = 0.27645172\n",
      "Iteration 23, loss = 0.27631374\n",
      "Iteration 24, loss = 0.27296271\n",
      "Iteration 25, loss = 0.27126973\n",
      "Iteration 26, loss = 0.26902900\n",
      "Iteration 27, loss = 0.26882050\n",
      "Iteration 28, loss = 0.26649257\n",
      "Iteration 29, loss = 0.26301996\n",
      "Iteration 30, loss = 0.26017758\n",
      "Iteration 31, loss = 0.25989905\n",
      "Iteration 32, loss = 0.25927894\n",
      "Iteration 33, loss = 0.25750092\n",
      "Iteration 34, loss = 0.25751161\n",
      "Iteration 35, loss = 0.25521373\n",
      "Iteration 36, loss = 0.25489710\n",
      "Iteration 37, loss = 0.25223066\n",
      "Iteration 38, loss = 0.25188371\n",
      "Iteration 39, loss = 0.24991867\n",
      "Iteration 40, loss = 0.25074804\n",
      "Iteration 41, loss = 0.24964254\n",
      "Iteration 42, loss = 0.25067928\n",
      "Iteration 43, loss = 0.24784764\n",
      "Iteration 44, loss = 0.24734352\n",
      "Iteration 45, loss = 0.24751534\n",
      "Iteration 46, loss = 0.24699151\n",
      "Iteration 47, loss = 0.24672849\n",
      "Iteration 48, loss = 0.24645409\n",
      "Iteration 49, loss = 0.24558089\n",
      "Iteration 50, loss = 0.24600343\n",
      "Iteration 51, loss = 0.24593620\n",
      "Iteration 52, loss = 0.24412605\n",
      "Iteration 53, loss = 0.24490690\n",
      "Iteration 54, loss = 0.24328007\n",
      "Iteration 55, loss = 0.24460756\n",
      "Iteration 56, loss = 0.24286288\n",
      "Iteration 57, loss = 0.24159824\n",
      "Iteration 58, loss = 0.24238795\n",
      "Iteration 59, loss = 0.24214417\n",
      "Iteration 60, loss = 0.24072129\n",
      "Iteration 61, loss = 0.23962207\n",
      "Iteration 62, loss = 0.24006232\n",
      "Iteration 63, loss = 0.24056347\n",
      "Iteration 64, loss = 0.24109640\n",
      "Iteration 65, loss = 0.24105690\n",
      "Iteration 66, loss = 0.23917268\n",
      "Iteration 67, loss = 0.23880899\n",
      "Iteration 68, loss = 0.24073459\n",
      "Iteration 69, loss = 0.23931695\n",
      "Iteration 70, loss = 0.23909349\n",
      "Iteration 71, loss = 0.23980576\n",
      "Iteration 72, loss = 0.23855742\n",
      "Iteration 73, loss = 0.23854123\n",
      "Iteration 74, loss = 0.24085757\n",
      "Iteration 75, loss = 0.23912133\n",
      "Iteration 76, loss = 0.23952830\n",
      "Iteration 77, loss = 0.23689841\n",
      "Iteration 78, loss = 0.23779096\n",
      "Iteration 79, loss = 0.23798954\n",
      "Iteration 80, loss = 0.23694456\n",
      "Iteration 81, loss = 0.23724908\n",
      "Iteration 82, loss = 0.23622299\n",
      "Iteration 83, loss = 0.23704055\n",
      "Iteration 84, loss = 0.23528876\n",
      "Iteration 85, loss = 0.23620885\n",
      "Iteration 86, loss = 0.23606476\n",
      "Iteration 87, loss = 0.23529545\n",
      "Iteration 88, loss = 0.23708359\n",
      "Iteration 89, loss = 0.23496043\n",
      "Iteration 90, loss = 0.23648551\n",
      "Iteration 91, loss = 0.23473423\n",
      "Iteration 92, loss = 0.23619963\n",
      "Iteration 93, loss = 0.23529862\n",
      "Iteration 94, loss = 0.23413662\n",
      "Iteration 95, loss = 0.23470545\n",
      "Iteration 96, loss = 0.23480540\n",
      "Iteration 97, loss = 0.23323790\n",
      "Iteration 98, loss = 0.23465371\n",
      "Iteration 99, loss = 0.23452615\n",
      "Iteration 100, loss = 0.23334903\n",
      "Iteration 101, loss = 0.23267470\n",
      "Iteration 102, loss = 0.23382096\n",
      "Iteration 103, loss = 0.23310463\n",
      "Iteration 104, loss = 0.23200522\n",
      "Iteration 105, loss = 0.23419509\n",
      "Iteration 106, loss = 0.23397352\n",
      "Iteration 107, loss = 0.23223148\n",
      "Iteration 108, loss = 0.23256364\n",
      "Iteration 109, loss = 0.23268136\n",
      "Iteration 110, loss = 0.23244968\n",
      "Iteration 111, loss = 0.23294786\n",
      "Iteration 112, loss = 0.23156454\n",
      "Iteration 113, loss = 0.23435950\n",
      "Iteration 114, loss = 0.23230561\n",
      "Iteration 115, loss = 0.23336112\n",
      "Iteration 116, loss = 0.23141922\n",
      "Iteration 117, loss = 0.23243211\n",
      "Iteration 118, loss = 0.23097416\n",
      "Iteration 119, loss = 0.23143333\n",
      "Iteration 120, loss = 0.23152677\n",
      "Iteration 121, loss = 0.23088126\n",
      "Iteration 122, loss = 0.23022349\n",
      "Iteration 123, loss = 0.23048655\n",
      "Iteration 124, loss = 0.23140057\n",
      "Iteration 125, loss = 0.23160356\n",
      "Iteration 126, loss = 0.23100009\n",
      "Iteration 127, loss = 0.23086248\n",
      "Iteration 128, loss = 0.23094242\n",
      "Iteration 129, loss = 0.23170584\n",
      "Iteration 130, loss = 0.23080259\n",
      "Iteration 131, loss = 0.23107666\n",
      "Iteration 132, loss = 0.23000193\n",
      "Iteration 133, loss = 0.22872136\n",
      "Iteration 134, loss = 0.22983424\n",
      "Iteration 135, loss = 0.22930763\n",
      "Iteration 136, loss = 0.22998942\n",
      "Iteration 137, loss = 0.23019294\n",
      "Iteration 138, loss = 0.22892785\n",
      "Iteration 139, loss = 0.23119341\n",
      "Iteration 140, loss = 0.23035541\n",
      "Iteration 141, loss = 0.22971785\n",
      "Iteration 142, loss = 0.22892877\n",
      "Iteration 143, loss = 0.22819131\n",
      "Iteration 144, loss = 0.22797283\n",
      "Iteration 145, loss = 0.23052789\n",
      "Iteration 146, loss = 0.22803786\n",
      "Iteration 147, loss = 0.22720151\n",
      "Iteration 148, loss = 0.22914446\n",
      "Iteration 149, loss = 0.22955676\n",
      "Iteration 150, loss = 0.22839511\n",
      "Iteration 151, loss = 0.22768776\n",
      "Iteration 152, loss = 0.22915034\n",
      "Iteration 153, loss = 0.22750857\n",
      "Iteration 154, loss = 0.22838875\n",
      "Iteration 155, loss = 0.22886524\n",
      "Iteration 156, loss = 0.22699107\n",
      "Iteration 157, loss = 0.22749208\n",
      "Iteration 158, loss = 0.22817952\n",
      "Iteration 159, loss = 0.22820454\n",
      "Iteration 160, loss = 0.22688933\n",
      "Iteration 161, loss = 0.22787316\n",
      "Iteration 162, loss = 0.22669910\n",
      "Iteration 163, loss = 0.22684179\n",
      "Iteration 164, loss = 0.22711120\n",
      "Iteration 165, loss = 0.22815777\n",
      "Iteration 166, loss = 0.22723276\n",
      "Iteration 167, loss = 0.22742209\n",
      "Iteration 168, loss = 0.22707114\n",
      "Iteration 169, loss = 0.22949810\n",
      "Iteration 170, loss = 0.22754174\n",
      "Iteration 171, loss = 0.22683674\n",
      "Iteration 172, loss = 0.22793703\n",
      "Iteration 173, loss = 0.22627547\n",
      "Iteration 174, loss = 0.22712942\n",
      "Iteration 175, loss = 0.22733932\n",
      "Iteration 176, loss = 0.22631076\n",
      "Iteration 177, loss = 0.22538609\n",
      "Iteration 178, loss = 0.22746534\n",
      "Iteration 179, loss = 0.22625945\n",
      "Iteration 180, loss = 0.22603527\n",
      "Iteration 181, loss = 0.22583092\n",
      "Iteration 182, loss = 0.22642386\n",
      "Iteration 183, loss = 0.22626878\n",
      "Iteration 184, loss = 0.22647571\n",
      "Iteration 185, loss = 0.22497204\n",
      "Iteration 186, loss = 0.22510773\n",
      "Iteration 187, loss = 0.22566351\n",
      "Iteration 188, loss = 0.22489196\n",
      "Iteration 189, loss = 0.22596310\n",
      "Iteration 190, loss = 0.22545066\n",
      "Iteration 191, loss = 0.22613982\n",
      "Iteration 192, loss = 0.22589902\n",
      "Iteration 193, loss = 0.22524523\n",
      "Iteration 194, loss = 0.22572392\n",
      "Iteration 195, loss = 0.22492809\n",
      "Iteration 196, loss = 0.22432370\n",
      "Iteration 197, loss = 0.22469871\n",
      "Iteration 198, loss = 0.22562843\n",
      "Iteration 199, loss = 0.22646098\n",
      "Iteration 200, loss = 0.22500284\n",
      "Iteration 201, loss = 0.22608764\n",
      "Iteration 202, loss = 0.22459714\n",
      "Iteration 203, loss = 0.22474507\n",
      "Iteration 204, loss = 0.22434384\n",
      "Iteration 205, loss = 0.22534769\n",
      "Iteration 206, loss = 0.22360134\n",
      "Iteration 207, loss = 0.22333487\n",
      "Iteration 208, loss = 0.22410783\n",
      "Iteration 209, loss = 0.22486186\n",
      "Iteration 210, loss = 0.22446747\n",
      "Iteration 211, loss = 0.22470578\n",
      "Iteration 212, loss = 0.22376358\n",
      "Iteration 213, loss = 0.22463613\n",
      "Iteration 214, loss = 0.22472439\n",
      "Iteration 215, loss = 0.22307334\n",
      "Iteration 216, loss = 0.22410421\n",
      "Iteration 217, loss = 0.22326833\n",
      "Iteration 218, loss = 0.22355426\n",
      "Iteration 219, loss = 0.22426826\n",
      "Iteration 220, loss = 0.22298084\n",
      "Iteration 221, loss = 0.22340107\n",
      "Iteration 222, loss = 0.22400805\n",
      "Iteration 223, loss = 0.22387044\n",
      "Iteration 224, loss = 0.22434151\n",
      "Iteration 225, loss = 0.22435994\n",
      "Iteration 226, loss = 0.22257477\n",
      "Iteration 227, loss = 0.22426884\n",
      "Iteration 228, loss = 0.22404068\n",
      "Iteration 229, loss = 0.22369039\n",
      "Iteration 230, loss = 0.22523532\n",
      "Iteration 231, loss = 0.22307248\n",
      "Iteration 232, loss = 0.22293618\n",
      "Iteration 233, loss = 0.22165004\n",
      "Iteration 234, loss = 0.22369696\n",
      "Iteration 235, loss = 0.22430924\n",
      "Iteration 236, loss = 0.22338883\n",
      "Iteration 237, loss = 0.22284460\n",
      "Iteration 238, loss = 0.22312993\n",
      "Iteration 239, loss = 0.22348484\n",
      "Iteration 240, loss = 0.22201345\n",
      "Iteration 241, loss = 0.22254676\n",
      "Iteration 242, loss = 0.22288656\n",
      "Iteration 243, loss = 0.22154496\n",
      "Iteration 244, loss = 0.22147623\n",
      "Iteration 245, loss = 0.22247552\n",
      "Iteration 246, loss = 0.22296334\n",
      "Iteration 247, loss = 0.22217288\n",
      "Iteration 248, loss = 0.22280789\n",
      "Iteration 249, loss = 0.22183529\n",
      "Iteration 250, loss = 0.22151983\n",
      "Iteration 251, loss = 0.22138226\n",
      "Iteration 1, loss = 0.41950013\n",
      "Iteration 2, loss = 0.35947995\n",
      "Iteration 3, loss = 0.34715154\n",
      "Iteration 4, loss = 0.33853379\n",
      "Iteration 5, loss = 0.33076563\n",
      "Iteration 6, loss = 0.32602897\n",
      "Iteration 7, loss = 0.32260993\n",
      "Iteration 8, loss = 0.31890063\n",
      "Iteration 9, loss = 0.31278507\n",
      "Iteration 10, loss = 0.30712618\n",
      "Iteration 11, loss = 0.30342862\n",
      "Iteration 12, loss = 0.29909694\n",
      "Iteration 13, loss = 0.29573207\n",
      "Iteration 14, loss = 0.29169488\n",
      "Iteration 15, loss = 0.29041709\n",
      "Iteration 16, loss = 0.28756977\n",
      "Iteration 17, loss = 0.28731327\n",
      "Iteration 18, loss = 0.28466933\n",
      "Iteration 19, loss = 0.28307872\n",
      "Iteration 20, loss = 0.28144355\n",
      "Iteration 21, loss = 0.27824161\n",
      "Iteration 22, loss = 0.27645172\n",
      "Iteration 23, loss = 0.27631374\n",
      "Iteration 24, loss = 0.27296271\n",
      "Iteration 25, loss = 0.27126973\n",
      "Iteration 26, loss = 0.26902900\n",
      "Iteration 27, loss = 0.26882050\n",
      "Iteration 28, loss = 0.26649257\n",
      "Iteration 29, loss = 0.26301996\n",
      "Iteration 30, loss = 0.26017758\n",
      "Iteration 31, loss = 0.25989905\n",
      "Iteration 32, loss = 0.25927894\n",
      "Iteration 33, loss = 0.25750092\n",
      "Iteration 34, loss = 0.25751161\n",
      "Iteration 35, loss = 0.25521373\n",
      "Iteration 36, loss = 0.25489710\n",
      "Iteration 37, loss = 0.25223066\n",
      "Iteration 38, loss = 0.25188371\n",
      "Iteration 39, loss = 0.24991867\n",
      "Iteration 40, loss = 0.25074804\n",
      "Iteration 41, loss = 0.24964254\n",
      "Iteration 42, loss = 0.25067928\n",
      "Iteration 43, loss = 0.24784764\n",
      "Iteration 44, loss = 0.24734352\n",
      "Iteration 45, loss = 0.24751534\n",
      "Iteration 46, loss = 0.24699151\n",
      "Iteration 47, loss = 0.24672849\n",
      "Iteration 48, loss = 0.24645409\n",
      "Iteration 49, loss = 0.24558089\n",
      "Iteration 50, loss = 0.24600343\n",
      "Iteration 51, loss = 0.24593620\n",
      "Iteration 52, loss = 0.24412605\n",
      "Iteration 53, loss = 0.24490690\n",
      "Iteration 54, loss = 0.24328007\n",
      "Iteration 55, loss = 0.24460756\n",
      "Iteration 56, loss = 0.24286288\n",
      "Iteration 57, loss = 0.24159824\n",
      "Iteration 58, loss = 0.24238795\n",
      "Iteration 59, loss = 0.24214417\n",
      "Iteration 60, loss = 0.24072129\n",
      "Iteration 61, loss = 0.23962207\n",
      "Iteration 62, loss = 0.24006232\n",
      "Iteration 63, loss = 0.24056347\n",
      "Iteration 64, loss = 0.24109640\n",
      "Iteration 65, loss = 0.24105690\n",
      "Iteration 66, loss = 0.23917268\n",
      "Iteration 67, loss = 0.23880899\n",
      "Iteration 68, loss = 0.24073459\n",
      "Iteration 69, loss = 0.23931695\n",
      "Iteration 70, loss = 0.23909349\n",
      "Iteration 71, loss = 0.23980576\n",
      "Iteration 72, loss = 0.23855742\n",
      "Iteration 73, loss = 0.23854123\n",
      "Iteration 74, loss = 0.24085757\n",
      "Iteration 75, loss = 0.23912133\n",
      "Iteration 76, loss = 0.23952830\n",
      "Iteration 77, loss = 0.23689841\n",
      "Iteration 78, loss = 0.23779096\n",
      "Iteration 79, loss = 0.23798954\n",
      "Iteration 80, loss = 0.23694456\n",
      "Iteration 81, loss = 0.23724908\n",
      "Iteration 82, loss = 0.23622299\n",
      "Iteration 83, loss = 0.23704055\n",
      "Iteration 84, loss = 0.23528876\n",
      "Iteration 85, loss = 0.23620885\n",
      "Iteration 86, loss = 0.23606476\n",
      "Iteration 87, loss = 0.23529545\n",
      "Iteration 88, loss = 0.23708359\n",
      "Iteration 89, loss = 0.23496043\n",
      "Iteration 90, loss = 0.23648551\n",
      "Iteration 91, loss = 0.23473423\n",
      "Iteration 92, loss = 0.23619963\n",
      "Iteration 93, loss = 0.23529862\n",
      "Iteration 94, loss = 0.23413662\n",
      "Iteration 95, loss = 0.23470545\n",
      "Iteration 96, loss = 0.23480540\n",
      "Iteration 97, loss = 0.23323790\n",
      "Iteration 98, loss = 0.23465371\n",
      "Iteration 99, loss = 0.23452615\n",
      "Iteration 100, loss = 0.23334903\n",
      "Iteration 101, loss = 0.23267470\n",
      "Iteration 102, loss = 0.23382096\n",
      "Iteration 103, loss = 0.23310463\n",
      "Iteration 104, loss = 0.23200522\n",
      "Iteration 105, loss = 0.23419509\n",
      "Iteration 106, loss = 0.23397352\n",
      "Iteration 107, loss = 0.23223148\n",
      "Iteration 108, loss = 0.23256364\n",
      "Iteration 109, loss = 0.23268136\n",
      "Iteration 110, loss = 0.23244968\n",
      "Iteration 111, loss = 0.23294786\n",
      "Iteration 112, loss = 0.23156454\n",
      "Iteration 113, loss = 0.23435950\n",
      "Iteration 114, loss = 0.23230561\n",
      "Iteration 115, loss = 0.23336112\n",
      "Iteration 116, loss = 0.23141922\n",
      "Iteration 117, loss = 0.23243211\n",
      "Iteration 118, loss = 0.23097416\n",
      "Iteration 119, loss = 0.23143333\n",
      "Iteration 120, loss = 0.23152677\n",
      "Iteration 121, loss = 0.23088126\n",
      "Iteration 122, loss = 0.23022349\n",
      "Iteration 123, loss = 0.23048655\n",
      "Iteration 124, loss = 0.23140057\n",
      "Iteration 125, loss = 0.23160356\n",
      "Iteration 126, loss = 0.23100009\n",
      "Iteration 127, loss = 0.23086248\n",
      "Iteration 128, loss = 0.23094242\n",
      "Iteration 129, loss = 0.23170584\n",
      "Iteration 130, loss = 0.23080259\n",
      "Iteration 131, loss = 0.23107666\n",
      "Iteration 132, loss = 0.23000193\n",
      "Iteration 133, loss = 0.22872136\n",
      "Iteration 134, loss = 0.22983424\n",
      "Iteration 135, loss = 0.22930763\n",
      "Iteration 136, loss = 0.22998942\n",
      "Iteration 137, loss = 0.23019294\n",
      "Iteration 138, loss = 0.22892785\n",
      "Iteration 139, loss = 0.23119341\n",
      "Iteration 140, loss = 0.23035541\n",
      "Iteration 141, loss = 0.22971785\n",
      "Iteration 142, loss = 0.22892877\n",
      "Iteration 143, loss = 0.22819131\n",
      "Iteration 144, loss = 0.22797283\n",
      "Iteration 145, loss = 0.23052789\n",
      "Iteration 146, loss = 0.22803786\n",
      "Iteration 147, loss = 0.22720151\n",
      "Iteration 148, loss = 0.22914446\n",
      "Iteration 149, loss = 0.22955676\n",
      "Iteration 150, loss = 0.22839511\n",
      "Iteration 151, loss = 0.22768776\n",
      "Iteration 152, loss = 0.22915034\n",
      "Iteration 153, loss = 0.22750857\n",
      "Iteration 154, loss = 0.22838875\n",
      "Iteration 155, loss = 0.22886524\n",
      "Iteration 156, loss = 0.22699107\n",
      "Iteration 157, loss = 0.22749208\n",
      "Iteration 158, loss = 0.22817952\n",
      "Iteration 159, loss = 0.22820454\n",
      "Iteration 160, loss = 0.22688933\n",
      "Iteration 161, loss = 0.22787316\n",
      "Iteration 162, loss = 0.22669910\n",
      "Iteration 163, loss = 0.22684179\n",
      "Iteration 164, loss = 0.22711120\n",
      "Iteration 165, loss = 0.22815777\n",
      "Iteration 166, loss = 0.22723276\n",
      "Iteration 167, loss = 0.22742209\n",
      "Iteration 168, loss = 0.22707114\n",
      "Iteration 169, loss = 0.22949810\n",
      "Iteration 170, loss = 0.22754174\n",
      "Iteration 171, loss = 0.22683674\n",
      "Iteration 172, loss = 0.22793703\n",
      "Iteration 173, loss = 0.22627547\n",
      "Iteration 174, loss = 0.22712942\n",
      "Iteration 175, loss = 0.22733932\n",
      "Iteration 176, loss = 0.22631076\n",
      "Iteration 177, loss = 0.22538609\n",
      "Iteration 178, loss = 0.22746534\n",
      "Iteration 179, loss = 0.22625945\n",
      "Iteration 180, loss = 0.22603527\n",
      "Iteration 181, loss = 0.22583092\n",
      "Iteration 182, loss = 0.22642386\n",
      "Iteration 183, loss = 0.22626878\n",
      "Iteration 184, loss = 0.22647571\n",
      "Iteration 185, loss = 0.22497204\n",
      "Iteration 186, loss = 0.22510773\n",
      "Iteration 187, loss = 0.22566351\n",
      "Iteration 188, loss = 0.22489196\n",
      "Iteration 189, loss = 0.22596310\n",
      "Iteration 190, loss = 0.22545066\n",
      "Iteration 191, loss = 0.22613982\n",
      "Iteration 192, loss = 0.22589902\n",
      "Iteration 193, loss = 0.22524523\n",
      "Iteration 194, loss = 0.22572392\n",
      "Iteration 195, loss = 0.22492809\n",
      "Iteration 196, loss = 0.22432370\n",
      "Iteration 197, loss = 0.22469871\n",
      "Iteration 198, loss = 0.22562843\n",
      "Iteration 199, loss = 0.22646098\n",
      "Iteration 200, loss = 0.22500284\n",
      "Iteration 201, loss = 0.22608764\n",
      "Iteration 202, loss = 0.22459714\n",
      "Iteration 203, loss = 0.22474507\n",
      "Iteration 204, loss = 0.22434384\n",
      "Iteration 205, loss = 0.22534769\n",
      "Iteration 206, loss = 0.22360134\n",
      "Iteration 207, loss = 0.22333487\n",
      "Iteration 208, loss = 0.22410783\n",
      "Iteration 209, loss = 0.22486186\n",
      "Iteration 210, loss = 0.22446747\n",
      "Iteration 211, loss = 0.22470578\n",
      "Iteration 212, loss = 0.22376358\n",
      "Iteration 213, loss = 0.22463613\n",
      "Iteration 214, loss = 0.22472439\n",
      "Iteration 215, loss = 0.22307334\n",
      "Iteration 216, loss = 0.22410421\n",
      "Iteration 217, loss = 0.22326833\n",
      "Iteration 218, loss = 0.22355426\n",
      "Iteration 219, loss = 0.22426826\n",
      "Iteration 220, loss = 0.22298084\n",
      "Iteration 221, loss = 0.22340107\n",
      "Iteration 222, loss = 0.22400805\n",
      "Iteration 223, loss = 0.22387044\n",
      "Iteration 224, loss = 0.22434151\n",
      "Iteration 225, loss = 0.22435994\n",
      "Iteration 226, loss = 0.22257477\n",
      "Iteration 227, loss = 0.22426884\n",
      "Iteration 228, loss = 0.22404068\n",
      "Iteration 229, loss = 0.22369039\n",
      "Iteration 230, loss = 0.22523532\n",
      "Iteration 231, loss = 0.22307248\n",
      "Iteration 232, loss = 0.22293618\n",
      "Iteration 233, loss = 0.22165004\n",
      "Iteration 234, loss = 0.22369696\n",
      "Iteration 235, loss = 0.22430924\n",
      "Iteration 236, loss = 0.22338883\n",
      "Iteration 237, loss = 0.22284460\n",
      "Iteration 238, loss = 0.22312993\n",
      "Iteration 239, loss = 0.22348484\n",
      "Iteration 240, loss = 0.22201345\n",
      "Iteration 241, loss = 0.22254676\n",
      "Iteration 242, loss = 0.22288656\n",
      "Iteration 243, loss = 0.22154496\n",
      "Iteration 244, loss = 0.22147623\n",
      "Iteration 245, loss = 0.22247552\n",
      "Iteration 246, loss = 0.22296334\n",
      "Iteration 247, loss = 0.22217288\n",
      "Iteration 248, loss = 0.22280789\n",
      "Iteration 249, loss = 0.22183529\n",
      "Iteration 250, loss = 0.22151983\n",
      "Iteration 251, loss = 0.22138226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.41994397\n",
      "Iteration 2, loss = 0.35968288\n",
      "Iteration 3, loss = 0.34732312\n",
      "Iteration 4, loss = 0.34004328\n",
      "Iteration 5, loss = 0.33307538\n",
      "Iteration 6, loss = 0.32690443\n",
      "Iteration 7, loss = 0.32378484\n",
      "Iteration 8, loss = 0.31918060\n",
      "Iteration 9, loss = 0.31414745\n",
      "Iteration 10, loss = 0.30907707\n",
      "Iteration 11, loss = 0.30351506\n",
      "Iteration 12, loss = 0.29993285\n",
      "Iteration 13, loss = 0.29784088\n",
      "Iteration 14, loss = 0.29370831\n",
      "Iteration 15, loss = 0.29064966\n",
      "Iteration 16, loss = 0.28885549\n",
      "Iteration 17, loss = 0.28768360\n",
      "Iteration 18, loss = 0.28517379\n",
      "Iteration 19, loss = 0.28214756\n",
      "Iteration 20, loss = 0.28049577\n",
      "Iteration 21, loss = 0.27897162\n",
      "Iteration 22, loss = 0.27682900\n",
      "Iteration 23, loss = 0.27638972\n",
      "Iteration 24, loss = 0.27353858\n",
      "Iteration 25, loss = 0.27323029\n",
      "Iteration 26, loss = 0.27002129\n",
      "Iteration 27, loss = 0.26934200\n",
      "Iteration 28, loss = 0.26729355\n",
      "Iteration 29, loss = 0.26584277\n",
      "Iteration 30, loss = 0.26517104\n",
      "Iteration 31, loss = 0.26170337\n",
      "Iteration 32, loss = 0.26201032\n",
      "Iteration 33, loss = 0.26020015\n",
      "Iteration 34, loss = 0.25803580\n",
      "Iteration 35, loss = 0.25523584\n",
      "Iteration 36, loss = 0.25581240\n",
      "Iteration 37, loss = 0.25473733\n",
      "Iteration 38, loss = 0.25378989\n",
      "Iteration 39, loss = 0.25269086\n",
      "Iteration 40, loss = 0.25232031\n",
      "Iteration 41, loss = 0.25053735\n",
      "Iteration 42, loss = 0.25293245\n",
      "Iteration 43, loss = 0.24946081\n",
      "Iteration 44, loss = 0.24891688\n",
      "Iteration 45, loss = 0.24942576\n",
      "Iteration 46, loss = 0.24776506\n",
      "Iteration 47, loss = 0.24831129\n",
      "Iteration 48, loss = 0.24789964\n",
      "Iteration 49, loss = 0.24495857\n",
      "Iteration 50, loss = 0.24693662\n",
      "Iteration 51, loss = 0.24526438\n",
      "Iteration 52, loss = 0.24597180\n",
      "Iteration 53, loss = 0.24511476\n",
      "Iteration 54, loss = 0.24519692\n",
      "Iteration 55, loss = 0.24325179\n",
      "Iteration 56, loss = 0.24452596\n",
      "Iteration 57, loss = 0.24376880\n",
      "Iteration 58, loss = 0.24288801\n",
      "Iteration 59, loss = 0.24260449\n",
      "Iteration 60, loss = 0.24501960\n",
      "Iteration 61, loss = 0.24301751\n",
      "Iteration 62, loss = 0.24148391\n",
      "Iteration 63, loss = 0.24176422\n",
      "Iteration 64, loss = 0.24000126\n",
      "Iteration 65, loss = 0.24085225\n",
      "Iteration 66, loss = 0.24050139\n",
      "Iteration 67, loss = 0.24152783\n",
      "Iteration 68, loss = 0.24154239\n",
      "Iteration 69, loss = 0.23959004\n",
      "Iteration 70, loss = 0.24180514\n",
      "Iteration 71, loss = 0.23971148\n",
      "Iteration 72, loss = 0.23866693\n",
      "Iteration 73, loss = 0.23914356\n",
      "Iteration 74, loss = 0.23983085\n",
      "Iteration 75, loss = 0.23698148\n",
      "Iteration 76, loss = 0.23888213\n",
      "Iteration 77, loss = 0.23828280\n",
      "Iteration 78, loss = 0.23668320\n",
      "Iteration 79, loss = 0.23779213\n",
      "Iteration 80, loss = 0.23860288\n",
      "Iteration 81, loss = 0.23704601\n",
      "Iteration 82, loss = 0.23742826\n",
      "Iteration 83, loss = 0.23714892\n",
      "Iteration 84, loss = 0.23735483\n",
      "Iteration 85, loss = 0.23724074\n",
      "Iteration 86, loss = 0.23651811\n",
      "Iteration 87, loss = 0.23670219\n",
      "Iteration 88, loss = 0.23698064\n",
      "Iteration 89, loss = 0.23533640\n",
      "Iteration 90, loss = 0.23607374\n",
      "Iteration 91, loss = 0.23487034\n",
      "Iteration 92, loss = 0.23598853\n",
      "Iteration 93, loss = 0.23470332\n",
      "Iteration 94, loss = 0.23425804\n",
      "Iteration 95, loss = 0.23524356\n",
      "Iteration 96, loss = 0.23514037\n",
      "Iteration 97, loss = 0.23509575\n",
      "Iteration 98, loss = 0.23460086\n",
      "Iteration 99, loss = 0.23346265\n",
      "Iteration 100, loss = 0.23394175\n",
      "Iteration 101, loss = 0.23378638\n",
      "Iteration 102, loss = 0.23474765\n",
      "Iteration 103, loss = 0.23391420\n",
      "Iteration 104, loss = 0.23329590\n",
      "Iteration 105, loss = 0.23269190\n",
      "Iteration 106, loss = 0.23203212\n",
      "Iteration 107, loss = 0.23209230\n",
      "Iteration 108, loss = 0.23330392\n",
      "Iteration 109, loss = 0.23359744\n",
      "Iteration 110, loss = 0.23144552\n",
      "Iteration 111, loss = 0.23284296\n",
      "Iteration 112, loss = 0.23295601\n",
      "Iteration 113, loss = 0.23395778\n",
      "Iteration 114, loss = 0.23211433\n",
      "Iteration 115, loss = 0.23342684\n",
      "Iteration 116, loss = 0.23249238\n",
      "Iteration 117, loss = 0.23259336\n",
      "Iteration 118, loss = 0.23049275\n",
      "Iteration 119, loss = 0.23314497\n",
      "Iteration 120, loss = 0.23085056\n",
      "Iteration 121, loss = 0.23208116\n",
      "Iteration 122, loss = 0.23267921\n",
      "Iteration 123, loss = 0.23116088\n",
      "Iteration 124, loss = 0.22989682\n",
      "Iteration 125, loss = 0.23144199\n",
      "Iteration 126, loss = 0.23125036\n",
      "Iteration 127, loss = 0.23041045\n",
      "Iteration 128, loss = 0.23003281\n",
      "Iteration 129, loss = 0.23115963\n",
      "Iteration 130, loss = 0.22974921\n",
      "Iteration 131, loss = 0.23036962\n",
      "Iteration 132, loss = 0.22951125\n",
      "Iteration 133, loss = 0.22910953\n",
      "Iteration 134, loss = 0.23143844\n",
      "Iteration 135, loss = 0.23024043\n",
      "Iteration 136, loss = 0.23133129\n",
      "Iteration 137, loss = 0.22973264\n",
      "Iteration 138, loss = 0.22938021\n",
      "Iteration 139, loss = 0.23037853\n",
      "Iteration 140, loss = 0.23113714\n",
      "Iteration 141, loss = 0.22966229\n",
      "Iteration 142, loss = 0.23001677\n",
      "Iteration 143, loss = 0.22865002\n",
      "Iteration 144, loss = 0.22994113\n",
      "Iteration 145, loss = 0.23031604\n",
      "Iteration 146, loss = 0.23077552\n",
      "Iteration 147, loss = 0.22872129\n",
      "Iteration 148, loss = 0.22930783\n",
      "Iteration 149, loss = 0.23027979\n",
      "Iteration 150, loss = 0.22928639\n",
      "Iteration 151, loss = 0.22900237\n",
      "Iteration 152, loss = 0.22830509\n",
      "Iteration 153, loss = 0.22954907\n",
      "Iteration 154, loss = 0.23080408\n",
      "Iteration 155, loss = 0.22916890\n",
      "Iteration 156, loss = 0.22871142\n",
      "Iteration 157, loss = 0.22861297\n",
      "Iteration 158, loss = 0.22790033\n",
      "Iteration 159, loss = 0.22851492\n",
      "Iteration 160, loss = 0.22908550\n",
      "Iteration 161, loss = 0.23045454\n",
      "Iteration 162, loss = 0.22909494\n",
      "Iteration 163, loss = 0.22848454\n",
      "Iteration 164, loss = 0.22842841\n",
      "Iteration 165, loss = 0.22784412\n",
      "Iteration 166, loss = 0.22824025\n",
      "Iteration 167, loss = 0.22758098\n",
      "Iteration 168, loss = 0.22778030\n",
      "Iteration 169, loss = 0.22589299\n",
      "Iteration 170, loss = 0.22651576\n",
      "Iteration 171, loss = 0.22720371\n",
      "Iteration 172, loss = 0.22907766\n",
      "Iteration 173, loss = 0.22760469\n",
      "Iteration 174, loss = 0.22727622\n",
      "Iteration 175, loss = 0.22781408\n",
      "Iteration 176, loss = 0.22593492\n",
      "Iteration 177, loss = 0.22592808\n",
      "Iteration 178, loss = 0.22786822\n",
      "Iteration 179, loss = 0.22724087\n",
      "Iteration 180, loss = 0.22688105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62134368\n",
      "Iteration 2, loss = 0.52424977\n",
      "Iteration 3, loss = 0.43825390\n",
      "Iteration 4, loss = 0.39271517\n",
      "Iteration 5, loss = 0.37542648\n",
      "Iteration 6, loss = 0.36733392\n",
      "Iteration 7, loss = 0.36253933\n",
      "Iteration 8, loss = 0.35944747\n",
      "Iteration 9, loss = 0.35741122\n",
      "Iteration 10, loss = 0.35524609\n",
      "Iteration 11, loss = 0.35318117\n",
      "Iteration 12, loss = 0.35178371\n",
      "Iteration 13, loss = 0.35088826\n",
      "Iteration 14, loss = 0.35008101\n",
      "Iteration 15, loss = 0.34821844\n",
      "Iteration 16, loss = 0.34702224\n",
      "Iteration 17, loss = 0.34576944\n",
      "Iteration 18, loss = 0.34496775\n",
      "Iteration 19, loss = 0.34378409\n",
      "Iteration 20, loss = 0.34269395\n",
      "Iteration 21, loss = 0.34239903\n",
      "Iteration 22, loss = 0.34089089\n",
      "Iteration 23, loss = 0.34010091\n",
      "Iteration 24, loss = 0.33952713\n",
      "Iteration 25, loss = 0.33880236\n",
      "Iteration 26, loss = 0.33828706\n",
      "Iteration 27, loss = 0.33685993\n",
      "Iteration 28, loss = 0.33659556\n",
      "Iteration 29, loss = 0.33535443\n",
      "Iteration 30, loss = 0.33463638\n",
      "Iteration 31, loss = 0.33459480\n",
      "Iteration 32, loss = 0.33323776\n",
      "Iteration 33, loss = 0.33296416\n",
      "Iteration 34, loss = 0.33185878\n",
      "Iteration 35, loss = 0.33154047\n",
      "Iteration 36, loss = 0.33054715\n",
      "Iteration 37, loss = 0.33025678\n",
      "Iteration 38, loss = 0.32980372\n",
      "Iteration 39, loss = 0.32892637\n",
      "Iteration 40, loss = 0.32811033\n",
      "Iteration 41, loss = 0.32797348\n",
      "Iteration 42, loss = 0.32720743\n",
      "Iteration 43, loss = 0.32730413\n",
      "Iteration 44, loss = 0.32609653\n",
      "Iteration 45, loss = 0.32564314\n",
      "Iteration 46, loss = 0.32482061\n",
      "Iteration 47, loss = 0.32476048\n",
      "Iteration 48, loss = 0.32409960\n",
      "Iteration 49, loss = 0.32351134\n",
      "Iteration 50, loss = 0.32302700\n",
      "Iteration 51, loss = 0.32237780\n",
      "Iteration 52, loss = 0.32198298\n",
      "Iteration 53, loss = 0.32104761\n",
      "Iteration 54, loss = 0.32045279\n",
      "Iteration 55, loss = 0.32079335\n",
      "Iteration 56, loss = 0.31969392\n",
      "Iteration 57, loss = 0.31910343\n",
      "Iteration 58, loss = 0.31876443\n",
      "Iteration 59, loss = 0.31871310\n",
      "Iteration 60, loss = 0.31752167\n",
      "Iteration 61, loss = 0.31732487\n",
      "Iteration 62, loss = 0.31694016\n",
      "Iteration 63, loss = 0.31644295\n",
      "Iteration 64, loss = 0.31645338\n",
      "Iteration 65, loss = 0.31550827\n",
      "Iteration 66, loss = 0.31512816\n",
      "Iteration 67, loss = 0.31455172\n",
      "Iteration 68, loss = 0.31434720\n",
      "Iteration 69, loss = 0.31413353\n",
      "Iteration 70, loss = 0.31351641\n",
      "Iteration 71, loss = 0.31310094\n",
      "Iteration 1, loss = 0.41994397\n",
      "Iteration 2, loss = 0.35968288\n",
      "Iteration 3, loss = 0.34732312\n",
      "Iteration 4, loss = 0.34004328\n",
      "Iteration 5, loss = 0.33307538\n",
      "Iteration 6, loss = 0.32690443\n",
      "Iteration 7, loss = 0.32378484\n",
      "Iteration 8, loss = 0.31918060\n",
      "Iteration 9, loss = 0.31414745\n",
      "Iteration 10, loss = 0.30907707\n",
      "Iteration 11, loss = 0.30351506\n",
      "Iteration 12, loss = 0.29993285\n",
      "Iteration 13, loss = 0.29784088\n",
      "Iteration 14, loss = 0.29370831\n",
      "Iteration 15, loss = 0.29064966\n",
      "Iteration 16, loss = 0.28885549\n",
      "Iteration 17, loss = 0.28768360\n",
      "Iteration 18, loss = 0.28517379\n",
      "Iteration 19, loss = 0.28214756\n",
      "Iteration 20, loss = 0.28049577\n",
      "Iteration 21, loss = 0.27897162\n",
      "Iteration 22, loss = 0.27682900\n",
      "Iteration 23, loss = 0.27638972\n",
      "Iteration 24, loss = 0.27353858\n",
      "Iteration 25, loss = 0.27323029\n",
      "Iteration 26, loss = 0.27002129\n",
      "Iteration 27, loss = 0.26934200\n",
      "Iteration 28, loss = 0.26729355\n",
      "Iteration 29, loss = 0.26584277\n",
      "Iteration 30, loss = 0.26517104\n",
      "Iteration 31, loss = 0.26170337\n",
      "Iteration 32, loss = 0.26201032\n",
      "Iteration 33, loss = 0.26020015\n",
      "Iteration 34, loss = 0.25803580\n",
      "Iteration 35, loss = 0.25523584\n",
      "Iteration 36, loss = 0.25581240\n",
      "Iteration 37, loss = 0.25473733\n",
      "Iteration 38, loss = 0.25378989\n",
      "Iteration 39, loss = 0.25269086\n",
      "Iteration 40, loss = 0.25232031\n",
      "Iteration 41, loss = 0.25053735\n",
      "Iteration 42, loss = 0.25293245\n",
      "Iteration 43, loss = 0.24946081\n",
      "Iteration 44, loss = 0.24891688\n",
      "Iteration 45, loss = 0.24942576\n",
      "Iteration 46, loss = 0.24776506\n",
      "Iteration 47, loss = 0.24831129\n",
      "Iteration 48, loss = 0.24789964\n",
      "Iteration 49, loss = 0.24495857\n",
      "Iteration 50, loss = 0.24693662\n",
      "Iteration 51, loss = 0.24526438\n",
      "Iteration 52, loss = 0.24597180\n",
      "Iteration 53, loss = 0.24511476\n",
      "Iteration 54, loss = 0.24519692\n",
      "Iteration 55, loss = 0.24325179\n",
      "Iteration 56, loss = 0.24452596\n",
      "Iteration 57, loss = 0.24376880\n",
      "Iteration 58, loss = 0.24288801\n",
      "Iteration 59, loss = 0.24260449\n",
      "Iteration 60, loss = 0.24501960\n",
      "Iteration 61, loss = 0.24301751\n",
      "Iteration 62, loss = 0.24148391\n",
      "Iteration 63, loss = 0.24176422\n",
      "Iteration 64, loss = 0.24000126\n",
      "Iteration 65, loss = 0.24085225\n",
      "Iteration 66, loss = 0.24050139\n",
      "Iteration 67, loss = 0.24152783\n",
      "Iteration 68, loss = 0.24154239\n",
      "Iteration 69, loss = 0.23959004\n",
      "Iteration 70, loss = 0.24180514\n",
      "Iteration 71, loss = 0.23971148\n",
      "Iteration 72, loss = 0.23866693\n",
      "Iteration 73, loss = 0.23914356\n",
      "Iteration 74, loss = 0.23983085\n",
      "Iteration 75, loss = 0.23698148\n",
      "Iteration 76, loss = 0.23888213\n",
      "Iteration 77, loss = 0.23828280\n",
      "Iteration 78, loss = 0.23668320\n",
      "Iteration 79, loss = 0.23779213\n",
      "Iteration 80, loss = 0.23860288\n",
      "Iteration 81, loss = 0.23704601\n",
      "Iteration 82, loss = 0.23742826\n",
      "Iteration 83, loss = 0.23714892\n",
      "Iteration 84, loss = 0.23735483\n",
      "Iteration 85, loss = 0.23724074\n",
      "Iteration 86, loss = 0.23651811\n",
      "Iteration 87, loss = 0.23670219\n",
      "Iteration 88, loss = 0.23698064\n",
      "Iteration 89, loss = 0.23533640\n",
      "Iteration 90, loss = 0.23607374\n",
      "Iteration 91, loss = 0.23487034\n",
      "Iteration 92, loss = 0.23598853\n",
      "Iteration 93, loss = 0.23470332\n",
      "Iteration 94, loss = 0.23425804\n",
      "Iteration 95, loss = 0.23524356\n",
      "Iteration 96, loss = 0.23514037\n",
      "Iteration 97, loss = 0.23509575\n",
      "Iteration 98, loss = 0.23460086\n",
      "Iteration 99, loss = 0.23346265\n",
      "Iteration 100, loss = 0.23394175\n",
      "Iteration 101, loss = 0.23378638\n",
      "Iteration 102, loss = 0.23474765\n",
      "Iteration 103, loss = 0.23391420\n",
      "Iteration 104, loss = 0.23329590\n",
      "Iteration 105, loss = 0.23269190\n",
      "Iteration 106, loss = 0.23203212\n",
      "Iteration 107, loss = 0.23209230\n",
      "Iteration 108, loss = 0.23330392\n",
      "Iteration 109, loss = 0.23359744\n",
      "Iteration 110, loss = 0.23144552\n",
      "Iteration 111, loss = 0.23284296\n",
      "Iteration 112, loss = 0.23295601\n",
      "Iteration 113, loss = 0.23395778\n",
      "Iteration 114, loss = 0.23211433\n",
      "Iteration 115, loss = 0.23342684\n",
      "Iteration 116, loss = 0.23249238\n",
      "Iteration 117, loss = 0.23259336\n",
      "Iteration 118, loss = 0.23049275\n",
      "Iteration 119, loss = 0.23314497\n",
      "Iteration 120, loss = 0.23085056\n",
      "Iteration 121, loss = 0.23208116\n",
      "Iteration 122, loss = 0.23267921\n",
      "Iteration 123, loss = 0.23116088\n",
      "Iteration 124, loss = 0.22989682\n",
      "Iteration 125, loss = 0.23144199\n",
      "Iteration 126, loss = 0.23125036\n",
      "Iteration 127, loss = 0.23041045\n",
      "Iteration 128, loss = 0.23003281\n",
      "Iteration 129, loss = 0.23115963\n",
      "Iteration 130, loss = 0.22974921\n",
      "Iteration 131, loss = 0.23036962\n",
      "Iteration 132, loss = 0.22951125\n",
      "Iteration 133, loss = 0.22910953\n",
      "Iteration 134, loss = 0.23143844\n",
      "Iteration 135, loss = 0.23024043\n",
      "Iteration 136, loss = 0.23133129\n",
      "Iteration 137, loss = 0.22973264\n",
      "Iteration 138, loss = 0.22938021\n",
      "Iteration 139, loss = 0.23037853\n",
      "Iteration 140, loss = 0.23113714\n",
      "Iteration 141, loss = 0.22966229\n",
      "Iteration 142, loss = 0.23001677\n",
      "Iteration 143, loss = 0.22865002\n",
      "Iteration 144, loss = 0.22994113\n",
      "Iteration 145, loss = 0.23031604\n",
      "Iteration 146, loss = 0.23077552\n",
      "Iteration 147, loss = 0.22872129\n",
      "Iteration 148, loss = 0.22930783\n",
      "Iteration 149, loss = 0.23027979\n",
      "Iteration 150, loss = 0.22928639\n",
      "Iteration 151, loss = 0.22900237\n",
      "Iteration 152, loss = 0.22830509\n",
      "Iteration 153, loss = 0.22954907\n",
      "Iteration 154, loss = 0.23080408\n",
      "Iteration 155, loss = 0.22916890\n",
      "Iteration 156, loss = 0.22871142\n",
      "Iteration 157, loss = 0.22861297\n",
      "Iteration 158, loss = 0.22790033\n",
      "Iteration 159, loss = 0.22851492\n",
      "Iteration 160, loss = 0.22908550\n",
      "Iteration 161, loss = 0.23045454\n",
      "Iteration 162, loss = 0.22909494\n",
      "Iteration 163, loss = 0.22848454\n",
      "Iteration 164, loss = 0.22842841\n",
      "Iteration 165, loss = 0.22784412\n",
      "Iteration 166, loss = 0.22824025\n",
      "Iteration 167, loss = 0.22758098\n",
      "Iteration 168, loss = 0.22778030\n",
      "Iteration 169, loss = 0.22589299\n",
      "Iteration 170, loss = 0.22651576\n",
      "Iteration 171, loss = 0.22720371\n",
      "Iteration 172, loss = 0.22907766\n",
      "Iteration 173, loss = 0.22760469\n",
      "Iteration 174, loss = 0.22727622\n",
      "Iteration 175, loss = 0.22781408\n",
      "Iteration 176, loss = 0.22593492\n",
      "Iteration 177, loss = 0.22592808\n",
      "Iteration 178, loss = 0.22786822\n",
      "Iteration 179, loss = 0.22724087\n",
      "Iteration 180, loss = 0.22688105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42106300\n",
      "Iteration 2, loss = 0.35826820\n",
      "Iteration 3, loss = 0.34670221\n",
      "Iteration 4, loss = 0.33945670\n",
      "Iteration 5, loss = 0.33276656\n",
      "Iteration 6, loss = 0.32703638\n",
      "Iteration 7, loss = 0.32094046\n",
      "Iteration 8, loss = 0.31632741\n",
      "Iteration 9, loss = 0.30943836\n",
      "Iteration 10, loss = 0.30378596\n",
      "Iteration 11, loss = 0.29925129\n",
      "Iteration 12, loss = 0.29703105\n",
      "Iteration 13, loss = 0.29331334\n",
      "Iteration 14, loss = 0.28873638\n",
      "Iteration 15, loss = 0.28555225\n",
      "Iteration 16, loss = 0.28424634\n",
      "Iteration 17, loss = 0.28172700\n",
      "Iteration 18, loss = 0.27963410\n",
      "Iteration 19, loss = 0.27760659\n",
      "Iteration 20, loss = 0.27670359\n",
      "Iteration 21, loss = 0.27502887\n",
      "Iteration 22, loss = 0.27463021\n",
      "Iteration 23, loss = 0.27187846\n",
      "Iteration 24, loss = 0.27104961\n",
      "Iteration 25, loss = 0.27000289\n",
      "Iteration 26, loss = 0.26894481\n",
      "Iteration 27, loss = 0.26726184\n",
      "Iteration 28, loss = 0.26572763\n",
      "Iteration 29, loss = 0.26445089\n",
      "Iteration 30, loss = 0.26452981\n",
      "Iteration 31, loss = 0.26176219\n",
      "Iteration 32, loss = 0.26156917\n",
      "Iteration 33, loss = 0.26088482\n",
      "Iteration 34, loss = 0.26012401\n",
      "Iteration 35, loss = 0.25664232\n",
      "Iteration 36, loss = 0.25704973\n",
      "Iteration 37, loss = 0.25693376\n",
      "Iteration 38, loss = 0.25633363\n",
      "Iteration 39, loss = 0.25406206\n",
      "Iteration 40, loss = 0.25499255\n",
      "Iteration 41, loss = 0.25322490\n",
      "Iteration 42, loss = 0.25324684\n",
      "Iteration 43, loss = 0.25153665\n",
      "Iteration 44, loss = 0.25025432\n",
      "Iteration 45, loss = 0.24959969\n",
      "Iteration 46, loss = 0.24860056\n",
      "Iteration 47, loss = 0.24938036\n",
      "Iteration 48, loss = 0.24817396\n",
      "Iteration 49, loss = 0.24714103\n",
      "Iteration 50, loss = 0.24752656\n",
      "Iteration 51, loss = 0.24654821\n",
      "Iteration 52, loss = 0.24707852\n",
      "Iteration 53, loss = 0.24700292\n",
      "Iteration 54, loss = 0.24646447\n",
      "Iteration 55, loss = 0.24596822\n",
      "Iteration 56, loss = 0.24417919\n",
      "Iteration 57, loss = 0.24489982\n",
      "Iteration 58, loss = 0.24360699\n",
      "Iteration 59, loss = 0.24524646\n",
      "Iteration 60, loss = 0.24522084\n",
      "Iteration 61, loss = 0.24153405\n",
      "Iteration 62, loss = 0.24513717\n",
      "Iteration 63, loss = 0.24120651\n",
      "Iteration 64, loss = 0.24386370\n",
      "Iteration 65, loss = 0.24165812\n",
      "Iteration 66, loss = 0.24211874\n",
      "Iteration 67, loss = 0.24015806\n",
      "Iteration 68, loss = 0.24166028\n",
      "Iteration 69, loss = 0.24191448\n",
      "Iteration 70, loss = 0.24117435\n",
      "Iteration 71, loss = 0.24179827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.41821725\n",
      "Iteration 2, loss = 0.35938982\n",
      "Iteration 3, loss = 0.34782301\n",
      "Iteration 4, loss = 0.34124944\n",
      "Iteration 5, loss = 0.33300249\n",
      "Iteration 6, loss = 0.32740301\n",
      "Iteration 7, loss = 0.32387178\n",
      "Iteration 8, loss = 0.32019316\n",
      "Iteration 9, loss = 0.31544302\n",
      "Iteration 10, loss = 0.30999776\n",
      "Iteration 11, loss = 0.30571020\n",
      "Iteration 12, loss = 0.30115249\n",
      "Iteration 13, loss = 0.29761386\n",
      "Iteration 14, loss = 0.29424112\n",
      "Iteration 15, loss = 0.29247038\n",
      "Iteration 16, loss = 0.28894527\n",
      "Iteration 17, loss = 0.28780162\n",
      "Iteration 18, loss = 0.28539979\n",
      "Iteration 19, loss = 0.28375012\n",
      "Iteration 20, loss = 0.28333413\n",
      "Iteration 21, loss = 0.28047911\n",
      "Iteration 22, loss = 0.27861651\n",
      "Iteration 23, loss = 0.27844091\n",
      "Iteration 24, loss = 0.27635505\n",
      "Iteration 25, loss = 0.27321848\n",
      "Iteration 26, loss = 0.27130914\n",
      "Iteration 27, loss = 0.27099200\n",
      "Iteration 28, loss = 0.27042182\n",
      "Iteration 29, loss = 0.26961930\n",
      "Iteration 30, loss = 0.26634607\n",
      "Iteration 31, loss = 0.26576133\n",
      "Iteration 32, loss = 0.26458259\n",
      "Iteration 33, loss = 0.26445048\n",
      "Iteration 34, loss = 0.26264336\n",
      "Iteration 35, loss = 0.26149564\n",
      "Iteration 36, loss = 0.25946572\n",
      "Iteration 37, loss = 0.25830947\n",
      "Iteration 38, loss = 0.25619833\n",
      "Iteration 39, loss = 0.25355851\n",
      "Iteration 40, loss = 0.25313687\n",
      "Iteration 41, loss = 0.25323508\n",
      "Iteration 42, loss = 0.25137751\n",
      "Iteration 43, loss = 0.25077272\n",
      "Iteration 44, loss = 0.25057448\n",
      "Iteration 45, loss = 0.24958136\n",
      "Iteration 46, loss = 0.24946139\n",
      "Iteration 47, loss = 0.24973001\n",
      "Iteration 48, loss = 0.24854149\n",
      "Iteration 49, loss = 0.24818432\n",
      "Iteration 50, loss = 0.24629554\n",
      "Iteration 51, loss = 0.24575279\n",
      "Iteration 52, loss = 0.24465859\n",
      "Iteration 53, loss = 0.24503354\n",
      "Iteration 54, loss = 0.24447488\n",
      "Iteration 55, loss = 0.24495465\n",
      "Iteration 56, loss = 0.24252139\n",
      "Iteration 57, loss = 0.24579155\n",
      "Iteration 58, loss = 0.24233824\n",
      "Iteration 59, loss = 0.24227099\n",
      "Iteration 60, loss = 0.24359753\n",
      "Iteration 61, loss = 0.24246413\n",
      "Iteration 62, loss = 0.24273913\n",
      "Iteration 63, loss = 0.24234560\n",
      "Iteration 64, loss = 0.24149541\n",
      "Iteration 65, loss = 0.24062466\n",
      "Iteration 66, loss = 0.24083245\n",
      "Iteration 67, loss = 0.24165994\n",
      "Iteration 68, loss = 0.24250841\n",
      "Iteration 69, loss = 0.24031149\n",
      "Iteration 70, loss = 0.23975518\n",
      "Iteration 71, loss = 0.24013178\n",
      "Iteration 72, loss = 0.23993028\n",
      "Iteration 73, loss = 0.23939248\n",
      "Iteration 74, loss = 0.23958021\n",
      "Iteration 75, loss = 0.23904162\n",
      "Iteration 76, loss = 0.23920977\n",
      "Iteration 77, loss = 0.23972232\n",
      "Iteration 78, loss = 0.23775451\n",
      "Iteration 79, loss = 0.23899254\n",
      "Iteration 80, loss = 0.23791634\n",
      "Iteration 81, loss = 0.23769185\n",
      "Iteration 82, loss = 0.23681649\n",
      "Iteration 83, loss = 0.23701664\n",
      "Iteration 84, loss = 0.23727943\n",
      "Iteration 85, loss = 0.23751297\n",
      "Iteration 86, loss = 0.23722279\n",
      "Iteration 87, loss = 0.23629728\n",
      "Iteration 88, loss = 0.23574445\n",
      "Iteration 89, loss = 0.23569522\n",
      "Iteration 90, loss = 0.23623334\n",
      "Iteration 91, loss = 0.23539799\n",
      "Iteration 92, loss = 0.23513956\n",
      "Iteration 93, loss = 0.23519838\n",
      "Iteration 94, loss = 0.23570623\n",
      "Iteration 95, loss = 0.23449393\n",
      "Iteration 96, loss = 0.23494687\n",
      "Iteration 97, loss = 0.23508374\n",
      "Iteration 98, loss = 0.23589755\n",
      "Iteration 99, loss = 0.23450413\n",
      "Iteration 100, loss = 0.23498462\n",
      "Iteration 101, loss = 0.23491487\n",
      "Iteration 102, loss = 0.23538862\n",
      "Iteration 103, loss = 0.23498332\n",
      "Iteration 104, loss = 0.23296576\n",
      "Iteration 105, loss = 0.23392707\n",
      "Iteration 106, loss = 0.23469302\n",
      "Iteration 107, loss = 0.23371014\n",
      "Iteration 108, loss = 0.23291874\n",
      "Iteration 109, loss = 0.23293848\n",
      "Iteration 110, loss = 0.23283619\n",
      "Iteration 111, loss = 0.23309949\n",
      "Iteration 112, loss = 0.23307192\n",
      "Iteration 113, loss = 0.23288823\n",
      "Iteration 114, loss = 0.23306624\n",
      "Iteration 115, loss = 0.23337440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62152458\n",
      "Iteration 2, loss = 0.52426381\n",
      "Iteration 3, loss = 0.43858505\n",
      "Iteration 4, loss = 0.39284364\n",
      "Iteration 5, loss = 0.37558837\n",
      "Iteration 6, loss = 0.36778607\n",
      "Iteration 7, loss = 0.36355447\n",
      "Iteration 8, loss = 0.36001331\n",
      "Iteration 9, loss = 0.35736533\n",
      "Iteration 10, loss = 0.35562043\n",
      "Iteration 11, loss = 0.35326514\n",
      "Iteration 12, loss = 0.35230108\n",
      "Iteration 13, loss = 0.35104179\n",
      "Iteration 14, loss = 0.35003008\n",
      "Iteration 15, loss = 0.34874163\n",
      "Iteration 16, loss = 0.34754042\n",
      "Iteration 17, loss = 0.34627538\n",
      "Iteration 18, loss = 0.34522998\n",
      "Iteration 19, loss = 0.34411020\n",
      "Iteration 20, loss = 0.34340637\n",
      "Iteration 21, loss = 0.34262084\n",
      "Iteration 22, loss = 0.34159236\n",
      "Iteration 23, loss = 0.34083363\n",
      "Iteration 24, loss = 0.33998732\n",
      "Iteration 25, loss = 0.33916601\n",
      "Iteration 26, loss = 0.33832433\n",
      "Iteration 27, loss = 0.33767235\n",
      "Iteration 28, loss = 0.33687501\n",
      "Iteration 29, loss = 0.33628801\n",
      "Iteration 30, loss = 0.33530624\n",
      "Iteration 31, loss = 0.33522592\n",
      "Iteration 32, loss = 0.33425574\n",
      "Iteration 33, loss = 0.33351290\n",
      "Iteration 34, loss = 0.33287160\n",
      "Iteration 35, loss = 0.33191151\n",
      "Iteration 36, loss = 0.33125617\n",
      "Iteration 37, loss = 0.33140652\n",
      "Iteration 38, loss = 0.33045424\n",
      "Iteration 39, loss = 0.32971324\n",
      "Iteration 40, loss = 0.32931281\n",
      "Iteration 41, loss = 0.32834184\n",
      "Iteration 42, loss = 0.32785150\n",
      "Iteration 43, loss = 0.32755627\n",
      "Iteration 44, loss = 0.32730241\n",
      "Iteration 45, loss = 0.32639239\n",
      "Iteration 46, loss = 0.32599881\n",
      "Iteration 47, loss = 0.32548730\n",
      "Iteration 48, loss = 0.32467674\n",
      "Iteration 49, loss = 0.32403171\n",
      "Iteration 50, loss = 0.32346318\n",
      "Iteration 51, loss = 0.32353247\n",
      "Iteration 52, loss = 0.32308537\n",
      "Iteration 53, loss = 0.32233688\n",
      "Iteration 54, loss = 0.32158431\n",
      "Iteration 55, loss = 0.32099129\n",
      "Iteration 56, loss = 0.32031988\n",
      "Iteration 57, loss = 0.32038483\n",
      "Iteration 58, loss = 0.31937956\n",
      "Iteration 59, loss = 0.31917001\n",
      "Iteration 60, loss = 0.31857594\n",
      "Iteration 61, loss = 0.31793303\n",
      "Iteration 62, loss = 0.31775450\n",
      "Iteration 63, loss = 0.31706718\n",
      "Iteration 64, loss = 0.31713484\n",
      "Iteration 65, loss = 0.31633899\n",
      "Iteration 66, loss = 0.31593511\n",
      "Iteration 67, loss = 0.31524288\n",
      "Iteration 68, loss = 0.31513217\n",
      "Iteration 69, loss = 0.31473686\n",
      "Iteration 70, loss = 0.31454109\n",
      "Iteration 71, loss = 0.31411585\n",
      "Iteration 72, loss = 0.31355311\n",
      "Iteration 73, loss = 0.31351083\n",
      "Iteration 74, loss = 0.31287185\n",
      "Iteration 75, loss = 0.31227249\n",
      "Iteration 76, loss = 0.31261800\n",
      "Iteration 77, loss = 0.31158634\n",
      "Iteration 78, loss = 0.31091590\n",
      "Iteration 79, loss = 0.31090940\n",
      "Iteration 80, loss = 0.31044238\n",
      "Iteration 81, loss = 0.31029802\n",
      "Iteration 82, loss = 0.30994833\n",
      "Iteration 83, loss = 0.30972725\n",
      "Iteration 84, loss = 0.30919997\n",
      "Iteration 85, loss = 0.30902908\n",
      "Iteration 86, loss = 0.30853826\n",
      "Iteration 87, loss = 0.30828842\n",
      "Iteration 88, loss = 0.30801832\n",
      "Iteration 89, loss = 0.30766570\n",
      "Iteration 90, loss = 0.30751399\n",
      "Iteration 91, loss = 0.30654184\n",
      "Iteration 92, loss = 0.30671253\n",
      "Iteration 93, loss = 0.30662746\n",
      "Iteration 94, loss = 0.30614771\n",
      "Iteration 95, loss = 0.30575707\n",
      "Iteration 96, loss = 0.30543942\n",
      "Iteration 97, loss = 0.30508338\n",
      "Iteration 98, loss = 0.30481927\n",
      "Iteration 99, loss = 0.30424900\n",
      "Iteration 100, loss = 0.30401733\n",
      "Iteration 101, loss = 0.30472911\n",
      "Iteration 102, loss = 0.30363167\n",
      "Iteration 103, loss = 0.30356931\n",
      "Iteration 104, loss = 0.30329199\n",
      "Iteration 105, loss = 0.30241389\n",
      "Iteration 106, loss = 0.30233166\n",
      "Iteration 107, loss = 0.30248346\n",
      "Iteration 108, loss = 0.30255592\n",
      "Iteration 109, loss = 0.30214872\n",
      "Iteration 110, loss = 0.30142495\n",
      "Iteration 111, loss = 0.30130174\n",
      "Iteration 112, loss = 0.30117023\n",
      "Iteration 113, loss = 0.30077963\n",
      "Iteration 114, loss = 0.29992107\n",
      "Iteration 115, loss = 0.30023004\n",
      "Iteration 116, loss = 0.30004884\n",
      "Iteration 117, loss = 0.30080661\n",
      "Iteration 118, loss = 0.29930196\n",
      "Iteration 119, loss = 0.29982883\n",
      "Iteration 120, loss = 0.29928376\n",
      "Iteration 121, loss = 0.29942070\n",
      "Iteration 122, loss = 0.29906161\n",
      "Iteration 123, loss = 0.29858019\n",
      "Iteration 124, loss = 0.29850567\n",
      "Iteration 125, loss = 0.29797536\n",
      "Iteration 126, loss = 0.29820806\n",
      "Iteration 127, loss = 0.29811008\n",
      "Iteration 128, loss = 0.29753629\n",
      "Iteration 129, loss = 0.29743512\n",
      "Iteration 130, loss = 0.29753056\n",
      "Iteration 131, loss = 0.29739282\n",
      "Iteration 132, loss = 0.29648473\n",
      "Iteration 133, loss = 0.29712838\n",
      "Iteration 134, loss = 0.29663697\n",
      "Iteration 135, loss = 0.29659937\n",
      "Iteration 136, loss = 0.29592073\n",
      "Iteration 137, loss = 0.29609905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.41821725\n",
      "Iteration 2, loss = 0.35938982\n",
      "Iteration 3, loss = 0.34782301\n",
      "Iteration 4, loss = 0.34124944\n",
      "Iteration 5, loss = 0.33300249\n",
      "Iteration 6, loss = 0.32740301\n",
      "Iteration 7, loss = 0.32387178\n",
      "Iteration 8, loss = 0.32019316\n",
      "Iteration 9, loss = 0.31544302\n",
      "Iteration 10, loss = 0.30999776\n",
      "Iteration 11, loss = 0.30571020\n",
      "Iteration 12, loss = 0.30115249\n",
      "Iteration 13, loss = 0.29761386\n",
      "Iteration 14, loss = 0.29424112\n",
      "Iteration 15, loss = 0.29247038\n",
      "Iteration 16, loss = 0.28894527\n",
      "Iteration 17, loss = 0.28780162\n",
      "Iteration 18, loss = 0.28539979\n",
      "Iteration 19, loss = 0.28375012\n",
      "Iteration 20, loss = 0.28333413\n",
      "Iteration 21, loss = 0.28047911\n",
      "Iteration 22, loss = 0.27861651\n",
      "Iteration 23, loss = 0.27844091\n",
      "Iteration 24, loss = 0.27635505\n",
      "Iteration 25, loss = 0.27321848\n",
      "Iteration 26, loss = 0.27130914\n",
      "Iteration 27, loss = 0.27099200\n",
      "Iteration 28, loss = 0.27042182\n",
      "Iteration 29, loss = 0.26961930\n",
      "Iteration 30, loss = 0.26634607\n",
      "Iteration 31, loss = 0.26576133\n",
      "Iteration 32, loss = 0.26458259\n",
      "Iteration 33, loss = 0.26445048\n",
      "Iteration 34, loss = 0.26264336\n",
      "Iteration 35, loss = 0.26149564\n",
      "Iteration 36, loss = 0.25946572\n",
      "Iteration 37, loss = 0.25830947\n",
      "Iteration 38, loss = 0.25619833\n",
      "Iteration 39, loss = 0.25355851\n",
      "Iteration 40, loss = 0.25313687\n",
      "Iteration 41, loss = 0.25323508\n",
      "Iteration 42, loss = 0.25137751\n",
      "Iteration 43, loss = 0.25077272\n",
      "Iteration 44, loss = 0.25057448\n",
      "Iteration 45, loss = 0.24958136\n",
      "Iteration 46, loss = 0.24946139\n",
      "Iteration 47, loss = 0.24973001\n",
      "Iteration 48, loss = 0.24854149\n",
      "Iteration 49, loss = 0.24818432\n",
      "Iteration 50, loss = 0.24629554\n",
      "Iteration 51, loss = 0.24575279\n",
      "Iteration 52, loss = 0.24465859\n",
      "Iteration 53, loss = 0.24503354\n",
      "Iteration 54, loss = 0.24447488\n",
      "Iteration 55, loss = 0.24495465\n",
      "Iteration 56, loss = 0.24252139\n",
      "Iteration 57, loss = 0.24579155\n",
      "Iteration 58, loss = 0.24233824\n",
      "Iteration 59, loss = 0.24227099\n",
      "Iteration 60, loss = 0.24359753\n",
      "Iteration 61, loss = 0.24246413\n",
      "Iteration 62, loss = 0.24273913\n",
      "Iteration 63, loss = 0.24234560\n",
      "Iteration 64, loss = 0.24149541\n",
      "Iteration 65, loss = 0.24062466\n",
      "Iteration 66, loss = 0.24083245\n",
      "Iteration 67, loss = 0.24165994\n",
      "Iteration 68, loss = 0.24250841\n",
      "Iteration 69, loss = 0.24031149\n",
      "Iteration 70, loss = 0.23975518\n",
      "Iteration 71, loss = 0.24013178\n",
      "Iteration 72, loss = 0.23993028\n",
      "Iteration 73, loss = 0.23939248\n",
      "Iteration 74, loss = 0.23958021\n",
      "Iteration 75, loss = 0.23904162\n",
      "Iteration 76, loss = 0.23920977\n",
      "Iteration 77, loss = 0.23972232\n",
      "Iteration 78, loss = 0.23775451\n",
      "Iteration 79, loss = 0.23899254\n",
      "Iteration 80, loss = 0.23791634\n",
      "Iteration 81, loss = 0.23769185\n",
      "Iteration 82, loss = 0.23681649\n",
      "Iteration 83, loss = 0.23701664\n",
      "Iteration 84, loss = 0.23727943\n",
      "Iteration 85, loss = 0.23751297\n",
      "Iteration 86, loss = 0.23722279\n",
      "Iteration 87, loss = 0.23629728\n",
      "Iteration 88, loss = 0.23574445\n",
      "Iteration 89, loss = 0.23569522\n",
      "Iteration 90, loss = 0.23623334\n",
      "Iteration 91, loss = 0.23539799\n",
      "Iteration 92, loss = 0.23513956\n",
      "Iteration 93, loss = 0.23519838\n",
      "Iteration 94, loss = 0.23570623\n",
      "Iteration 95, loss = 0.23449393\n",
      "Iteration 96, loss = 0.23494687\n",
      "Iteration 97, loss = 0.23508374\n",
      "Iteration 98, loss = 0.23589755\n",
      "Iteration 99, loss = 0.23450413\n",
      "Iteration 100, loss = 0.23498462\n",
      "Iteration 101, loss = 0.23491487\n",
      "Iteration 102, loss = 0.23538862\n",
      "Iteration 103, loss = 0.23498332\n",
      "Iteration 104, loss = 0.23296576\n",
      "Iteration 105, loss = 0.23392707\n",
      "Iteration 106, loss = 0.23469302\n",
      "Iteration 107, loss = 0.23371014\n",
      "Iteration 108, loss = 0.23291874\n",
      "Iteration 109, loss = 0.23293848\n",
      "Iteration 110, loss = 0.23283619\n",
      "Iteration 111, loss = 0.23309949\n",
      "Iteration 112, loss = 0.23307192\n",
      "Iteration 113, loss = 0.23288823\n",
      "Iteration 114, loss = 0.23306624\n",
      "Iteration 115, loss = 0.23337440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62128844\n",
      "Iteration 2, loss = 0.52434898\n",
      "Iteration 3, loss = 0.43825416\n",
      "Iteration 4, loss = 0.39219033\n",
      "Iteration 5, loss = 0.37498223\n",
      "Iteration 6, loss = 0.36706631\n",
      "Iteration 7, loss = 0.36224841\n",
      "Iteration 8, loss = 0.35914364\n",
      "Iteration 9, loss = 0.35705987\n",
      "Iteration 10, loss = 0.35530711\n",
      "Iteration 11, loss = 0.35299400\n",
      "Iteration 12, loss = 0.35189595\n",
      "Iteration 13, loss = 0.35037963\n",
      "Iteration 14, loss = 0.34929012\n",
      "Iteration 15, loss = 0.34802531\n",
      "Iteration 16, loss = 0.34694781\n",
      "Iteration 17, loss = 0.34620021\n",
      "Iteration 18, loss = 0.34491045\n",
      "Iteration 19, loss = 0.34405398\n",
      "Iteration 20, loss = 0.34282178\n",
      "Iteration 21, loss = 0.34211706\n",
      "Iteration 22, loss = 0.34135978\n",
      "Iteration 23, loss = 0.34011880\n",
      "Iteration 24, loss = 0.33980054\n",
      "Iteration 25, loss = 0.33892705\n",
      "Iteration 26, loss = 0.33782164\n",
      "Iteration 27, loss = 0.33693065\n",
      "Iteration 28, loss = 0.33662835\n",
      "Iteration 29, loss = 0.33525961\n",
      "Iteration 30, loss = 0.33516339\n",
      "Iteration 31, loss = 0.33462711\n",
      "Iteration 32, loss = 0.33317549\n",
      "Iteration 33, loss = 0.33264388\n",
      "Iteration 34, loss = 0.33209529\n",
      "Iteration 35, loss = 0.33150924\n",
      "Iteration 36, loss = 0.33092477\n",
      "Iteration 37, loss = 0.33039992\n",
      "Iteration 38, loss = 0.32970508\n",
      "Iteration 39, loss = 0.32914337\n",
      "Iteration 40, loss = 0.32841723\n",
      "Iteration 41, loss = 0.32802233\n",
      "Iteration 42, loss = 0.32747869\n",
      "Iteration 43, loss = 0.32694672\n",
      "Iteration 44, loss = 0.32639412\n",
      "Iteration 45, loss = 0.32530135\n",
      "Iteration 46, loss = 0.32458936\n",
      "Iteration 47, loss = 0.32493317\n",
      "Iteration 48, loss = 0.32372471\n",
      "Iteration 49, loss = 0.32326486\n",
      "Iteration 50, loss = 0.32265653\n",
      "Iteration 51, loss = 0.32260307\n",
      "Iteration 52, loss = 0.32181547\n",
      "Iteration 53, loss = 0.32073649\n",
      "Iteration 54, loss = 0.32059097\n",
      "Iteration 55, loss = 0.32018329\n",
      "Iteration 56, loss = 0.31910237\n",
      "Iteration 57, loss = 0.31940832\n",
      "Iteration 58, loss = 0.31830170\n",
      "Iteration 59, loss = 0.31827945\n",
      "Iteration 60, loss = 0.31775587\n",
      "Iteration 61, loss = 0.31692498\n",
      "Iteration 62, loss = 0.31634439\n",
      "Iteration 63, loss = 0.31615790\n",
      "Iteration 64, loss = 0.31600923\n",
      "Iteration 65, loss = 0.31513916\n",
      "Iteration 66, loss = 0.31488643\n",
      "Iteration 67, loss = 0.31426473\n",
      "Iteration 68, loss = 0.31387203\n",
      "Iteration 69, loss = 0.31341242\n",
      "Iteration 70, loss = 0.31369950\n",
      "Iteration 71, loss = 0.31249566\n",
      "Iteration 72, loss = 0.31241592\n",
      "Iteration 73, loss = 0.31212946\n",
      "Iteration 74, loss = 0.31132746\n",
      "Iteration 75, loss = 0.31076574\n",
      "Iteration 76, loss = 0.31128197\n",
      "Iteration 77, loss = 0.31005676\n",
      "Iteration 78, loss = 0.30993918\n",
      "Iteration 79, loss = 0.30931510\n",
      "Iteration 80, loss = 0.30920226\n",
      "Iteration 81, loss = 0.30887205\n",
      "Iteration 82, loss = 0.30867810\n",
      "Iteration 83, loss = 0.30825134\n",
      "Iteration 84, loss = 0.30808519\n",
      "Iteration 85, loss = 0.30774755\n",
      "Iteration 86, loss = 0.30723123\n",
      "Iteration 87, loss = 0.30677819\n",
      "Iteration 88, loss = 0.30633057\n",
      "Iteration 89, loss = 0.30632733\n",
      "Iteration 90, loss = 0.30623014\n",
      "Iteration 91, loss = 0.30548567\n",
      "Iteration 92, loss = 0.30597088\n",
      "Iteration 93, loss = 0.30530656\n",
      "Iteration 94, loss = 0.30471846\n",
      "Iteration 95, loss = 0.30474450\n",
      "Iteration 96, loss = 0.30379350\n",
      "Iteration 97, loss = 0.30371926\n",
      "Iteration 98, loss = 0.30361104\n",
      "Iteration 99, loss = 0.30333621\n",
      "Iteration 100, loss = 0.30299754\n",
      "Iteration 101, loss = 0.30264722\n",
      "Iteration 102, loss = 0.30250585\n",
      "Iteration 103, loss = 0.30235468\n",
      "Iteration 104, loss = 0.30198311\n",
      "Iteration 105, loss = 0.30117880\n",
      "Iteration 106, loss = 0.30146506\n",
      "Iteration 107, loss = 0.30151184\n",
      "Iteration 108, loss = 0.30176591\n",
      "Iteration 109, loss = 0.30066650\n",
      "Iteration 110, loss = 0.29996721\n",
      "Iteration 111, loss = 0.30005071\n",
      "Iteration 112, loss = 0.30004581\n",
      "Iteration 113, loss = 0.30009177\n",
      "Iteration 114, loss = 0.29942916\n",
      "Iteration 115, loss = 0.29953439\n",
      "Iteration 116, loss = 0.29872157\n",
      "Iteration 117, loss = 0.29944756\n",
      "Iteration 118, loss = 0.29862490\n",
      "Iteration 119, loss = 0.29853296\n",
      "Iteration 120, loss = 0.29827158\n",
      "Iteration 121, loss = 0.29831097\n",
      "Iteration 122, loss = 0.29787788\n",
      "Iteration 123, loss = 0.29776326\n",
      "Iteration 124, loss = 0.29787136\n",
      "Iteration 125, loss = 0.29737693\n",
      "Iteration 126, loss = 0.29682673\n",
      "Iteration 127, loss = 0.29682897\n",
      "Iteration 128, loss = 0.29676032\n",
      "Iteration 129, loss = 0.29633748\n",
      "Iteration 130, loss = 0.29608134\n",
      "Iteration 131, loss = 0.29665383\n",
      "Iteration 132, loss = 0.29562249\n",
      "Iteration 133, loss = 0.29582109\n",
      "Iteration 134, loss = 0.29536083\n",
      "Iteration 135, loss = 0.29484436\n",
      "Iteration 136, loss = 0.29536558\n",
      "Iteration 137, loss = 0.29568345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 0.22348120\n",
      "Iteration 253, loss = 0.22150547\n",
      "Iteration 254, loss = 0.22129646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.41841079\n",
      "Iteration 2, loss = 0.35889440\n",
      "Iteration 3, loss = 0.34666020\n",
      "Iteration 4, loss = 0.33899210\n",
      "Iteration 5, loss = 0.33224438\n",
      "Iteration 6, loss = 0.32402830\n",
      "Iteration 7, loss = 0.31749444\n",
      "Iteration 8, loss = 0.31101482\n",
      "Iteration 9, loss = 0.30257560\n",
      "Iteration 10, loss = 0.29871035\n",
      "Iteration 11, loss = 0.29399708\n",
      "Iteration 12, loss = 0.29051971\n",
      "Iteration 13, loss = 0.28725489\n",
      "Iteration 14, loss = 0.28539865\n",
      "Iteration 15, loss = 0.28278547\n",
      "Iteration 16, loss = 0.27967698\n",
      "Iteration 17, loss = 0.27752868\n",
      "Iteration 18, loss = 0.27689634\n",
      "Iteration 19, loss = 0.27526338\n",
      "Iteration 20, loss = 0.27305753\n",
      "Iteration 21, loss = 0.27175613\n",
      "Iteration 22, loss = 0.27077759\n",
      "Iteration 23, loss = 0.26806078\n",
      "Iteration 24, loss = 0.26721101\n",
      "Iteration 25, loss = 0.26640061\n",
      "Iteration 26, loss = 0.26547706\n",
      "Iteration 27, loss = 0.26484169\n",
      "Iteration 28, loss = 0.26340802\n",
      "Iteration 29, loss = 0.26232823\n",
      "Iteration 30, loss = 0.26071402\n",
      "Iteration 31, loss = 0.26002791\n",
      "Iteration 32, loss = 0.25780775\n",
      "Iteration 33, loss = 0.25762675\n",
      "Iteration 34, loss = 0.25572995\n",
      "Iteration 35, loss = 0.25570007\n",
      "Iteration 36, loss = 0.25476362\n",
      "Iteration 37, loss = 0.25359396\n",
      "Iteration 38, loss = 0.25368203\n",
      "Iteration 39, loss = 0.25311737\n",
      "Iteration 40, loss = 0.24995217\n",
      "Iteration 41, loss = 0.25185569\n",
      "Iteration 42, loss = 0.25031564\n",
      "Iteration 43, loss = 0.24839311\n",
      "Iteration 44, loss = 0.24854670\n",
      "Iteration 45, loss = 0.24821972\n",
      "Iteration 46, loss = 0.24841859\n",
      "Iteration 47, loss = 0.24622774\n",
      "Iteration 48, loss = 0.24675881\n",
      "Iteration 49, loss = 0.24695909\n",
      "Iteration 50, loss = 0.24577763\n",
      "Iteration 51, loss = 0.24511276\n",
      "Iteration 52, loss = 0.24506335\n",
      "Iteration 53, loss = 0.24304229\n",
      "Iteration 54, loss = 0.24422093\n",
      "Iteration 55, loss = 0.24545078\n",
      "Iteration 56, loss = 0.24326295\n",
      "Iteration 57, loss = 0.24469784\n",
      "Iteration 58, loss = 0.24379552\n",
      "Iteration 59, loss = 0.24264749\n",
      "Iteration 60, loss = 0.24264511\n",
      "Iteration 61, loss = 0.24363557\n",
      "Iteration 62, loss = 0.24129817\n",
      "Iteration 63, loss = 0.24120909\n",
      "Iteration 64, loss = 0.24181847\n",
      "Iteration 65, loss = 0.24227541\n",
      "Iteration 66, loss = 0.24023745\n",
      "Iteration 67, loss = 0.24136904\n",
      "Iteration 68, loss = 0.24065403\n",
      "Iteration 69, loss = 0.24121871\n",
      "Iteration 70, loss = 0.23960220\n",
      "Iteration 71, loss = 0.24044407\n",
      "Iteration 72, loss = 0.24115888\n",
      "Iteration 73, loss = 0.23929509\n",
      "Iteration 74, loss = 0.23994382\n",
      "Iteration 75, loss = 0.24055217\n",
      "Iteration 76, loss = 0.24067079\n",
      "Iteration 77, loss = 0.23702014\n",
      "Iteration 78, loss = 0.23771930\n",
      "Iteration 79, loss = 0.23835311\n",
      "Iteration 80, loss = 0.23734294\n",
      "Iteration 81, loss = 0.23739646\n",
      "Iteration 82, loss = 0.23899293\n",
      "Iteration 83, loss = 0.23871734\n",
      "Iteration 84, loss = 0.23728242\n",
      "Iteration 85, loss = 0.23852112\n",
      "Iteration 86, loss = 0.23707386\n",
      "Iteration 87, loss = 0.23727756\n",
      "Iteration 88, loss = 0.23737613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65656495\n",
      "Iteration 2, loss = 0.60842144\n",
      "Iteration 3, loss = 0.58519548\n",
      "Iteration 4, loss = 0.56456126\n",
      "Iteration 5, loss = 0.54511057\n",
      "Iteration 6, loss = 0.52697450\n",
      "Iteration 7, loss = 0.51036428\n",
      "Iteration 8, loss = 0.49563100\n",
      "Iteration 9, loss = 0.48262876\n",
      "Iteration 10, loss = 0.47114818\n",
      "Iteration 11, loss = 0.46107498\n",
      "Iteration 12, loss = 0.45221502\n",
      "Iteration 13, loss = 0.44427313\n",
      "Iteration 14, loss = 0.43734648\n",
      "Iteration 15, loss = 0.43108577\n",
      "Iteration 16, loss = 0.42549043\n",
      "Iteration 17, loss = 0.42050739\n",
      "Iteration 18, loss = 0.41607235\n",
      "Iteration 19, loss = 0.41207895\n",
      "Iteration 20, loss = 0.40838445\n",
      "Iteration 21, loss = 0.40509279\n",
      "Iteration 22, loss = 0.40210874\n",
      "Iteration 23, loss = 0.39937950\n",
      "Iteration 24, loss = 0.39687354\n",
      "Iteration 25, loss = 0.39454310\n",
      "Iteration 26, loss = 0.39225546\n",
      "Iteration 27, loss = 0.39030458\n",
      "Iteration 28, loss = 0.38847236\n",
      "Iteration 29, loss = 0.38676187\n",
      "Iteration 30, loss = 0.38500983\n",
      "Iteration 31, loss = 0.38347229\n",
      "Iteration 32, loss = 0.38195948\n",
      "Iteration 33, loss = 0.38061721\n",
      "Iteration 34, loss = 0.37928550\n",
      "Iteration 35, loss = 0.37801404\n",
      "Iteration 36, loss = 0.37698455\n",
      "Iteration 37, loss = 0.37576888\n",
      "Iteration 38, loss = 0.37486549\n",
      "Iteration 39, loss = 0.37356515\n",
      "Iteration 40, loss = 0.37273462\n",
      "Iteration 41, loss = 0.37179421\n",
      "Iteration 42, loss = 0.37082010\n",
      "Iteration 43, loss = 0.36999171\n",
      "Iteration 44, loss = 0.36914838\n",
      "Iteration 45, loss = 0.36837548\n",
      "Iteration 46, loss = 0.36752211\n",
      "Iteration 47, loss = 0.36681171\n",
      "Iteration 48, loss = 0.36611764\n",
      "Iteration 49, loss = 0.36542526\n",
      "Iteration 50, loss = 0.36480865\n",
      "Iteration 51, loss = 0.36398927\n",
      "Iteration 52, loss = 0.36348061\n",
      "Iteration 53, loss = 0.36286854\n",
      "Iteration 54, loss = 0.36224537\n",
      "Iteration 55, loss = 0.36173100\n",
      "Iteration 56, loss = 0.36107468\n",
      "Iteration 57, loss = 0.36065310\n",
      "Iteration 58, loss = 0.36007287\n",
      "Iteration 59, loss = 0.35949117\n",
      "Iteration 60, loss = 0.35914970\n",
      "Iteration 61, loss = 0.35862854\n",
      "Iteration 62, loss = 0.35824118\n",
      "Iteration 63, loss = 0.35774946\n",
      "Iteration 64, loss = 0.35718921\n",
      "Iteration 65, loss = 0.35687964\n",
      "Iteration 66, loss = 0.35646499\n",
      "Iteration 67, loss = 0.35605096\n",
      "Iteration 68, loss = 0.35579368\n",
      "Iteration 69, loss = 0.35544948\n",
      "Iteration 70, loss = 0.35499204\n",
      "Iteration 71, loss = 0.35451650\n",
      "Iteration 72, loss = 0.35421020\n",
      "Iteration 73, loss = 0.35392055\n",
      "Iteration 74, loss = 0.35366679\n",
      "Iteration 75, loss = 0.35331049\n",
      "Iteration 76, loss = 0.35291676\n",
      "Iteration 77, loss = 0.35266880\n",
      "Iteration 78, loss = 0.35230314\n",
      "Iteration 79, loss = 0.35206506\n",
      "Iteration 80, loss = 0.35176095\n",
      "Iteration 81, loss = 0.35154382\n",
      "Iteration 82, loss = 0.35134587\n",
      "Iteration 83, loss = 0.35096707\n",
      "Iteration 84, loss = 0.35062641\n",
      "Iteration 85, loss = 0.35040770\n",
      "Iteration 86, loss = 0.35015112\n",
      "Iteration 87, loss = 0.34986764\n",
      "Iteration 88, loss = 0.34974096\n",
      "Iteration 89, loss = 0.34936567\n",
      "Iteration 90, loss = 0.34935817\n",
      "Iteration 91, loss = 0.34891883\n",
      "Iteration 92, loss = 0.34883452\n",
      "Iteration 93, loss = 0.34852635\n",
      "Iteration 94, loss = 0.34849799\n",
      "Iteration 95, loss = 0.34814407\n",
      "Iteration 96, loss = 0.34780084\n",
      "Iteration 97, loss = 0.34757585\n",
      "Iteration 98, loss = 0.34751090\n",
      "Iteration 99, loss = 0.34726537\n",
      "Iteration 100, loss = 0.34715553\n",
      "Iteration 101, loss = 0.34700683\n",
      "Iteration 102, loss = 0.34687738\n",
      "Iteration 103, loss = 0.34644771\n",
      "Iteration 104, loss = 0.34652748\n",
      "Iteration 105, loss = 0.34618680\n",
      "Iteration 106, loss = 0.34603850\n",
      "Iteration 107, loss = 0.34583801\n",
      "Iteration 108, loss = 0.34572471\n",
      "Iteration 109, loss = 0.34548470\n",
      "Iteration 110, loss = 0.34534053\n",
      "Iteration 111, loss = 0.34525901\n",
      "Iteration 112, loss = 0.34508351\n",
      "Iteration 113, loss = 0.34498193\n",
      "Iteration 114, loss = 0.34473048\n",
      "Iteration 115, loss = 0.34454339\n",
      "Iteration 116, loss = 0.34448277\n",
      "Iteration 117, loss = 0.34429999\n",
      "Iteration 118, loss = 0.34400643\n",
      "Iteration 119, loss = 0.34395358\n",
      "Iteration 120, loss = 0.34382983\n",
      "Iteration 121, loss = 0.34374625\n",
      "Iteration 122, loss = 0.34349584\n",
      "Iteration 123, loss = 0.34328593\n",
      "Iteration 124, loss = 0.34331017\n",
      "Iteration 125, loss = 0.34317038\n",
      "Iteration 126, loss = 0.34297866\n",
      "Iteration 127, loss = 0.34289053\n",
      "Iteration 128, loss = 0.34276024\n",
      "Iteration 129, loss = 0.34271924\n",
      "Iteration 130, loss = 0.34267638\n",
      "Iteration 131, loss = 0.34244557\n",
      "Iteration 132, loss = 0.34235394\n",
      "Iteration 133, loss = 0.34210662\n",
      "Iteration 134, loss = 0.34196996\n",
      "Iteration 135, loss = 0.34191595\n",
      "Iteration 136, loss = 0.34175035\n",
      "Iteration 137, loss = 0.34173615\n",
      "Iteration 138, loss = 0.34163829\n",
      "Iteration 139, loss = 0.34158208\n",
      "Iteration 140, loss = 0.34127171\n",
      "Iteration 141, loss = 0.34127973\n",
      "Iteration 142, loss = 0.34102581\n",
      "Iteration 143, loss = 0.34098914\n",
      "Iteration 144, loss = 0.34089658\n",
      "Iteration 145, loss = 0.34071586\n",
      "Iteration 146, loss = 0.34062943\n",
      "Iteration 147, loss = 0.34060132\n",
      "Iteration 148, loss = 0.34038191\n",
      "Iteration 149, loss = 0.34023285\n",
      "Iteration 150, loss = 0.34014631\n",
      "Iteration 151, loss = 0.34019775\n",
      "Iteration 152, loss = 0.34002935\n",
      "Iteration 153, loss = 0.33995555\n",
      "Iteration 154, loss = 0.33982777\n",
      "Iteration 155, loss = 0.33967037\n",
      "Iteration 156, loss = 0.33961596\n",
      "Iteration 157, loss = 0.33959382\n",
      "Iteration 158, loss = 0.33939995\n",
      "Iteration 252, loss = 0.28047453\n",
      "Iteration 253, loss = 0.27996641\n",
      "Iteration 254, loss = 0.28021527\n",
      "Iteration 255, loss = 0.28016095\n",
      "Iteration 256, loss = 0.28016401\n",
      "Iteration 257, loss = 0.27937514\n",
      "Iteration 258, loss = 0.28036741\n",
      "Iteration 259, loss = 0.27917173\n",
      "Iteration 260, loss = 0.27974662\n",
      "Iteration 261, loss = 0.27928230\n",
      "Iteration 262, loss = 0.27982464\n",
      "Iteration 263, loss = 0.27906407\n",
      "Iteration 264, loss = 0.27885847\n",
      "Iteration 265, loss = 0.27873654\n",
      "Iteration 266, loss = 0.27845393\n",
      "Iteration 267, loss = 0.27894251\n",
      "Iteration 268, loss = 0.27858428\n",
      "Iteration 269, loss = 0.27851471\n",
      "Iteration 270, loss = 0.27818740\n",
      "Iteration 271, loss = 0.27829624\n",
      "Iteration 272, loss = 0.27854907\n",
      "Iteration 273, loss = 0.27855795\n",
      "Iteration 274, loss = 0.27756230\n",
      "Iteration 275, loss = 0.27669075\n",
      "Iteration 276, loss = 0.27738608\n",
      "Iteration 277, loss = 0.27777019\n",
      "Iteration 278, loss = 0.27648228\n",
      "Iteration 279, loss = 0.27776725\n",
      "Iteration 280, loss = 0.27710117\n",
      "Iteration 281, loss = 0.27646358\n",
      "Iteration 282, loss = 0.27611166\n",
      "Iteration 283, loss = 0.27617246\n",
      "Iteration 284, loss = 0.27647883\n",
      "Iteration 285, loss = 0.27566358\n",
      "Iteration 286, loss = 0.27590763\n",
      "Iteration 287, loss = 0.27600666\n",
      "Iteration 288, loss = 0.27563568\n",
      "Iteration 289, loss = 0.27554700\n",
      "Iteration 290, loss = 0.27497344\n",
      "Iteration 291, loss = 0.27521867\n",
      "Iteration 292, loss = 0.27470176\n",
      "Iteration 293, loss = 0.27473904\n",
      "Iteration 294, loss = 0.27468755\n",
      "Iteration 295, loss = 0.27437720\n",
      "Iteration 296, loss = 0.27443004\n",
      "Iteration 297, loss = 0.27321149\n",
      "Iteration 298, loss = 0.27384457\n",
      "Iteration 299, loss = 0.27392038\n",
      "Iteration 300, loss = 0.27392114\n",
      "Iteration 1, loss = 0.41841079\n",
      "Iteration 2, loss = 0.35889440\n",
      "Iteration 3, loss = 0.34666020\n",
      "Iteration 4, loss = 0.33899210\n",
      "Iteration 5, loss = 0.33224438\n",
      "Iteration 6, loss = 0.32402830\n",
      "Iteration 7, loss = 0.31749444\n",
      "Iteration 8, loss = 0.31101482\n",
      "Iteration 9, loss = 0.30257560\n",
      "Iteration 10, loss = 0.29871035\n",
      "Iteration 11, loss = 0.29399708\n",
      "Iteration 12, loss = 0.29051971\n",
      "Iteration 13, loss = 0.28725489\n",
      "Iteration 14, loss = 0.28539865\n",
      "Iteration 15, loss = 0.28278547\n",
      "Iteration 16, loss = 0.27967698\n",
      "Iteration 17, loss = 0.27752868\n",
      "Iteration 18, loss = 0.27689634\n",
      "Iteration 19, loss = 0.27526338\n",
      "Iteration 20, loss = 0.27305753\n",
      "Iteration 21, loss = 0.27175613\n",
      "Iteration 22, loss = 0.27077759\n",
      "Iteration 23, loss = 0.26806078\n",
      "Iteration 24, loss = 0.26721101\n",
      "Iteration 25, loss = 0.26640061\n",
      "Iteration 26, loss = 0.26547706\n",
      "Iteration 27, loss = 0.26484169\n",
      "Iteration 28, loss = 0.26340802\n",
      "Iteration 29, loss = 0.26232823\n",
      "Iteration 30, loss = 0.26071402\n",
      "Iteration 31, loss = 0.26002791\n",
      "Iteration 32, loss = 0.25780775\n",
      "Iteration 33, loss = 0.25762675\n",
      "Iteration 34, loss = 0.25572995\n",
      "Iteration 35, loss = 0.25570007\n",
      "Iteration 36, loss = 0.25476362\n",
      "Iteration 37, loss = 0.25359396\n",
      "Iteration 38, loss = 0.25368203\n",
      "Iteration 39, loss = 0.25311737\n",
      "Iteration 40, loss = 0.24995217\n",
      "Iteration 41, loss = 0.25185569\n",
      "Iteration 42, loss = 0.25031564\n",
      "Iteration 43, loss = 0.24839311\n",
      "Iteration 44, loss = 0.24854670\n",
      "Iteration 45, loss = 0.24821972\n",
      "Iteration 46, loss = 0.24841859\n",
      "Iteration 47, loss = 0.24622774\n",
      "Iteration 48, loss = 0.24675881\n",
      "Iteration 49, loss = 0.24695909\n",
      "Iteration 50, loss = 0.24577763\n",
      "Iteration 51, loss = 0.24511276\n",
      "Iteration 52, loss = 0.24506335\n",
      "Iteration 53, loss = 0.24304229\n",
      "Iteration 54, loss = 0.24422093\n",
      "Iteration 55, loss = 0.24545078\n",
      "Iteration 56, loss = 0.24326295\n",
      "Iteration 57, loss = 0.24469784\n",
      "Iteration 58, loss = 0.24379552\n",
      "Iteration 59, loss = 0.24264749\n",
      "Iteration 60, loss = 0.24264511\n",
      "Iteration 61, loss = 0.24363557\n",
      "Iteration 62, loss = 0.24129817\n",
      "Iteration 63, loss = 0.24120909\n",
      "Iteration 64, loss = 0.24181847\n",
      "Iteration 65, loss = 0.24227541\n",
      "Iteration 66, loss = 0.24023745\n",
      "Iteration 67, loss = 0.24136904\n",
      "Iteration 68, loss = 0.24065403\n",
      "Iteration 69, loss = 0.24121871\n",
      "Iteration 70, loss = 0.23960220\n",
      "Iteration 71, loss = 0.24044407\n",
      "Iteration 72, loss = 0.24115888\n",
      "Iteration 73, loss = 0.23929509\n",
      "Iteration 74, loss = 0.23994382\n",
      "Iteration 75, loss = 0.24055217\n",
      "Iteration 76, loss = 0.24067079\n",
      "Iteration 77, loss = 0.23702014\n",
      "Iteration 78, loss = 0.23771930\n",
      "Iteration 79, loss = 0.23835311\n",
      "Iteration 80, loss = 0.23734294\n",
      "Iteration 81, loss = 0.23739646\n",
      "Iteration 82, loss = 0.23899293\n",
      "Iteration 83, loss = 0.23871734\n",
      "Iteration 84, loss = 0.23728242\n",
      "Iteration 85, loss = 0.23852112\n",
      "Iteration 86, loss = 0.23707386\n",
      "Iteration 87, loss = 0.23727756\n",
      "Iteration 88, loss = 0.23737613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65660380\n",
      "Iteration 2, loss = 0.60839806\n",
      "Iteration 3, loss = 0.58516878\n",
      "Iteration 4, loss = 0.56449964\n",
      "Iteration 5, loss = 0.54511558\n",
      "Iteration 6, loss = 0.52686999\n",
      "Iteration 7, loss = 0.51038824\n",
      "Iteration 8, loss = 0.49561857\n",
      "Iteration 9, loss = 0.48264207\n",
      "Iteration 10, loss = 0.47119211\n",
      "Iteration 11, loss = 0.46108903\n",
      "Iteration 12, loss = 0.45207266\n",
      "Iteration 13, loss = 0.44425936\n",
      "Iteration 14, loss = 0.43709802\n",
      "Iteration 15, loss = 0.43091201\n",
      "Iteration 16, loss = 0.42528115\n",
      "Iteration 17, loss = 0.42038534\n",
      "Iteration 18, loss = 0.41586271\n",
      "Iteration 19, loss = 0.41182686\n",
      "Iteration 20, loss = 0.40819603\n",
      "Iteration 21, loss = 0.40498715\n",
      "Iteration 22, loss = 0.40188973\n",
      "Iteration 23, loss = 0.39917108\n",
      "Iteration 24, loss = 0.39669406\n",
      "Iteration 25, loss = 0.39435932\n",
      "Iteration 26, loss = 0.39217190\n",
      "Iteration 27, loss = 0.39012897\n",
      "Iteration 28, loss = 0.38831211\n",
      "Iteration 29, loss = 0.38656448\n",
      "Iteration 30, loss = 0.38493512\n",
      "Iteration 31, loss = 0.38334378\n",
      "Iteration 32, loss = 0.38196578\n",
      "Iteration 33, loss = 0.38056441\n",
      "Iteration 34, loss = 0.37925441\n",
      "Iteration 35, loss = 0.37800035\n",
      "Iteration 36, loss = 0.37686671\n",
      "Iteration 37, loss = 0.37571513\n",
      "Iteration 38, loss = 0.37469994\n",
      "Iteration 39, loss = 0.37359073\n",
      "Iteration 40, loss = 0.37264753\n",
      "Iteration 41, loss = 0.37172520\n",
      "Iteration 42, loss = 0.37085415\n",
      "Iteration 43, loss = 0.36989337\n",
      "Iteration 44, loss = 0.36905861\n",
      "Iteration 45, loss = 0.36837822\n",
      "Iteration 46, loss = 0.36751630\n",
      "Iteration 47, loss = 0.36684143\n",
      "Iteration 48, loss = 0.36601267\n",
      "Iteration 49, loss = 0.36520412\n",
      "Iteration 50, loss = 0.36460325\n",
      "Iteration 51, loss = 0.36411318\n",
      "Iteration 52, loss = 0.36335983\n",
      "Iteration 53, loss = 0.36263213\n",
      "Iteration 54, loss = 0.36221404\n",
      "Iteration 55, loss = 0.36154996\n",
      "Iteration 56, loss = 0.36094001\n",
      "Iteration 57, loss = 0.36044017\n",
      "Iteration 58, loss = 0.36001628\n",
      "Iteration 59, loss = 0.35950879\n",
      "Iteration 60, loss = 0.35900848\n",
      "Iteration 61, loss = 0.35852419\n",
      "Iteration 62, loss = 0.35804133\n",
      "Iteration 63, loss = 0.35761515\n",
      "Iteration 64, loss = 0.35717024\n",
      "Iteration 65, loss = 0.35675586\n",
      "Iteration 66, loss = 0.35631100\n",
      "Iteration 67, loss = 0.35593043\n",
      "Iteration 68, loss = 0.35560263\n",
      "Iteration 69, loss = 0.35521940\n",
      "Iteration 70, loss = 0.35471070\n",
      "Iteration 71, loss = 0.35435786\n",
      "Iteration 72, loss = 0.35408979\n",
      "Iteration 73, loss = 0.35380461\n",
      "Iteration 74, loss = 0.35336591\n",
      "Iteration 75, loss = 0.35306507\n",
      "Iteration 76, loss = 0.35282310\n",
      "Iteration 77, loss = 0.35242750\n",
      "Iteration 78, loss = 0.35214164\n",
      "Iteration 79, loss = 0.35182532\n",
      "Iteration 80, loss = 0.35167278\n",
      "Iteration 81, loss = 0.35122414\n",
      "Iteration 82, loss = 0.35103899\n",
      "Iteration 83, loss = 0.35075229\n",
      "Iteration 84, loss = 0.35050070\n",
      "Iteration 85, loss = 0.35012767\n",
      "Iteration 86, loss = 0.35006951\n",
      "Iteration 87, loss = 0.34977488\n",
      "Iteration 88, loss = 0.34955117\n",
      "Iteration 89, loss = 0.34925456\n",
      "Iteration 90, loss = 0.34889932\n",
      "Iteration 91, loss = 0.34884149\n",
      "Iteration 92, loss = 0.34855372\n",
      "Iteration 93, loss = 0.34830870\n",
      "Iteration 94, loss = 0.34810359\n",
      "Iteration 95, loss = 0.34792723\n",
      "Iteration 96, loss = 0.34772414\n",
      "Iteration 97, loss = 0.34750312\n",
      "Iteration 98, loss = 0.34723773\n",
      "Iteration 99, loss = 0.34723952\n",
      "Iteration 100, loss = 0.34706688\n",
      "Iteration 101, loss = 0.34687985\n",
      "Iteration 102, loss = 0.34656405\n",
      "Iteration 103, loss = 0.34636148\n",
      "Iteration 104, loss = 0.34617299\n",
      "Iteration 105, loss = 0.34611512\n",
      "Iteration 106, loss = 0.34583195\n",
      "Iteration 107, loss = 0.34565643\n",
      "Iteration 108, loss = 0.34546606\n",
      "Iteration 109, loss = 0.34528613\n",
      "Iteration 110, loss = 0.34532155\n",
      "Iteration 111, loss = 0.34522948\n",
      "Iteration 112, loss = 0.34483830\n",
      "Iteration 113, loss = 0.34469803\n",
      "Iteration 114, loss = 0.34464542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72, loss = 0.24001597\n",
      "Iteration 73, loss = 0.23943949\n",
      "Iteration 74, loss = 0.23933001\n",
      "Iteration 75, loss = 0.24036836\n",
      "Iteration 76, loss = 0.23939833\n",
      "Iteration 77, loss = 0.23930227\n",
      "Iteration 78, loss = 0.23912687\n",
      "Iteration 79, loss = 0.23960154\n",
      "Iteration 80, loss = 0.23856309\n",
      "Iteration 81, loss = 0.23913822\n",
      "Iteration 82, loss = 0.23823749\n",
      "Iteration 83, loss = 0.23667306\n",
      "Iteration 84, loss = 0.23872659\n",
      "Iteration 85, loss = 0.23894523\n",
      "Iteration 86, loss = 0.23764981\n",
      "Iteration 87, loss = 0.23842497\n",
      "Iteration 88, loss = 0.23830929\n",
      "Iteration 89, loss = 0.23803377\n",
      "Iteration 90, loss = 0.23678242\n",
      "Iteration 91, loss = 0.23558817\n",
      "Iteration 92, loss = 0.23622707\n",
      "Iteration 93, loss = 0.23649178\n",
      "Iteration 94, loss = 0.23574752\n",
      "Iteration 95, loss = 0.23576938\n",
      "Iteration 96, loss = 0.23598699\n",
      "Iteration 97, loss = 0.23655469\n",
      "Iteration 98, loss = 0.23648204\n",
      "Iteration 99, loss = 0.23615831\n",
      "Iteration 100, loss = 0.23570226\n",
      "Iteration 101, loss = 0.23435094\n",
      "Iteration 102, loss = 0.23518063\n",
      "Iteration 103, loss = 0.23454646\n",
      "Iteration 104, loss = 0.23500449\n",
      "Iteration 105, loss = 0.23371575\n",
      "Iteration 106, loss = 0.23304217\n",
      "Iteration 107, loss = 0.23505842\n",
      "Iteration 108, loss = 0.23375282\n",
      "Iteration 109, loss = 0.23448336\n",
      "Iteration 110, loss = 0.23559039\n",
      "Iteration 111, loss = 0.23298001\n",
      "Iteration 112, loss = 0.23479018\n",
      "Iteration 113, loss = 0.23359237\n",
      "Iteration 114, loss = 0.23224876\n",
      "Iteration 115, loss = 0.23376145\n",
      "Iteration 116, loss = 0.23226204\n",
      "Iteration 117, loss = 0.23171819\n",
      "Iteration 118, loss = 0.23361264\n",
      "Iteration 119, loss = 0.23314514\n",
      "Iteration 120, loss = 0.23267015\n",
      "Iteration 121, loss = 0.23324516\n",
      "Iteration 122, loss = 0.23277671\n",
      "Iteration 123, loss = 0.23297207\n",
      "Iteration 124, loss = 0.23184378\n",
      "Iteration 125, loss = 0.23250210\n",
      "Iteration 126, loss = 0.23143361\n",
      "Iteration 127, loss = 0.23156162\n",
      "Iteration 128, loss = 0.23291994\n",
      "Iteration 129, loss = 0.23287729\n",
      "Iteration 130, loss = 0.23111167\n",
      "Iteration 131, loss = 0.23297830\n",
      "Iteration 132, loss = 0.23128817\n",
      "Iteration 133, loss = 0.23057377\n",
      "Iteration 134, loss = 0.23171160\n",
      "Iteration 135, loss = 0.23155429\n",
      "Iteration 136, loss = 0.23181196\n",
      "Iteration 137, loss = 0.23116416\n",
      "Iteration 138, loss = 0.23134604\n",
      "Iteration 139, loss = 0.23191472\n",
      "Iteration 140, loss = 0.23057854\n",
      "Iteration 141, loss = 0.23119519\n",
      "Iteration 142, loss = 0.23058327\n",
      "Iteration 143, loss = 0.23013232\n",
      "Iteration 144, loss = 0.23011215\n",
      "Iteration 145, loss = 0.23042789\n",
      "Iteration 146, loss = 0.23033789\n",
      "Iteration 147, loss = 0.23056792\n",
      "Iteration 148, loss = 0.23008820\n",
      "Iteration 149, loss = 0.23112862\n",
      "Iteration 150, loss = 0.23027609\n",
      "Iteration 151, loss = 0.23148140\n",
      "Iteration 152, loss = 0.23212405\n",
      "Iteration 153, loss = 0.23077118\n",
      "Iteration 154, loss = 0.22797365\n",
      "Iteration 155, loss = 0.22941957\n",
      "Iteration 156, loss = 0.22906135\n",
      "Iteration 157, loss = 0.22888811\n",
      "Iteration 158, loss = 0.23098800\n",
      "Iteration 159, loss = 0.22887628\n",
      "Iteration 160, loss = 0.22908465\n",
      "Iteration 161, loss = 0.22865705\n",
      "Iteration 162, loss = 0.22985460\n",
      "Iteration 163, loss = 0.22920794\n",
      "Iteration 164, loss = 0.22922016\n",
      "Iteration 165, loss = 0.22798084\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65624843\n",
      "Iteration 2, loss = 0.60832709\n",
      "Iteration 3, loss = 0.58518214\n",
      "Iteration 4, loss = 0.56463910\n",
      "Iteration 5, loss = 0.54521381\n",
      "Iteration 6, loss = 0.52699398\n",
      "Iteration 7, loss = 0.51032920\n",
      "Iteration 8, loss = 0.49552535\n",
      "Iteration 9, loss = 0.48248014\n",
      "Iteration 10, loss = 0.47101473\n",
      "Iteration 11, loss = 0.46088939\n",
      "Iteration 12, loss = 0.45195384\n",
      "Iteration 13, loss = 0.44403656\n",
      "Iteration 14, loss = 0.43692157\n",
      "Iteration 15, loss = 0.43065532\n",
      "Iteration 16, loss = 0.42498063\n",
      "Iteration 17, loss = 0.41996399\n",
      "Iteration 18, loss = 0.41546917\n",
      "Iteration 19, loss = 0.41140312\n",
      "Iteration 20, loss = 0.40770972\n",
      "Iteration 21, loss = 0.40434715\n",
      "Iteration 22, loss = 0.40134671\n",
      "Iteration 23, loss = 0.39858749\n",
      "Iteration 24, loss = 0.39601073\n",
      "Iteration 25, loss = 0.39361226\n",
      "Iteration 26, loss = 0.39142769\n",
      "Iteration 27, loss = 0.38934187\n",
      "Iteration 28, loss = 0.38742874\n",
      "Iteration 29, loss = 0.38563851\n",
      "Iteration 30, loss = 0.38394826\n",
      "Iteration 31, loss = 0.38242366\n",
      "Iteration 32, loss = 0.38101376\n",
      "Iteration 33, loss = 0.37952994\n",
      "Iteration 34, loss = 0.37816889\n",
      "Iteration 35, loss = 0.37689688\n",
      "Iteration 36, loss = 0.37577007\n",
      "Iteration 37, loss = 0.37461783\n",
      "Iteration 38, loss = 0.37351878\n",
      "Iteration 39, loss = 0.37234627\n",
      "Iteration 40, loss = 0.37149119\n",
      "Iteration 41, loss = 0.37045994\n",
      "Iteration 42, loss = 0.36954752\n",
      "Iteration 43, loss = 0.36870732\n",
      "Iteration 44, loss = 0.36781391\n",
      "Iteration 45, loss = 0.36697454\n",
      "Iteration 46, loss = 0.36622966\n",
      "Iteration 47, loss = 0.36549643\n",
      "Iteration 48, loss = 0.36475688\n",
      "Iteration 49, loss = 0.36399957\n",
      "Iteration 50, loss = 0.36328215\n",
      "Iteration 51, loss = 0.36269524\n",
      "Iteration 52, loss = 0.36203304\n",
      "Iteration 53, loss = 0.36139561\n",
      "Iteration 54, loss = 0.36078175\n",
      "Iteration 55, loss = 0.36020856\n",
      "Iteration 56, loss = 0.35965520\n",
      "Iteration 57, loss = 0.35920780\n",
      "Iteration 58, loss = 0.35860019\n",
      "Iteration 59, loss = 0.35818077\n",
      "Iteration 60, loss = 0.35780487\n",
      "Iteration 61, loss = 0.35713066\n",
      "Iteration 62, loss = 0.35670663\n",
      "Iteration 63, loss = 0.35628245\n",
      "Iteration 64, loss = 0.35587962\n",
      "Iteration 65, loss = 0.35537927\n",
      "Iteration 66, loss = 0.35491714\n",
      "Iteration 67, loss = 0.35461741\n",
      "Iteration 68, loss = 0.35429175\n",
      "Iteration 69, loss = 0.35398123\n",
      "Iteration 70, loss = 0.35342137\n",
      "Iteration 71, loss = 0.35306114\n",
      "Iteration 72, loss = 0.35281905\n",
      "Iteration 73, loss = 0.35242666\n",
      "Iteration 74, loss = 0.35221960\n",
      "Iteration 75, loss = 0.35188000\n",
      "Iteration 76, loss = 0.35166950\n",
      "Iteration 77, loss = 0.35117186\n",
      "Iteration 78, loss = 0.35094540\n",
      "Iteration 79, loss = 0.35059498\n",
      "Iteration 80, loss = 0.35038380\n",
      "Iteration 81, loss = 0.35003898\n",
      "Iteration 82, loss = 0.35003257\n",
      "Iteration 83, loss = 0.34961374\n",
      "Iteration 84, loss = 0.34932957\n",
      "Iteration 85, loss = 0.34897860\n",
      "Iteration 86, loss = 0.34888479\n",
      "Iteration 87, loss = 0.34848125\n",
      "Iteration 88, loss = 0.34848213\n",
      "Iteration 89, loss = 0.34810985\n",
      "Iteration 90, loss = 0.34786424\n",
      "Iteration 91, loss = 0.34758798\n",
      "Iteration 92, loss = 0.34747482\n",
      "Iteration 93, loss = 0.34734842\n",
      "Iteration 94, loss = 0.34709709\n",
      "Iteration 95, loss = 0.34679885\n",
      "Iteration 96, loss = 0.34671531\n",
      "Iteration 97, loss = 0.34633570\n",
      "Iteration 98, loss = 0.34623477\n",
      "Iteration 99, loss = 0.34600488\n",
      "Iteration 100, loss = 0.34594194\n",
      "Iteration 101, loss = 0.34576054\n",
      "Iteration 102, loss = 0.34552364\n",
      "Iteration 103, loss = 0.34538052\n",
      "Iteration 104, loss = 0.34518875\n",
      "Iteration 105, loss = 0.34513194\n",
      "Iteration 106, loss = 0.34494197\n",
      "Iteration 107, loss = 0.34460675\n",
      "Iteration 108, loss = 0.34442635\n",
      "Iteration 109, loss = 0.34436826\n",
      "Iteration 110, loss = 0.34418819\n",
      "Iteration 111, loss = 0.34409172\n",
      "Iteration 112, loss = 0.34385003\n",
      "Iteration 113, loss = 0.34375745\n",
      "Iteration 114, loss = 0.34351020\n",
      "Iteration 115, loss = 0.34333549\n",
      "Iteration 116, loss = 0.34328777\n",
      "Iteration 117, loss = 0.34307785\n",
      "Iteration 118, loss = 0.34296493\n",
      "Iteration 119, loss = 0.34276795\n",
      "Iteration 120, loss = 0.34267020\n",
      "Iteration 121, loss = 0.34246510\n",
      "Iteration 122, loss = 0.34234821\n",
      "Iteration 123, loss = 0.34222448\n",
      "Iteration 124, loss = 0.34217559\n",
      "Iteration 125, loss = 0.34199894\n",
      "Iteration 126, loss = 0.34197969\n",
      "Iteration 127, loss = 0.34181494\n",
      "Iteration 128, loss = 0.34159828\n",
      "Iteration 129, loss = 0.34145873\n",
      "Iteration 130, loss = 0.34145222\n",
      "Iteration 131, loss = 0.34125325\n",
      "Iteration 132, loss = 0.34121503\n",
      "Iteration 133, loss = 0.34100241\n",
      "Iteration 134, loss = 0.34097339\n",
      "Iteration 135, loss = 0.34076889\n",
      "Iteration 136, loss = 0.34061939\n",
      "Iteration 137, loss = 0.34058047\n",
      "Iteration 138, loss = 0.34046164\n",
      "Iteration 139, loss = 0.34039157\n",
      "Iteration 140, loss = 0.34017161\n",
      "Iteration 141, loss = 0.34026941\n",
      "Iteration 142, loss = 0.33994995\n",
      "Iteration 143, loss = 0.33995325\n",
      "Iteration 144, loss = 0.33984701\n",
      "Iteration 145, loss = 0.33963783\n",
      "Iteration 146, loss = 0.33954184\n",
      "Iteration 147, loss = 0.33942530\n",
      "Iteration 148, loss = 0.33932320\n",
      "Iteration 149, loss = 0.33909751\n",
      "Iteration 150, loss = 0.33911222\n",
      "Iteration 151, loss = 0.33902491\n",
      "Iteration 152, loss = 0.33908178\n",
      "Iteration 153, loss = 0.33889714\n",
      "Iteration 154, loss = 0.33878191\n",
      "Iteration 155, loss = 0.33865980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 0.22348120\n",
      "Iteration 253, loss = 0.22150547\n",
      "Iteration 254, loss = 0.22129646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.41925204\n",
      "Iteration 2, loss = 0.35769312\n",
      "Iteration 3, loss = 0.34593948\n",
      "Iteration 4, loss = 0.33760829\n",
      "Iteration 5, loss = 0.33037981\n",
      "Iteration 6, loss = 0.32598350\n",
      "Iteration 7, loss = 0.32160661\n",
      "Iteration 8, loss = 0.31751030\n",
      "Iteration 9, loss = 0.31087344\n",
      "Iteration 10, loss = 0.30591202\n",
      "Iteration 11, loss = 0.30225875\n",
      "Iteration 12, loss = 0.29815404\n",
      "Iteration 13, loss = 0.29374406\n",
      "Iteration 14, loss = 0.28991595\n",
      "Iteration 15, loss = 0.28753631\n",
      "Iteration 16, loss = 0.28409139\n",
      "Iteration 17, loss = 0.28213790\n",
      "Iteration 18, loss = 0.28009124\n",
      "Iteration 19, loss = 0.27645087\n",
      "Iteration 20, loss = 0.27603817\n",
      "Iteration 21, loss = 0.27501639\n",
      "Iteration 22, loss = 0.27324449\n",
      "Iteration 23, loss = 0.27067212\n",
      "Iteration 24, loss = 0.27064439\n",
      "Iteration 25, loss = 0.26808315\n",
      "Iteration 26, loss = 0.26724964\n",
      "Iteration 27, loss = 0.26584825\n",
      "Iteration 28, loss = 0.26494540\n",
      "Iteration 29, loss = 0.26225583\n",
      "Iteration 30, loss = 0.26195773\n",
      "Iteration 31, loss = 0.26107725\n",
      "Iteration 32, loss = 0.25790988\n",
      "Iteration 33, loss = 0.25635043\n",
      "Iteration 34, loss = 0.25517857\n",
      "Iteration 35, loss = 0.25575362\n",
      "Iteration 36, loss = 0.25315320\n",
      "Iteration 37, loss = 0.25357127\n",
      "Iteration 38, loss = 0.25308584\n",
      "Iteration 39, loss = 0.25229045\n",
      "Iteration 40, loss = 0.25031827\n",
      "Iteration 41, loss = 0.25106265\n",
      "Iteration 42, loss = 0.25046592\n",
      "Iteration 43, loss = 0.24900716\n",
      "Iteration 44, loss = 0.24744947\n",
      "Iteration 45, loss = 0.24821736\n",
      "Iteration 46, loss = 0.24786621\n",
      "Iteration 47, loss = 0.24679926\n",
      "Iteration 48, loss = 0.24736654\n",
      "Iteration 49, loss = 0.24612995\n",
      "Iteration 50, loss = 0.24595870\n",
      "Iteration 51, loss = 0.24605538\n",
      "Iteration 52, loss = 0.24523506\n",
      "Iteration 53, loss = 0.24335530\n",
      "Iteration 54, loss = 0.24380461\n",
      "Iteration 55, loss = 0.24472795\n",
      "Iteration 56, loss = 0.24271168\n",
      "Iteration 57, loss = 0.24360398\n",
      "Iteration 58, loss = 0.24327153\n",
      "Iteration 59, loss = 0.24324842\n",
      "Iteration 60, loss = 0.24197108\n",
      "Iteration 61, loss = 0.24160351\n",
      "Iteration 62, loss = 0.24410064\n",
      "Iteration 63, loss = 0.24268078\n",
      "Iteration 64, loss = 0.24113684\n",
      "Iteration 65, loss = 0.24178529\n",
      "Iteration 66, loss = 0.24250111\n",
      "Iteration 67, loss = 0.24075805\n",
      "Iteration 68, loss = 0.24005097\n",
      "Iteration 69, loss = 0.23977924\n",
      "Iteration 70, loss = 0.23901881\n",
      "Iteration 71, loss = 0.24208601\n",
      "Iteration 72, loss = 0.24018300\n",
      "Iteration 73, loss = 0.23945013\n",
      "Iteration 74, loss = 0.24084444\n",
      "Iteration 75, loss = 0.23927569\n",
      "Iteration 76, loss = 0.23959908\n",
      "Iteration 77, loss = 0.23837227\n",
      "Iteration 78, loss = 0.23812419\n",
      "Iteration 79, loss = 0.23652441\n",
      "Iteration 80, loss = 0.23841284\n",
      "Iteration 81, loss = 0.23703692\n",
      "Iteration 82, loss = 0.23853148\n",
      "Iteration 83, loss = 0.23774987\n",
      "Iteration 84, loss = 0.23711667\n",
      "Iteration 85, loss = 0.23812270\n",
      "Iteration 86, loss = 0.23690243\n",
      "Iteration 87, loss = 0.23770291\n",
      "Iteration 88, loss = 0.23890407\n",
      "Iteration 89, loss = 0.23743058\n",
      "Iteration 90, loss = 0.23472727\n",
      "Iteration 91, loss = 0.23661665\n",
      "Iteration 92, loss = 0.23630444\n",
      "Iteration 93, loss = 0.23623818\n",
      "Iteration 94, loss = 0.23620990\n",
      "Iteration 95, loss = 0.23705324\n",
      "Iteration 96, loss = 0.23593222\n",
      "Iteration 97, loss = 0.23519480\n",
      "Iteration 98, loss = 0.23420495\n",
      "Iteration 99, loss = 0.23572882\n",
      "Iteration 100, loss = 0.23329111\n",
      "Iteration 101, loss = 0.23484049\n",
      "Iteration 102, loss = 0.23233231\n",
      "Iteration 103, loss = 0.23531360\n",
      "Iteration 104, loss = 0.23308180\n",
      "Iteration 105, loss = 0.23297692\n",
      "Iteration 106, loss = 0.23568191\n",
      "Iteration 107, loss = 0.23272788\n",
      "Iteration 108, loss = 0.23574161\n",
      "Iteration 109, loss = 0.23283875\n",
      "Iteration 110, loss = 0.23604729\n",
      "Iteration 111, loss = 0.23468482\n",
      "Iteration 112, loss = 0.23221994\n",
      "Iteration 113, loss = 0.23315101\n",
      "Iteration 114, loss = 0.23254717\n",
      "Iteration 115, loss = 0.23506616\n",
      "Iteration 116, loss = 0.23273237\n",
      "Iteration 117, loss = 0.23352170\n",
      "Iteration 118, loss = 0.23175624\n",
      "Iteration 119, loss = 0.23373924\n",
      "Iteration 120, loss = 0.23106743\n",
      "Iteration 121, loss = 0.23308430\n",
      "Iteration 122, loss = 0.23196698\n",
      "Iteration 123, loss = 0.23236897\n",
      "Iteration 124, loss = 0.23390731\n",
      "Iteration 125, loss = 0.23318654\n",
      "Iteration 126, loss = 0.23350830\n",
      "Iteration 127, loss = 0.23282027\n",
      "Iteration 128, loss = 0.23097343\n",
      "Iteration 129, loss = 0.23276485\n",
      "Iteration 130, loss = 0.23276614\n",
      "Iteration 131, loss = 0.23149478\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49829481\n",
      "Iteration 2, loss = 0.39377489\n",
      "Iteration 3, loss = 0.37168628\n",
      "Iteration 4, loss = 0.36108422\n",
      "Iteration 5, loss = 0.35515609\n",
      "Iteration 6, loss = 0.35173882\n",
      "Iteration 7, loss = 0.34869852\n",
      "Iteration 8, loss = 0.34651212\n",
      "Iteration 9, loss = 0.34499630\n",
      "Iteration 10, loss = 0.34286633\n",
      "Iteration 11, loss = 0.34211205\n",
      "Iteration 12, loss = 0.34059955\n",
      "Iteration 13, loss = 0.33961868\n",
      "Iteration 14, loss = 0.33893069\n",
      "Iteration 15, loss = 0.33791072\n",
      "Iteration 16, loss = 0.33730634\n",
      "Iteration 17, loss = 0.33630566\n",
      "Iteration 18, loss = 0.33585786\n",
      "Iteration 19, loss = 0.33529501\n",
      "Iteration 20, loss = 0.33423614\n",
      "Iteration 21, loss = 0.33359600\n",
      "Iteration 22, loss = 0.33287917\n",
      "Iteration 23, loss = 0.33241103\n",
      "Iteration 24, loss = 0.33159133\n",
      "Iteration 25, loss = 0.33058386\n",
      "Iteration 26, loss = 0.33030558\n",
      "Iteration 27, loss = 0.32952108\n",
      "Iteration 28, loss = 0.32931366\n",
      "Iteration 29, loss = 0.32871757\n",
      "Iteration 30, loss = 0.32815200\n",
      "Iteration 31, loss = 0.32752365\n",
      "Iteration 32, loss = 0.32729716\n",
      "Iteration 33, loss = 0.32699396\n",
      "Iteration 34, loss = 0.32671493\n",
      "Iteration 35, loss = 0.32627727\n",
      "Iteration 36, loss = 0.32622242\n",
      "Iteration 37, loss = 0.32551509\n",
      "Iteration 38, loss = 0.32567609\n",
      "Iteration 39, loss = 0.32491139\n",
      "Iteration 40, loss = 0.32517080\n",
      "Iteration 41, loss = 0.32458965\n",
      "Iteration 42, loss = 0.32408763\n",
      "Iteration 43, loss = 0.32410776\n",
      "Iteration 44, loss = 0.32351722\n",
      "Iteration 45, loss = 0.32348461\n",
      "Iteration 46, loss = 0.32327623\n",
      "Iteration 47, loss = 0.32325355\n",
      "Iteration 48, loss = 0.32301928\n",
      "Iteration 49, loss = 0.32264440\n",
      "Iteration 50, loss = 0.32245815\n",
      "Iteration 51, loss = 0.32221924\n",
      "Iteration 52, loss = 0.32210757\n",
      "Iteration 53, loss = 0.32186081\n",
      "Iteration 54, loss = 0.32159258\n",
      "Iteration 55, loss = 0.32158877\n",
      "Iteration 56, loss = 0.32122097\n",
      "Iteration 57, loss = 0.32114709\n",
      "Iteration 58, loss = 0.32084694\n",
      "Iteration 59, loss = 0.32055781\n",
      "Iteration 60, loss = 0.32059373\n",
      "Iteration 61, loss = 0.32034499\n",
      "Iteration 62, loss = 0.32032881\n",
      "Iteration 63, loss = 0.31990387\n",
      "Iteration 64, loss = 0.31944008\n",
      "Iteration 65, loss = 0.31952941\n",
      "Iteration 66, loss = 0.31932340\n",
      "Iteration 67, loss = 0.31894166\n",
      "Iteration 68, loss = 0.31919258\n",
      "Iteration 69, loss = 0.31915098\n",
      "Iteration 70, loss = 0.31856777\n",
      "Iteration 71, loss = 0.31840616\n",
      "Iteration 72, loss = 0.31835601\n",
      "Iteration 73, loss = 0.31827084\n",
      "Iteration 74, loss = 0.31817040\n",
      "Iteration 75, loss = 0.31788901\n",
      "Iteration 76, loss = 0.31770860\n",
      "Iteration 77, loss = 0.31769428\n",
      "Iteration 78, loss = 0.31735790\n",
      "Iteration 79, loss = 0.31750909\n",
      "Iteration 80, loss = 0.31713604\n",
      "Iteration 81, loss = 0.31727864\n",
      "Iteration 82, loss = 0.31693321\n",
      "Iteration 83, loss = 0.31658319\n",
      "Iteration 84, loss = 0.31648117\n",
      "Iteration 85, loss = 0.31642907\n",
      "Iteration 86, loss = 0.31621821\n",
      "Iteration 87, loss = 0.31603466\n",
      "Iteration 88, loss = 0.31630464\n",
      "Iteration 89, loss = 0.31571775\n",
      "Iteration 90, loss = 0.31588462\n",
      "Iteration 91, loss = 0.31518306\n",
      "Iteration 92, loss = 0.31532576\n",
      "Iteration 93, loss = 0.31498080\n",
      "Iteration 94, loss = 0.31536934\n",
      "Iteration 95, loss = 0.31493202\n",
      "Iteration 96, loss = 0.31459357\n",
      "Iteration 97, loss = 0.31445598\n",
      "Iteration 98, loss = 0.31462045\n",
      "Iteration 99, loss = 0.31426513\n",
      "Iteration 100, loss = 0.31414812\n",
      "Iteration 101, loss = 0.31412950\n",
      "Iteration 102, loss = 0.31417293\n",
      "Iteration 103, loss = 0.31372883\n",
      "Iteration 104, loss = 0.31382866\n",
      "Iteration 105, loss = 0.31344754\n",
      "Iteration 106, loss = 0.31332646\n",
      "Iteration 107, loss = 0.31329048\n",
      "Iteration 108, loss = 0.31316333\n",
      "Iteration 109, loss = 0.31299155\n",
      "Iteration 110, loss = 0.31279932\n",
      "Iteration 111, loss = 0.31279972\n",
      "Iteration 112, loss = 0.31258828\n",
      "Iteration 113, loss = 0.31262721\n",
      "Iteration 114, loss = 0.31214870\n",
      "Iteration 115, loss = 0.31202438\n",
      "Iteration 252, loss = 0.28044684\n",
      "Iteration 253, loss = 0.28013626\n",
      "Iteration 254, loss = 0.27995100\n",
      "Iteration 255, loss = 0.28028630\n",
      "Iteration 256, loss = 0.28014258\n",
      "Iteration 257, loss = 0.27940295\n",
      "Iteration 258, loss = 0.28042690\n",
      "Iteration 259, loss = 0.27959799\n",
      "Iteration 260, loss = 0.27945489\n",
      "Iteration 261, loss = 0.27928294\n",
      "Iteration 262, loss = 0.27926419\n",
      "Iteration 263, loss = 0.27936490\n",
      "Iteration 264, loss = 0.27863991\n",
      "Iteration 265, loss = 0.27883025\n",
      "Iteration 266, loss = 0.27826016\n",
      "Iteration 267, loss = 0.27856109\n",
      "Iteration 268, loss = 0.27874952\n",
      "Iteration 269, loss = 0.27850274\n",
      "Iteration 270, loss = 0.27780925\n",
      "Iteration 271, loss = 0.27906045\n",
      "Iteration 272, loss = 0.27799432\n",
      "Iteration 273, loss = 0.27821177\n",
      "Iteration 274, loss = 0.27715778\n",
      "Iteration 275, loss = 0.27655179\n",
      "Iteration 276, loss = 0.27681337\n",
      "Iteration 277, loss = 0.27711337\n",
      "Iteration 278, loss = 0.27696919\n",
      "Iteration 279, loss = 0.27690724\n",
      "Iteration 280, loss = 0.27674932\n",
      "Iteration 281, loss = 0.27643341\n",
      "Iteration 282, loss = 0.27638415\n",
      "Iteration 283, loss = 0.27587269\n",
      "Iteration 284, loss = 0.27599334\n",
      "Iteration 285, loss = 0.27630312\n",
      "Iteration 286, loss = 0.27517110\n",
      "Iteration 287, loss = 0.27572678\n",
      "Iteration 288, loss = 0.27592546\n",
      "Iteration 289, loss = 0.27567957\n",
      "Iteration 290, loss = 0.27478647\n",
      "Iteration 291, loss = 0.27526258\n",
      "Iteration 292, loss = 0.27428430\n",
      "Iteration 293, loss = 0.27496366\n",
      "Iteration 294, loss = 0.27475785\n",
      "Iteration 295, loss = 0.27426915\n",
      "Iteration 296, loss = 0.27405440\n",
      "Iteration 297, loss = 0.27353375\n",
      "Iteration 298, loss = 0.27359291\n",
      "Iteration 299, loss = 0.27410054\n",
      "Iteration 300, loss = 0.27369377\n",
      "Iteration 1, loss = 0.41925204\n",
      "Iteration 2, loss = 0.35769312\n",
      "Iteration 3, loss = 0.34593948\n",
      "Iteration 4, loss = 0.33760829\n",
      "Iteration 5, loss = 0.33037981\n",
      "Iteration 6, loss = 0.32598350\n",
      "Iteration 7, loss = 0.32160661\n",
      "Iteration 8, loss = 0.31751030\n",
      "Iteration 9, loss = 0.31087344\n",
      "Iteration 10, loss = 0.30591202\n",
      "Iteration 11, loss = 0.30225875\n",
      "Iteration 12, loss = 0.29815404\n",
      "Iteration 13, loss = 0.29374406\n",
      "Iteration 14, loss = 0.28991595\n",
      "Iteration 15, loss = 0.28753631\n",
      "Iteration 16, loss = 0.28409139\n",
      "Iteration 17, loss = 0.28213790\n",
      "Iteration 18, loss = 0.28009124\n",
      "Iteration 19, loss = 0.27645087\n",
      "Iteration 20, loss = 0.27603817\n",
      "Iteration 21, loss = 0.27501639\n",
      "Iteration 22, loss = 0.27324449\n",
      "Iteration 23, loss = 0.27067212\n",
      "Iteration 24, loss = 0.27064439\n",
      "Iteration 25, loss = 0.26808315\n",
      "Iteration 26, loss = 0.26724964\n",
      "Iteration 27, loss = 0.26584825\n",
      "Iteration 28, loss = 0.26494540\n",
      "Iteration 29, loss = 0.26225583\n",
      "Iteration 30, loss = 0.26195773\n",
      "Iteration 31, loss = 0.26107725\n",
      "Iteration 32, loss = 0.25790988\n",
      "Iteration 33, loss = 0.25635043\n",
      "Iteration 34, loss = 0.25517857\n",
      "Iteration 35, loss = 0.25575362\n",
      "Iteration 36, loss = 0.25315320\n",
      "Iteration 37, loss = 0.25357127\n",
      "Iteration 38, loss = 0.25308584\n",
      "Iteration 39, loss = 0.25229045\n",
      "Iteration 40, loss = 0.25031827\n",
      "Iteration 41, loss = 0.25106265\n",
      "Iteration 42, loss = 0.25046592\n",
      "Iteration 43, loss = 0.24900716\n",
      "Iteration 44, loss = 0.24744947\n",
      "Iteration 45, loss = 0.24821736\n",
      "Iteration 46, loss = 0.24786621\n",
      "Iteration 47, loss = 0.24679926\n",
      "Iteration 48, loss = 0.24736654\n",
      "Iteration 49, loss = 0.24612995\n",
      "Iteration 50, loss = 0.24595870\n",
      "Iteration 51, loss = 0.24605538\n",
      "Iteration 52, loss = 0.24523506\n",
      "Iteration 53, loss = 0.24335530\n",
      "Iteration 54, loss = 0.24380461\n",
      "Iteration 55, loss = 0.24472795\n",
      "Iteration 56, loss = 0.24271168\n",
      "Iteration 57, loss = 0.24360398\n",
      "Iteration 58, loss = 0.24327153\n",
      "Iteration 59, loss = 0.24324842\n",
      "Iteration 60, loss = 0.24197108\n",
      "Iteration 61, loss = 0.24160351\n",
      "Iteration 62, loss = 0.24410064\n",
      "Iteration 63, loss = 0.24268078\n",
      "Iteration 64, loss = 0.24113684\n",
      "Iteration 65, loss = 0.24178529\n",
      "Iteration 66, loss = 0.24250111\n",
      "Iteration 67, loss = 0.24075805\n",
      "Iteration 68, loss = 0.24005097\n",
      "Iteration 69, loss = 0.23977924\n",
      "Iteration 70, loss = 0.23901881\n",
      "Iteration 71, loss = 0.24208601\n",
      "Iteration 72, loss = 0.24018300\n",
      "Iteration 73, loss = 0.23945013\n",
      "Iteration 74, loss = 0.24084444\n",
      "Iteration 75, loss = 0.23927569\n",
      "Iteration 76, loss = 0.23959908\n",
      "Iteration 77, loss = 0.23837227\n",
      "Iteration 78, loss = 0.23812419\n",
      "Iteration 79, loss = 0.23652441\n",
      "Iteration 80, loss = 0.23841284\n",
      "Iteration 81, loss = 0.23703692\n",
      "Iteration 82, loss = 0.23853148\n",
      "Iteration 83, loss = 0.23774987\n",
      "Iteration 84, loss = 0.23711667\n",
      "Iteration 85, loss = 0.23812270\n",
      "Iteration 86, loss = 0.23690243\n",
      "Iteration 87, loss = 0.23770291\n",
      "Iteration 88, loss = 0.23890407\n",
      "Iteration 89, loss = 0.23743058\n",
      "Iteration 90, loss = 0.23472727\n",
      "Iteration 91, loss = 0.23661665\n",
      "Iteration 92, loss = 0.23630444\n",
      "Iteration 93, loss = 0.23623818\n",
      "Iteration 94, loss = 0.23620990\n",
      "Iteration 95, loss = 0.23705324\n",
      "Iteration 96, loss = 0.23593222\n",
      "Iteration 97, loss = 0.23519480\n",
      "Iteration 98, loss = 0.23420495\n",
      "Iteration 99, loss = 0.23572882\n",
      "Iteration 100, loss = 0.23329111\n",
      "Iteration 101, loss = 0.23484049\n",
      "Iteration 102, loss = 0.23233231\n",
      "Iteration 103, loss = 0.23531360\n",
      "Iteration 104, loss = 0.23308180\n",
      "Iteration 105, loss = 0.23297692\n",
      "Iteration 106, loss = 0.23568191\n",
      "Iteration 107, loss = 0.23272788\n",
      "Iteration 108, loss = 0.23574161\n",
      "Iteration 109, loss = 0.23283875\n",
      "Iteration 110, loss = 0.23604729\n",
      "Iteration 111, loss = 0.23468482\n",
      "Iteration 112, loss = 0.23221994\n",
      "Iteration 113, loss = 0.23315101\n",
      "Iteration 114, loss = 0.23254717\n",
      "Iteration 115, loss = 0.23506616\n",
      "Iteration 116, loss = 0.23273237\n",
      "Iteration 117, loss = 0.23352170\n",
      "Iteration 118, loss = 0.23175624\n",
      "Iteration 119, loss = 0.23373924\n",
      "Iteration 120, loss = 0.23106743\n",
      "Iteration 121, loss = 0.23308430\n",
      "Iteration 122, loss = 0.23196698\n",
      "Iteration 123, loss = 0.23236897\n",
      "Iteration 124, loss = 0.23390731\n",
      "Iteration 125, loss = 0.23318654\n",
      "Iteration 126, loss = 0.23350830\n",
      "Iteration 127, loss = 0.23282027\n",
      "Iteration 128, loss = 0.23097343\n",
      "Iteration 129, loss = 0.23276485\n",
      "Iteration 130, loss = 0.23276614\n",
      "Iteration 131, loss = 0.23149478\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49825320\n",
      "Iteration 2, loss = 0.39289127\n",
      "Iteration 3, loss = 0.36996266\n",
      "Iteration 4, loss = 0.35973032\n",
      "Iteration 5, loss = 0.35410431\n",
      "Iteration 6, loss = 0.34997211\n",
      "Iteration 7, loss = 0.34731359\n",
      "Iteration 8, loss = 0.34518130\n",
      "Iteration 9, loss = 0.34338341\n",
      "Iteration 10, loss = 0.34158647\n",
      "Iteration 11, loss = 0.34110960\n",
      "Iteration 12, loss = 0.33974364\n",
      "Iteration 13, loss = 0.33858253\n",
      "Iteration 14, loss = 0.33789763\n",
      "Iteration 15, loss = 0.33708279\n",
      "Iteration 16, loss = 0.33640220\n",
      "Iteration 17, loss = 0.33578181\n",
      "Iteration 18, loss = 0.33519330\n",
      "Iteration 19, loss = 0.33435492\n",
      "Iteration 20, loss = 0.33367376\n",
      "Iteration 21, loss = 0.33314117\n",
      "Iteration 22, loss = 0.33266577\n",
      "Iteration 23, loss = 0.33213679\n",
      "Iteration 24, loss = 0.33187969\n",
      "Iteration 25, loss = 0.33080080\n",
      "Iteration 26, loss = 0.33047000\n",
      "Iteration 27, loss = 0.32951753\n",
      "Iteration 28, loss = 0.32879450\n",
      "Iteration 29, loss = 0.32832789\n",
      "Iteration 30, loss = 0.32797988\n",
      "Iteration 31, loss = 0.32749473\n",
      "Iteration 32, loss = 0.32714818\n",
      "Iteration 33, loss = 0.32656866\n",
      "Iteration 34, loss = 0.32644693\n",
      "Iteration 35, loss = 0.32568402\n",
      "Iteration 36, loss = 0.32561041\n",
      "Iteration 37, loss = 0.32513735\n",
      "Iteration 38, loss = 0.32469471\n",
      "Iteration 39, loss = 0.32430780\n",
      "Iteration 40, loss = 0.32430518\n",
      "Iteration 41, loss = 0.32373050\n",
      "Iteration 42, loss = 0.32323644\n",
      "Iteration 43, loss = 0.32325064\n",
      "Iteration 44, loss = 0.32287460\n",
      "Iteration 45, loss = 0.32252779\n",
      "Iteration 46, loss = 0.32241052\n",
      "Iteration 47, loss = 0.32229741\n",
      "Iteration 48, loss = 0.32201030\n",
      "Iteration 49, loss = 0.32179306\n",
      "Iteration 50, loss = 0.32145865\n",
      "Iteration 51, loss = 0.32125229\n",
      "Iteration 52, loss = 0.32135631\n",
      "Iteration 53, loss = 0.32102208\n",
      "Iteration 54, loss = 0.32081484\n",
      "Iteration 55, loss = 0.32026708\n",
      "Iteration 56, loss = 0.32038619\n",
      "Iteration 57, loss = 0.32035780\n",
      "Iteration 58, loss = 0.32024581\n",
      "Iteration 59, loss = 0.32004934\n",
      "Iteration 60, loss = 0.32007126\n",
      "Iteration 61, loss = 0.31929584\n",
      "Iteration 62, loss = 0.31933585\n",
      "Iteration 63, loss = 0.31903409\n",
      "Iteration 64, loss = 0.31887833\n",
      "Iteration 65, loss = 0.31846888\n",
      "Iteration 66, loss = 0.31826910\n",
      "Iteration 67, loss = 0.31821413\n",
      "Iteration 68, loss = 0.31833642\n",
      "Iteration 69, loss = 0.31813335\n",
      "Iteration 70, loss = 0.31741478\n",
      "Iteration 71, loss = 0.31768053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 0.28008155\n",
      "Iteration 253, loss = 0.27951919\n",
      "Iteration 254, loss = 0.27962317\n",
      "Iteration 255, loss = 0.27988080\n",
      "Iteration 256, loss = 0.27986827\n",
      "Iteration 257, loss = 0.27941192\n",
      "Iteration 258, loss = 0.27959394\n",
      "Iteration 259, loss = 0.27930321\n",
      "Iteration 260, loss = 0.27902233\n",
      "Iteration 261, loss = 0.27892441\n",
      "Iteration 262, loss = 0.27906118\n",
      "Iteration 263, loss = 0.27864037\n",
      "Iteration 264, loss = 0.27892579\n",
      "Iteration 265, loss = 0.27844614\n",
      "Iteration 266, loss = 0.27813151\n",
      "Iteration 267, loss = 0.27845934\n",
      "Iteration 268, loss = 0.27793719\n",
      "Iteration 269, loss = 0.27772792\n",
      "Iteration 270, loss = 0.27773731\n",
      "Iteration 271, loss = 0.27755737\n",
      "Iteration 272, loss = 0.27801606\n",
      "Iteration 273, loss = 0.27722057\n",
      "Iteration 274, loss = 0.27698081\n",
      "Iteration 275, loss = 0.27670038\n",
      "Iteration 276, loss = 0.27663265\n",
      "Iteration 277, loss = 0.27669203\n",
      "Iteration 278, loss = 0.27639759\n",
      "Iteration 279, loss = 0.27670682\n",
      "Iteration 280, loss = 0.27655187\n",
      "Iteration 281, loss = 0.27539968\n",
      "Iteration 282, loss = 0.27540874\n",
      "Iteration 283, loss = 0.27546993\n",
      "Iteration 284, loss = 0.27547663\n",
      "Iteration 285, loss = 0.27583302\n",
      "Iteration 286, loss = 0.27502096\n",
      "Iteration 287, loss = 0.27444990\n",
      "Iteration 288, loss = 0.27506202\n",
      "Iteration 289, loss = 0.27392363\n",
      "Iteration 290, loss = 0.27478634\n",
      "Iteration 291, loss = 0.27431464\n",
      "Iteration 292, loss = 0.27315277\n",
      "Iteration 293, loss = 0.27400068\n",
      "Iteration 294, loss = 0.27378367\n",
      "Iteration 295, loss = 0.27356624\n",
      "Iteration 296, loss = 0.27345328\n",
      "Iteration 297, loss = 0.27279013\n",
      "Iteration 298, loss = 0.27223314\n",
      "Iteration 299, loss = 0.27261181\n",
      "Iteration 300, loss = 0.27252086\n",
      "Iteration 1, loss = 0.42106300\n",
      "Iteration 2, loss = 0.35826820\n",
      "Iteration 3, loss = 0.34670221\n",
      "Iteration 4, loss = 0.33945670\n",
      "Iteration 5, loss = 0.33276656\n",
      "Iteration 6, loss = 0.32703638\n",
      "Iteration 7, loss = 0.32094046\n",
      "Iteration 8, loss = 0.31632741\n",
      "Iteration 9, loss = 0.30943836\n",
      "Iteration 10, loss = 0.30378596\n",
      "Iteration 11, loss = 0.29925129\n",
      "Iteration 12, loss = 0.29703105\n",
      "Iteration 13, loss = 0.29331334\n",
      "Iteration 14, loss = 0.28873638\n",
      "Iteration 15, loss = 0.28555225\n",
      "Iteration 16, loss = 0.28424634\n",
      "Iteration 17, loss = 0.28172700\n",
      "Iteration 18, loss = 0.27963410\n",
      "Iteration 19, loss = 0.27760659\n",
      "Iteration 20, loss = 0.27670359\n",
      "Iteration 21, loss = 0.27502887\n",
      "Iteration 22, loss = 0.27463021\n",
      "Iteration 23, loss = 0.27187846\n",
      "Iteration 24, loss = 0.27104961\n",
      "Iteration 25, loss = 0.27000289\n",
      "Iteration 26, loss = 0.26894481\n",
      "Iteration 27, loss = 0.26726184\n",
      "Iteration 28, loss = 0.26572763\n",
      "Iteration 29, loss = 0.26445089\n",
      "Iteration 30, loss = 0.26452981\n",
      "Iteration 31, loss = 0.26176219\n",
      "Iteration 32, loss = 0.26156917\n",
      "Iteration 33, loss = 0.26088482\n",
      "Iteration 34, loss = 0.26012401\n",
      "Iteration 35, loss = 0.25664232\n",
      "Iteration 36, loss = 0.25704973\n",
      "Iteration 37, loss = 0.25693376\n",
      "Iteration 38, loss = 0.25633363\n",
      "Iteration 39, loss = 0.25406206\n",
      "Iteration 40, loss = 0.25499255\n",
      "Iteration 41, loss = 0.25322490\n",
      "Iteration 42, loss = 0.25324684\n",
      "Iteration 43, loss = 0.25153665\n",
      "Iteration 44, loss = 0.25025432\n",
      "Iteration 45, loss = 0.24959969\n",
      "Iteration 46, loss = 0.24860056\n",
      "Iteration 47, loss = 0.24938036\n",
      "Iteration 48, loss = 0.24817396\n",
      "Iteration 49, loss = 0.24714103\n",
      "Iteration 50, loss = 0.24752656\n",
      "Iteration 51, loss = 0.24654821\n",
      "Iteration 52, loss = 0.24707852\n",
      "Iteration 53, loss = 0.24700292\n",
      "Iteration 54, loss = 0.24646447\n",
      "Iteration 55, loss = 0.24596822\n",
      "Iteration 56, loss = 0.24417919\n",
      "Iteration 57, loss = 0.24489982\n",
      "Iteration 58, loss = 0.24360699\n",
      "Iteration 59, loss = 0.24524646\n",
      "Iteration 60, loss = 0.24522084\n",
      "Iteration 61, loss = 0.24153405\n",
      "Iteration 62, loss = 0.24513717\n",
      "Iteration 63, loss = 0.24120651\n",
      "Iteration 64, loss = 0.24386370\n",
      "Iteration 65, loss = 0.24165812\n",
      "Iteration 66, loss = 0.24211874\n",
      "Iteration 67, loss = 0.24015806\n",
      "Iteration 68, loss = 0.24166028\n",
      "Iteration 69, loss = 0.24191448\n",
      "Iteration 70, loss = 0.24117435\n",
      "Iteration 71, loss = 0.24179827\n",
      "Iteration 72, loss = 0.24001597\n",
      "Iteration 73, loss = 0.23943949\n",
      "Iteration 74, loss = 0.23933001\n",
      "Iteration 75, loss = 0.24036836\n",
      "Iteration 76, loss = 0.23939833\n",
      "Iteration 77, loss = 0.23930227\n",
      "Iteration 78, loss = 0.23912687\n",
      "Iteration 79, loss = 0.23960154\n",
      "Iteration 80, loss = 0.23856309\n",
      "Iteration 81, loss = 0.23913822\n",
      "Iteration 82, loss = 0.23823749\n",
      "Iteration 83, loss = 0.23667306\n",
      "Iteration 84, loss = 0.23872659\n",
      "Iteration 85, loss = 0.23894523\n",
      "Iteration 86, loss = 0.23764981\n",
      "Iteration 87, loss = 0.23842497\n",
      "Iteration 88, loss = 0.23830929\n",
      "Iteration 89, loss = 0.23803377\n",
      "Iteration 90, loss = 0.23678242\n",
      "Iteration 91, loss = 0.23558817\n",
      "Iteration 92, loss = 0.23622707\n",
      "Iteration 93, loss = 0.23649178\n",
      "Iteration 94, loss = 0.23574752\n",
      "Iteration 95, loss = 0.23576938\n",
      "Iteration 96, loss = 0.23598699\n",
      "Iteration 97, loss = 0.23655469\n",
      "Iteration 98, loss = 0.23648204\n",
      "Iteration 99, loss = 0.23615831\n",
      "Iteration 100, loss = 0.23570226\n",
      "Iteration 101, loss = 0.23435094\n",
      "Iteration 102, loss = 0.23518063\n",
      "Iteration 103, loss = 0.23454646\n",
      "Iteration 104, loss = 0.23500449\n",
      "Iteration 105, loss = 0.23371575\n",
      "Iteration 106, loss = 0.23304217\n",
      "Iteration 107, loss = 0.23505842\n",
      "Iteration 108, loss = 0.23375282\n",
      "Iteration 109, loss = 0.23448336\n",
      "Iteration 110, loss = 0.23559039\n",
      "Iteration 111, loss = 0.23298001\n",
      "Iteration 112, loss = 0.23479018\n",
      "Iteration 113, loss = 0.23359237\n",
      "Iteration 114, loss = 0.23224876\n",
      "Iteration 115, loss = 0.23376145\n",
      "Iteration 116, loss = 0.23226204\n",
      "Iteration 117, loss = 0.23171819\n",
      "Iteration 118, loss = 0.23361264\n",
      "Iteration 119, loss = 0.23314514\n",
      "Iteration 120, loss = 0.23267015\n",
      "Iteration 121, loss = 0.23324516\n",
      "Iteration 122, loss = 0.23277671\n",
      "Iteration 123, loss = 0.23297207\n",
      "Iteration 124, loss = 0.23184378\n",
      "Iteration 125, loss = 0.23250210\n",
      "Iteration 126, loss = 0.23143361\n",
      "Iteration 127, loss = 0.23156162\n",
      "Iteration 128, loss = 0.23291994\n",
      "Iteration 129, loss = 0.23287729\n",
      "Iteration 130, loss = 0.23111167\n",
      "Iteration 131, loss = 0.23297830\n",
      "Iteration 132, loss = 0.23128817\n",
      "Iteration 133, loss = 0.23057377\n",
      "Iteration 134, loss = 0.23171160\n",
      "Iteration 135, loss = 0.23155429\n",
      "Iteration 136, loss = 0.23181196\n",
      "Iteration 137, loss = 0.23116416\n",
      "Iteration 138, loss = 0.23134604\n",
      "Iteration 139, loss = 0.23191472\n",
      "Iteration 140, loss = 0.23057854\n",
      "Iteration 141, loss = 0.23119519\n",
      "Iteration 142, loss = 0.23058327\n",
      "Iteration 143, loss = 0.23013232\n",
      "Iteration 144, loss = 0.23011215\n",
      "Iteration 145, loss = 0.23042789\n",
      "Iteration 146, loss = 0.23033789\n",
      "Iteration 147, loss = 0.23056792\n",
      "Iteration 148, loss = 0.23008820\n",
      "Iteration 149, loss = 0.23112862\n",
      "Iteration 150, loss = 0.23027609\n",
      "Iteration 151, loss = 0.23148140\n",
      "Iteration 152, loss = 0.23212405\n",
      "Iteration 153, loss = 0.23077118\n",
      "Iteration 154, loss = 0.22797365\n",
      "Iteration 155, loss = 0.22941957\n",
      "Iteration 156, loss = 0.22906135\n",
      "Iteration 157, loss = 0.22888811\n",
      "Iteration 158, loss = 0.23098800\n",
      "Iteration 159, loss = 0.22887628\n",
      "Iteration 160, loss = 0.22908465\n",
      "Iteration 161, loss = 0.22865705\n",
      "Iteration 162, loss = 0.22985460\n",
      "Iteration 163, loss = 0.22920794\n",
      "Iteration 164, loss = 0.22922016\n",
      "Iteration 165, loss = 0.22798084\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65624843\n",
      "Iteration 2, loss = 0.60832709\n",
      "Iteration 3, loss = 0.58518214\n",
      "Iteration 4, loss = 0.56463910\n",
      "Iteration 5, loss = 0.54521381\n",
      "Iteration 6, loss = 0.52699398\n",
      "Iteration 7, loss = 0.51032920\n",
      "Iteration 8, loss = 0.49552535\n",
      "Iteration 9, loss = 0.48248014\n",
      "Iteration 10, loss = 0.47101473\n",
      "Iteration 11, loss = 0.46088939\n",
      "Iteration 12, loss = 0.45195384\n",
      "Iteration 13, loss = 0.44403656\n",
      "Iteration 14, loss = 0.43692157\n",
      "Iteration 15, loss = 0.43065532\n",
      "Iteration 16, loss = 0.42498063\n",
      "Iteration 17, loss = 0.41996399\n",
      "Iteration 18, loss = 0.41546917\n",
      "Iteration 19, loss = 0.41140312\n",
      "Iteration 20, loss = 0.40770972\n",
      "Iteration 21, loss = 0.40434715\n",
      "Iteration 22, loss = 0.40134671\n",
      "Iteration 23, loss = 0.39858749\n",
      "Iteration 24, loss = 0.39601073\n",
      "Iteration 25, loss = 0.39361226\n",
      "Iteration 26, loss = 0.39142769\n",
      "Iteration 27, loss = 0.38934187\n",
      "Iteration 28, loss = 0.38742874\n",
      "Iteration 29, loss = 0.38563851\n",
      "Iteration 30, loss = 0.38394826\n",
      "Iteration 31, loss = 0.38242366\n",
      "Iteration 32, loss = 0.38101376\n",
      "Iteration 33, loss = 0.37952994\n",
      "Iteration 34, loss = 0.37816889\n",
      "Iteration 35, loss = 0.37689688\n",
      "Iteration 36, loss = 0.37577007\n",
      "Iteration 252, loss = 0.28044684\n",
      "Iteration 253, loss = 0.28013626\n",
      "Iteration 254, loss = 0.27995100\n",
      "Iteration 255, loss = 0.28028630\n",
      "Iteration 256, loss = 0.28014258\n",
      "Iteration 257, loss = 0.27940295\n",
      "Iteration 258, loss = 0.28042690\n",
      "Iteration 259, loss = 0.27959799\n",
      "Iteration 260, loss = 0.27945489\n",
      "Iteration 261, loss = 0.27928294\n",
      "Iteration 262, loss = 0.27926419\n",
      "Iteration 263, loss = 0.27936490\n",
      "Iteration 264, loss = 0.27863991\n",
      "Iteration 265, loss = 0.27883025\n",
      "Iteration 266, loss = 0.27826016\n",
      "Iteration 267, loss = 0.27856109\n",
      "Iteration 268, loss = 0.27874952\n",
      "Iteration 269, loss = 0.27850274\n",
      "Iteration 270, loss = 0.27780925\n",
      "Iteration 271, loss = 0.27906045\n",
      "Iteration 272, loss = 0.27799432\n",
      "Iteration 273, loss = 0.27821177\n",
      "Iteration 274, loss = 0.27715778\n",
      "Iteration 275, loss = 0.27655179\n",
      "Iteration 276, loss = 0.27681337\n",
      "Iteration 277, loss = 0.27711337\n",
      "Iteration 278, loss = 0.27696919\n",
      "Iteration 279, loss = 0.27690724\n",
      "Iteration 280, loss = 0.27674932\n",
      "Iteration 281, loss = 0.27643341\n",
      "Iteration 282, loss = 0.27638415\n",
      "Iteration 283, loss = 0.27587269\n",
      "Iteration 284, loss = 0.27599334\n",
      "Iteration 285, loss = 0.27630312\n",
      "Iteration 286, loss = 0.27517110\n",
      "Iteration 287, loss = 0.27572678\n",
      "Iteration 288, loss = 0.27592546\n",
      "Iteration 289, loss = 0.27567957\n",
      "Iteration 290, loss = 0.27478647\n",
      "Iteration 291, loss = 0.27526258\n",
      "Iteration 292, loss = 0.27428430\n",
      "Iteration 293, loss = 0.27496366\n",
      "Iteration 294, loss = 0.27475785\n",
      "Iteration 295, loss = 0.27426915\n",
      "Iteration 296, loss = 0.27405440\n",
      "Iteration 297, loss = 0.27353375\n",
      "Iteration 298, loss = 0.27359291\n",
      "Iteration 299, loss = 0.27410054\n",
      "Iteration 300, loss = 0.27369377\n",
      "Iteration 1, loss = 0.62152458\n",
      "Iteration 2, loss = 0.52426381\n",
      "Iteration 3, loss = 0.43858505\n",
      "Iteration 4, loss = 0.39284364\n",
      "Iteration 5, loss = 0.37558837\n",
      "Iteration 6, loss = 0.36778607\n",
      "Iteration 7, loss = 0.36355447\n",
      "Iteration 8, loss = 0.36001331\n",
      "Iteration 9, loss = 0.35736533\n",
      "Iteration 10, loss = 0.35562043\n",
      "Iteration 11, loss = 0.35326514\n",
      "Iteration 12, loss = 0.35230108\n",
      "Iteration 13, loss = 0.35104179\n",
      "Iteration 14, loss = 0.35003008\n",
      "Iteration 15, loss = 0.34874163\n",
      "Iteration 16, loss = 0.34754042\n",
      "Iteration 17, loss = 0.34627538\n",
      "Iteration 18, loss = 0.34522998\n",
      "Iteration 19, loss = 0.34411020\n",
      "Iteration 20, loss = 0.34340637\n",
      "Iteration 21, loss = 0.34262084\n",
      "Iteration 22, loss = 0.34159236\n",
      "Iteration 23, loss = 0.34083363\n",
      "Iteration 24, loss = 0.33998732\n",
      "Iteration 25, loss = 0.33916601\n",
      "Iteration 26, loss = 0.33832433\n",
      "Iteration 27, loss = 0.33767235\n",
      "Iteration 28, loss = 0.33687501\n",
      "Iteration 29, loss = 0.33628801\n",
      "Iteration 30, loss = 0.33530624\n",
      "Iteration 31, loss = 0.33522592\n",
      "Iteration 32, loss = 0.33425574\n",
      "Iteration 33, loss = 0.33351290\n",
      "Iteration 34, loss = 0.33287160\n",
      "Iteration 35, loss = 0.33191151\n",
      "Iteration 36, loss = 0.33125617\n",
      "Iteration 37, loss = 0.33140652\n",
      "Iteration 38, loss = 0.33045424\n",
      "Iteration 39, loss = 0.32971324\n",
      "Iteration 40, loss = 0.32931281\n",
      "Iteration 41, loss = 0.32834184\n",
      "Iteration 42, loss = 0.32785150\n",
      "Iteration 43, loss = 0.32755627\n",
      "Iteration 44, loss = 0.32730241\n",
      "Iteration 45, loss = 0.32639239\n",
      "Iteration 46, loss = 0.32599881\n",
      "Iteration 47, loss = 0.32548730\n",
      "Iteration 48, loss = 0.32467674\n",
      "Iteration 49, loss = 0.32403171\n",
      "Iteration 50, loss = 0.32346318\n",
      "Iteration 51, loss = 0.32353247\n",
      "Iteration 52, loss = 0.32308537\n",
      "Iteration 53, loss = 0.32233688\n",
      "Iteration 54, loss = 0.32158431\n",
      "Iteration 55, loss = 0.32099129\n",
      "Iteration 56, loss = 0.32031988\n",
      "Iteration 57, loss = 0.32038483\n",
      "Iteration 58, loss = 0.31937956\n",
      "Iteration 59, loss = 0.31917001\n",
      "Iteration 60, loss = 0.31857594\n",
      "Iteration 61, loss = 0.31793303\n",
      "Iteration 62, loss = 0.31775450\n",
      "Iteration 63, loss = 0.31706718\n",
      "Iteration 64, loss = 0.31713484\n",
      "Iteration 65, loss = 0.31633899\n",
      "Iteration 66, loss = 0.31593511\n",
      "Iteration 67, loss = 0.31524288\n",
      "Iteration 68, loss = 0.31513217\n",
      "Iteration 69, loss = 0.31473686\n",
      "Iteration 70, loss = 0.31454109\n",
      "Iteration 71, loss = 0.31411585\n",
      "Iteration 72, loss = 0.31355311\n",
      "Iteration 73, loss = 0.31351083\n",
      "Iteration 74, loss = 0.31287185\n",
      "Iteration 75, loss = 0.31227249\n",
      "Iteration 76, loss = 0.31261800\n",
      "Iteration 77, loss = 0.31158634\n",
      "Iteration 78, loss = 0.31091590\n",
      "Iteration 79, loss = 0.31090940\n",
      "Iteration 80, loss = 0.31044238\n",
      "Iteration 81, loss = 0.31029802\n",
      "Iteration 82, loss = 0.30994833\n",
      "Iteration 83, loss = 0.30972725\n",
      "Iteration 84, loss = 0.30919997\n",
      "Iteration 85, loss = 0.30902908\n",
      "Iteration 86, loss = 0.30853826\n",
      "Iteration 87, loss = 0.30828842\n",
      "Iteration 88, loss = 0.30801832\n",
      "Iteration 89, loss = 0.30766570\n",
      "Iteration 90, loss = 0.30751399\n",
      "Iteration 91, loss = 0.30654184\n",
      "Iteration 92, loss = 0.30671253\n",
      "Iteration 93, loss = 0.30662746\n",
      "Iteration 94, loss = 0.30614771\n",
      "Iteration 95, loss = 0.30575707\n",
      "Iteration 96, loss = 0.30543942\n",
      "Iteration 97, loss = 0.30508338\n",
      "Iteration 98, loss = 0.30481927\n",
      "Iteration 99, loss = 0.30424900\n",
      "Iteration 100, loss = 0.30401733\n",
      "Iteration 101, loss = 0.30472911\n",
      "Iteration 102, loss = 0.30363167\n",
      "Iteration 103, loss = 0.30356931\n",
      "Iteration 104, loss = 0.30329199\n",
      "Iteration 105, loss = 0.30241389\n",
      "Iteration 106, loss = 0.30233166\n",
      "Iteration 107, loss = 0.30248346\n",
      "Iteration 108, loss = 0.30255592\n",
      "Iteration 109, loss = 0.30214872\n",
      "Iteration 110, loss = 0.30142495\n",
      "Iteration 111, loss = 0.30130174\n",
      "Iteration 112, loss = 0.30117023\n",
      "Iteration 113, loss = 0.30077963\n",
      "Iteration 114, loss = 0.29992107\n",
      "Iteration 115, loss = 0.30023004\n",
      "Iteration 116, loss = 0.30004884\n",
      "Iteration 117, loss = 0.30080661\n",
      "Iteration 118, loss = 0.29930196\n",
      "Iteration 119, loss = 0.29982883\n",
      "Iteration 120, loss = 0.29928376\n",
      "Iteration 121, loss = 0.29942070\n",
      "Iteration 122, loss = 0.29906161\n",
      "Iteration 123, loss = 0.29858019\n",
      "Iteration 124, loss = 0.29850567\n",
      "Iteration 125, loss = 0.29797536\n",
      "Iteration 126, loss = 0.29820806\n",
      "Iteration 127, loss = 0.29811008\n",
      "Iteration 128, loss = 0.29753629\n",
      "Iteration 129, loss = 0.29743512\n",
      "Iteration 130, loss = 0.29753056\n",
      "Iteration 131, loss = 0.29739282\n",
      "Iteration 132, loss = 0.29648473\n",
      "Iteration 133, loss = 0.29712838\n",
      "Iteration 134, loss = 0.29663697\n",
      "Iteration 135, loss = 0.29659937\n",
      "Iteration 136, loss = 0.29592073\n",
      "Iteration 137, loss = 0.29609905\n",
      "Iteration 138, loss = 0.29602708\n",
      "Iteration 139, loss = 0.29574175\n",
      "Iteration 140, loss = 0.29507921\n",
      "Iteration 141, loss = 0.29501095\n",
      "Iteration 142, loss = 0.29506165\n",
      "Iteration 143, loss = 0.29479387\n",
      "Iteration 144, loss = 0.29411851\n",
      "Iteration 145, loss = 0.29515876\n",
      "Iteration 146, loss = 0.29412188\n",
      "Iteration 147, loss = 0.29394962\n",
      "Iteration 148, loss = 0.29436458\n",
      "Iteration 149, loss = 0.29383435\n",
      "Iteration 150, loss = 0.29367052\n",
      "Iteration 151, loss = 0.29370371\n",
      "Iteration 152, loss = 0.29347699\n",
      "Iteration 153, loss = 0.29367231\n",
      "Iteration 154, loss = 0.29330478\n",
      "Iteration 155, loss = 0.29309474\n",
      "Iteration 156, loss = 0.29262410\n",
      "Iteration 157, loss = 0.29302137\n",
      "Iteration 158, loss = 0.29280335\n",
      "Iteration 159, loss = 0.29214711\n",
      "Iteration 160, loss = 0.29236827\n",
      "Iteration 161, loss = 0.29212204\n",
      "Iteration 162, loss = 0.29145076\n",
      "Iteration 163, loss = 0.29133511\n",
      "Iteration 164, loss = 0.29104614\n",
      "Iteration 165, loss = 0.29129777\n",
      "Iteration 166, loss = 0.29149907\n",
      "Iteration 167, loss = 0.29123086\n",
      "Iteration 168, loss = 0.29106663\n",
      "Iteration 169, loss = 0.29019072\n",
      "Iteration 170, loss = 0.29076772\n",
      "Iteration 171, loss = 0.29090732\n",
      "Iteration 172, loss = 0.29059578\n",
      "Iteration 173, loss = 0.29033869\n",
      "Iteration 174, loss = 0.28990837\n",
      "Iteration 175, loss = 0.28960415\n",
      "Iteration 176, loss = 0.28988931\n",
      "Iteration 177, loss = 0.28963066\n",
      "Iteration 178, loss = 0.28939584\n",
      "Iteration 179, loss = 0.28895367\n",
      "Iteration 180, loss = 0.28913708\n",
      "Iteration 181, loss = 0.28906593\n",
      "Iteration 182, loss = 0.28858595\n",
      "Iteration 183, loss = 0.28867599\n",
      "Iteration 184, loss = 0.28847202\n",
      "Iteration 185, loss = 0.28844898\n",
      "Iteration 186, loss = 0.28814984\n",
      "Iteration 187, loss = 0.28815773\n",
      "Iteration 188, loss = 0.28812394\n",
      "Iteration 189, loss = 0.28779182\n",
      "Iteration 190, loss = 0.28738617\n",
      "Iteration 191, loss = 0.28753822\n",
      "Iteration 192, loss = 0.28678860\n",
      "Iteration 193, loss = 0.28674606\n",
      "Iteration 194, loss = 0.28699639\n",
      "Iteration 195, loss = 0.28671122\n",
      "Iteration 196, loss = 0.28652408\n",
      "Iteration 197, loss = 0.28612587\n",
      "Iteration 198, loss = 0.28607266\n",
      "Iteration 199, loss = 0.28583261\n",
      "Iteration 200, loss = 0.28557939\n",
      "Iteration 201, loss = 0.28530425\n",
      "Iteration 202, loss = 0.28444438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 0.28008155\n",
      "Iteration 253, loss = 0.27951919\n",
      "Iteration 254, loss = 0.27962317\n",
      "Iteration 255, loss = 0.27988080\n",
      "Iteration 256, loss = 0.27986827\n",
      "Iteration 257, loss = 0.27941192\n",
      "Iteration 258, loss = 0.27959394\n",
      "Iteration 259, loss = 0.27930321\n",
      "Iteration 260, loss = 0.27902233\n",
      "Iteration 261, loss = 0.27892441\n",
      "Iteration 262, loss = 0.27906118\n",
      "Iteration 263, loss = 0.27864037\n",
      "Iteration 264, loss = 0.27892579\n",
      "Iteration 265, loss = 0.27844614\n",
      "Iteration 266, loss = 0.27813151\n",
      "Iteration 267, loss = 0.27845934\n",
      "Iteration 268, loss = 0.27793719\n",
      "Iteration 269, loss = 0.27772792\n",
      "Iteration 270, loss = 0.27773731\n",
      "Iteration 271, loss = 0.27755737\n",
      "Iteration 272, loss = 0.27801606\n",
      "Iteration 273, loss = 0.27722057\n",
      "Iteration 274, loss = 0.27698081\n",
      "Iteration 275, loss = 0.27670038\n",
      "Iteration 276, loss = 0.27663265\n",
      "Iteration 277, loss = 0.27669203\n",
      "Iteration 278, loss = 0.27639759\n",
      "Iteration 279, loss = 0.27670682\n",
      "Iteration 280, loss = 0.27655187\n",
      "Iteration 281, loss = 0.27539968\n",
      "Iteration 282, loss = 0.27540874\n",
      "Iteration 283, loss = 0.27546993\n",
      "Iteration 284, loss = 0.27547663\n",
      "Iteration 285, loss = 0.27583302\n",
      "Iteration 286, loss = 0.27502096\n",
      "Iteration 287, loss = 0.27444990\n",
      "Iteration 288, loss = 0.27506202\n",
      "Iteration 289, loss = 0.27392363\n",
      "Iteration 290, loss = 0.27478634\n",
      "Iteration 291, loss = 0.27431464\n",
      "Iteration 292, loss = 0.27315277\n",
      "Iteration 293, loss = 0.27400068\n",
      "Iteration 294, loss = 0.27378367\n",
      "Iteration 295, loss = 0.27356624\n",
      "Iteration 296, loss = 0.27345328\n",
      "Iteration 297, loss = 0.27279013\n",
      "Iteration 298, loss = 0.27223314\n",
      "Iteration 299, loss = 0.27261181\n",
      "Iteration 300, loss = 0.27252086\n",
      "Iteration 1, loss = 0.62128844\n",
      "Iteration 2, loss = 0.52434898\n",
      "Iteration 3, loss = 0.43825416\n",
      "Iteration 4, loss = 0.39219033\n",
      "Iteration 5, loss = 0.37498223\n",
      "Iteration 6, loss = 0.36706631\n",
      "Iteration 7, loss = 0.36224841\n",
      "Iteration 8, loss = 0.35914364\n",
      "Iteration 9, loss = 0.35705987\n",
      "Iteration 10, loss = 0.35530711\n",
      "Iteration 11, loss = 0.35299400\n",
      "Iteration 12, loss = 0.35189595\n",
      "Iteration 13, loss = 0.35037963\n",
      "Iteration 14, loss = 0.34929012\n",
      "Iteration 15, loss = 0.34802531\n",
      "Iteration 16, loss = 0.34694781\n",
      "Iteration 17, loss = 0.34620021\n",
      "Iteration 18, loss = 0.34491045\n",
      "Iteration 19, loss = 0.34405398\n",
      "Iteration 20, loss = 0.34282178\n",
      "Iteration 21, loss = 0.34211706\n",
      "Iteration 22, loss = 0.34135978\n",
      "Iteration 23, loss = 0.34011880\n",
      "Iteration 24, loss = 0.33980054\n",
      "Iteration 25, loss = 0.33892705\n",
      "Iteration 26, loss = 0.33782164\n",
      "Iteration 27, loss = 0.33693065\n",
      "Iteration 28, loss = 0.33662835\n",
      "Iteration 29, loss = 0.33525961\n",
      "Iteration 30, loss = 0.33516339\n",
      "Iteration 31, loss = 0.33462711\n",
      "Iteration 32, loss = 0.33317549\n",
      "Iteration 33, loss = 0.33264388\n",
      "Iteration 34, loss = 0.33209529\n",
      "Iteration 35, loss = 0.33150924\n",
      "Iteration 36, loss = 0.33092477\n",
      "Iteration 37, loss = 0.33039992\n",
      "Iteration 38, loss = 0.32970508\n",
      "Iteration 39, loss = 0.32914337\n",
      "Iteration 40, loss = 0.32841723\n",
      "Iteration 41, loss = 0.32802233\n",
      "Iteration 42, loss = 0.32747869\n",
      "Iteration 43, loss = 0.32694672\n",
      "Iteration 44, loss = 0.32639412\n",
      "Iteration 45, loss = 0.32530135\n",
      "Iteration 46, loss = 0.32458936\n",
      "Iteration 47, loss = 0.32493317\n",
      "Iteration 48, loss = 0.32372471\n",
      "Iteration 49, loss = 0.32326486\n",
      "Iteration 50, loss = 0.32265653\n",
      "Iteration 51, loss = 0.32260307\n",
      "Iteration 52, loss = 0.32181547\n",
      "Iteration 53, loss = 0.32073649\n",
      "Iteration 54, loss = 0.32059097\n",
      "Iteration 55, loss = 0.32018329\n",
      "Iteration 56, loss = 0.31910237\n",
      "Iteration 57, loss = 0.31940832\n",
      "Iteration 58, loss = 0.31830170\n",
      "Iteration 59, loss = 0.31827945\n",
      "Iteration 60, loss = 0.31775587\n",
      "Iteration 61, loss = 0.31692498\n",
      "Iteration 62, loss = 0.31634439\n",
      "Iteration 63, loss = 0.31615790\n",
      "Iteration 64, loss = 0.31600923\n",
      "Iteration 65, loss = 0.31513916\n",
      "Iteration 66, loss = 0.31488643\n",
      "Iteration 67, loss = 0.31426473\n",
      "Iteration 68, loss = 0.31387203\n",
      "Iteration 69, loss = 0.31341242\n",
      "Iteration 70, loss = 0.31369950\n",
      "Iteration 71, loss = 0.31249566\n",
      "Iteration 72, loss = 0.31241592\n",
      "Iteration 73, loss = 0.31212946\n",
      "Iteration 74, loss = 0.31132746\n",
      "Iteration 75, loss = 0.31076574\n",
      "Iteration 76, loss = 0.31128197\n",
      "Iteration 77, loss = 0.31005676\n",
      "Iteration 78, loss = 0.30993918\n",
      "Iteration 79, loss = 0.30931510\n",
      "Iteration 80, loss = 0.30920226\n",
      "Iteration 81, loss = 0.30887205\n",
      "Iteration 82, loss = 0.30867810\n",
      "Iteration 83, loss = 0.30825134\n",
      "Iteration 84, loss = 0.30808519\n",
      "Iteration 85, loss = 0.30774755\n",
      "Iteration 86, loss = 0.30723123\n",
      "Iteration 87, loss = 0.30677819\n",
      "Iteration 88, loss = 0.30633057\n",
      "Iteration 89, loss = 0.30632733\n",
      "Iteration 90, loss = 0.30623014\n",
      "Iteration 91, loss = 0.30548567\n",
      "Iteration 92, loss = 0.30597088\n",
      "Iteration 93, loss = 0.30530656\n",
      "Iteration 94, loss = 0.30471846\n",
      "Iteration 95, loss = 0.30474450\n",
      "Iteration 96, loss = 0.30379350\n",
      "Iteration 97, loss = 0.30371926\n",
      "Iteration 98, loss = 0.30361104\n",
      "Iteration 99, loss = 0.30333621\n",
      "Iteration 100, loss = 0.30299754\n",
      "Iteration 101, loss = 0.30264722\n",
      "Iteration 102, loss = 0.30250585\n",
      "Iteration 103, loss = 0.30235468\n",
      "Iteration 104, loss = 0.30198311\n",
      "Iteration 105, loss = 0.30117880\n",
      "Iteration 106, loss = 0.30146506\n",
      "Iteration 107, loss = 0.30151184\n",
      "Iteration 108, loss = 0.30176591\n",
      "Iteration 109, loss = 0.30066650\n",
      "Iteration 110, loss = 0.29996721\n",
      "Iteration 111, loss = 0.30005071\n",
      "Iteration 112, loss = 0.30004581\n",
      "Iteration 113, loss = 0.30009177\n",
      "Iteration 114, loss = 0.29942916\n",
      "Iteration 115, loss = 0.29953439\n",
      "Iteration 116, loss = 0.29872157\n",
      "Iteration 117, loss = 0.29944756\n",
      "Iteration 118, loss = 0.29862490\n",
      "Iteration 119, loss = 0.29853296\n",
      "Iteration 120, loss = 0.29827158\n",
      "Iteration 121, loss = 0.29831097\n",
      "Iteration 122, loss = 0.29787788\n",
      "Iteration 123, loss = 0.29776326\n",
      "Iteration 124, loss = 0.29787136\n",
      "Iteration 125, loss = 0.29737693\n",
      "Iteration 126, loss = 0.29682673\n",
      "Iteration 127, loss = 0.29682897\n",
      "Iteration 128, loss = 0.29676032\n",
      "Iteration 129, loss = 0.29633748\n",
      "Iteration 130, loss = 0.29608134\n",
      "Iteration 131, loss = 0.29665383\n",
      "Iteration 132, loss = 0.29562249\n",
      "Iteration 133, loss = 0.29582109\n",
      "Iteration 134, loss = 0.29536083\n",
      "Iteration 135, loss = 0.29484436\n",
      "Iteration 136, loss = 0.29536558\n",
      "Iteration 137, loss = 0.29568345\n",
      "Iteration 138, loss = 0.29454818\n",
      "Iteration 139, loss = 0.29460688\n",
      "Iteration 140, loss = 0.29377684\n",
      "Iteration 141, loss = 0.29394910\n",
      "Iteration 142, loss = 0.29457716\n",
      "Iteration 143, loss = 0.29372483\n",
      "Iteration 144, loss = 0.29334186\n",
      "Iteration 145, loss = 0.29350848\n",
      "Iteration 146, loss = 0.29384068\n",
      "Iteration 147, loss = 0.29342100\n",
      "Iteration 148, loss = 0.29319818\n",
      "Iteration 149, loss = 0.29307567\n",
      "Iteration 150, loss = 0.29288304\n",
      "Iteration 151, loss = 0.29261625\n",
      "Iteration 152, loss = 0.29218104\n",
      "Iteration 153, loss = 0.29277430\n",
      "Iteration 154, loss = 0.29255116\n",
      "Iteration 155, loss = 0.29266805\n",
      "Iteration 156, loss = 0.29171362\n",
      "Iteration 157, loss = 0.29173504\n",
      "Iteration 158, loss = 0.29169405\n",
      "Iteration 159, loss = 0.29083848\n",
      "Iteration 160, loss = 0.29136416\n",
      "Iteration 161, loss = 0.29143158\n",
      "Iteration 162, loss = 0.29105505\n",
      "Iteration 163, loss = 0.29057818\n",
      "Iteration 164, loss = 0.29075169\n",
      "Iteration 165, loss = 0.29035454\n",
      "Iteration 166, loss = 0.29075485\n",
      "Iteration 167, loss = 0.29059606\n",
      "Iteration 168, loss = 0.29023651\n",
      "Iteration 169, loss = 0.29017302\n",
      "Iteration 170, loss = 0.29034156\n",
      "Iteration 171, loss = 0.28975730\n",
      "Iteration 172, loss = 0.29018285\n",
      "Iteration 173, loss = 0.28930924\n",
      "Iteration 174, loss = 0.28970521\n",
      "Iteration 175, loss = 0.28938624\n",
      "Iteration 176, loss = 0.28922133\n",
      "Iteration 177, loss = 0.28925072\n",
      "Iteration 178, loss = 0.28917158\n",
      "Iteration 179, loss = 0.28857407\n",
      "Iteration 180, loss = 0.28852516\n",
      "Iteration 181, loss = 0.28825766\n",
      "Iteration 182, loss = 0.28866998\n",
      "Iteration 183, loss = 0.28838869\n",
      "Iteration 184, loss = 0.28854411\n",
      "Iteration 185, loss = 0.28822491\n",
      "Iteration 186, loss = 0.28789968\n",
      "Iteration 187, loss = 0.28754797\n",
      "Iteration 188, loss = 0.28794588\n",
      "Iteration 189, loss = 0.28720801\n",
      "Iteration 190, loss = 0.28716582\n",
      "Iteration 191, loss = 0.28734659\n",
      "Iteration 192, loss = 0.28648424\n",
      "Iteration 193, loss = 0.28652390\n",
      "Iteration 194, loss = 0.28683186\n",
      "Iteration 195, loss = 0.28639429\n",
      "Iteration 196, loss = 0.28711548\n",
      "Iteration 197, loss = 0.28615459\n",
      "Iteration 198, loss = 0.28630454\n",
      "Iteration 199, loss = 0.28611392\n",
      "Iteration 200, loss = 0.28606559\n",
      "Iteration 201, loss = 0.28624403\n",
      "Iteration 202, loss = 0.28522178\n",
      "Iteration 252, loss = 0.28047453\n",
      "Iteration 253, loss = 0.27996641\n",
      "Iteration 254, loss = 0.28021527\n",
      "Iteration 255, loss = 0.28016095\n",
      "Iteration 256, loss = 0.28016401\n",
      "Iteration 257, loss = 0.27937514\n",
      "Iteration 258, loss = 0.28036741\n",
      "Iteration 259, loss = 0.27917173\n",
      "Iteration 260, loss = 0.27974662\n",
      "Iteration 261, loss = 0.27928230\n",
      "Iteration 262, loss = 0.27982464\n",
      "Iteration 263, loss = 0.27906407\n",
      "Iteration 264, loss = 0.27885847\n",
      "Iteration 265, loss = 0.27873654\n",
      "Iteration 266, loss = 0.27845393\n",
      "Iteration 267, loss = 0.27894251\n",
      "Iteration 268, loss = 0.27858428\n",
      "Iteration 269, loss = 0.27851471\n",
      "Iteration 270, loss = 0.27818740\n",
      "Iteration 271, loss = 0.27829624\n",
      "Iteration 272, loss = 0.27854907\n",
      "Iteration 273, loss = 0.27855795\n",
      "Iteration 274, loss = 0.27756230\n",
      "Iteration 275, loss = 0.27669075\n",
      "Iteration 276, loss = 0.27738608\n",
      "Iteration 277, loss = 0.27777019\n",
      "Iteration 278, loss = 0.27648228\n",
      "Iteration 279, loss = 0.27776725\n",
      "Iteration 280, loss = 0.27710117\n",
      "Iteration 281, loss = 0.27646358\n",
      "Iteration 282, loss = 0.27611166\n",
      "Iteration 283, loss = 0.27617246\n",
      "Iteration 284, loss = 0.27647883\n",
      "Iteration 285, loss = 0.27566358\n",
      "Iteration 286, loss = 0.27590763\n",
      "Iteration 287, loss = 0.27600666\n",
      "Iteration 288, loss = 0.27563568\n",
      "Iteration 289, loss = 0.27554700\n",
      "Iteration 290, loss = 0.27497344\n",
      "Iteration 291, loss = 0.27521867\n",
      "Iteration 292, loss = 0.27470176\n",
      "Iteration 293, loss = 0.27473904\n",
      "Iteration 294, loss = 0.27468755\n",
      "Iteration 295, loss = 0.27437720\n",
      "Iteration 296, loss = 0.27443004\n",
      "Iteration 297, loss = 0.27321149\n",
      "Iteration 298, loss = 0.27384457\n",
      "Iteration 299, loss = 0.27392038\n",
      "Iteration 300, loss = 0.27392114\n",
      "Iteration 1, loss = 0.62134368\n",
      "Iteration 2, loss = 0.52424977\n",
      "Iteration 3, loss = 0.43825390\n",
      "Iteration 4, loss = 0.39271517\n",
      "Iteration 5, loss = 0.37542648\n",
      "Iteration 6, loss = 0.36733392\n",
      "Iteration 7, loss = 0.36253933\n",
      "Iteration 8, loss = 0.35944747\n",
      "Iteration 9, loss = 0.35741122\n",
      "Iteration 10, loss = 0.35524609\n",
      "Iteration 11, loss = 0.35318117\n",
      "Iteration 12, loss = 0.35178371\n",
      "Iteration 13, loss = 0.35088826\n",
      "Iteration 14, loss = 0.35008101\n",
      "Iteration 15, loss = 0.34821844\n",
      "Iteration 16, loss = 0.34702224\n",
      "Iteration 17, loss = 0.34576944\n",
      "Iteration 18, loss = 0.34496775\n",
      "Iteration 19, loss = 0.34378409\n",
      "Iteration 20, loss = 0.34269395\n",
      "Iteration 21, loss = 0.34239903\n",
      "Iteration 22, loss = 0.34089089\n",
      "Iteration 23, loss = 0.34010091\n",
      "Iteration 24, loss = 0.33952713\n",
      "Iteration 25, loss = 0.33880236\n",
      "Iteration 26, loss = 0.33828706\n",
      "Iteration 27, loss = 0.33685993\n",
      "Iteration 28, loss = 0.33659556\n",
      "Iteration 29, loss = 0.33535443\n",
      "Iteration 30, loss = 0.33463638\n",
      "Iteration 31, loss = 0.33459480\n",
      "Iteration 32, loss = 0.33323776\n",
      "Iteration 33, loss = 0.33296416\n",
      "Iteration 34, loss = 0.33185878\n",
      "Iteration 35, loss = 0.33154047\n",
      "Iteration 36, loss = 0.33054715\n",
      "Iteration 37, loss = 0.33025678\n",
      "Iteration 38, loss = 0.32980372\n",
      "Iteration 39, loss = 0.32892637\n",
      "Iteration 40, loss = 0.32811033\n",
      "Iteration 41, loss = 0.32797348\n",
      "Iteration 42, loss = 0.32720743\n",
      "Iteration 43, loss = 0.32730413\n",
      "Iteration 44, loss = 0.32609653\n",
      "Iteration 45, loss = 0.32564314\n",
      "Iteration 46, loss = 0.32482061\n",
      "Iteration 47, loss = 0.32476048\n",
      "Iteration 48, loss = 0.32409960\n",
      "Iteration 49, loss = 0.32351134\n",
      "Iteration 50, loss = 0.32302700\n",
      "Iteration 51, loss = 0.32237780\n",
      "Iteration 52, loss = 0.32198298\n",
      "Iteration 53, loss = 0.32104761\n",
      "Iteration 54, loss = 0.32045279\n",
      "Iteration 55, loss = 0.32079335\n",
      "Iteration 56, loss = 0.31969392\n",
      "Iteration 57, loss = 0.31910343\n",
      "Iteration 58, loss = 0.31876443\n",
      "Iteration 59, loss = 0.31871310\n",
      "Iteration 60, loss = 0.31752167\n",
      "Iteration 61, loss = 0.31732487\n",
      "Iteration 62, loss = 0.31694016\n",
      "Iteration 63, loss = 0.31644295\n",
      "Iteration 64, loss = 0.31645338\n",
      "Iteration 65, loss = 0.31550827\n",
      "Iteration 66, loss = 0.31512816\n",
      "Iteration 67, loss = 0.31455172\n",
      "Iteration 68, loss = 0.31434720\n",
      "Iteration 69, loss = 0.31413353\n",
      "Iteration 70, loss = 0.31351641\n",
      "Iteration 71, loss = 0.31310094\n",
      "Iteration 72, loss = 0.31272416\n",
      "Iteration 73, loss = 0.31203532\n",
      "Iteration 74, loss = 0.31190194\n",
      "Iteration 75, loss = 0.31146338\n",
      "Iteration 76, loss = 0.31129825\n",
      "Iteration 77, loss = 0.31066697\n",
      "Iteration 78, loss = 0.31075881\n",
      "Iteration 79, loss = 0.31038178\n",
      "Iteration 80, loss = 0.30983342\n",
      "Iteration 81, loss = 0.30939032\n",
      "Iteration 82, loss = 0.30924553\n",
      "Iteration 83, loss = 0.30881035\n",
      "Iteration 84, loss = 0.30864497\n",
      "Iteration 85, loss = 0.30834894\n",
      "Iteration 86, loss = 0.30811707\n",
      "Iteration 87, loss = 0.30783303\n",
      "Iteration 88, loss = 0.30776580\n",
      "Iteration 89, loss = 0.30763639\n",
      "Iteration 90, loss = 0.30662990\n",
      "Iteration 91, loss = 0.30654484\n",
      "Iteration 92, loss = 0.30686139\n",
      "Iteration 93, loss = 0.30602141\n",
      "Iteration 94, loss = 0.30530534\n",
      "Iteration 95, loss = 0.30564788\n",
      "Iteration 96, loss = 0.30501211\n",
      "Iteration 97, loss = 0.30431501\n",
      "Iteration 98, loss = 0.30499454\n",
      "Iteration 99, loss = 0.30397760\n",
      "Iteration 100, loss = 0.30407133\n",
      "Iteration 101, loss = 0.30377614\n",
      "Iteration 102, loss = 0.30328356\n",
      "Iteration 103, loss = 0.30316756\n",
      "Iteration 104, loss = 0.30285756\n",
      "Iteration 105, loss = 0.30255628\n",
      "Iteration 106, loss = 0.30245854\n",
      "Iteration 107, loss = 0.30203180\n",
      "Iteration 108, loss = 0.30204636\n",
      "Iteration 109, loss = 0.30144273\n",
      "Iteration 110, loss = 0.30104721\n",
      "Iteration 111, loss = 0.30089593\n",
      "Iteration 112, loss = 0.30121160\n",
      "Iteration 113, loss = 0.30095213\n",
      "Iteration 114, loss = 0.30028914\n",
      "Iteration 115, loss = 0.30019979\n",
      "Iteration 116, loss = 0.29918345\n",
      "Iteration 117, loss = 0.29974916\n",
      "Iteration 118, loss = 0.29913079\n",
      "Iteration 119, loss = 0.29899333\n",
      "Iteration 120, loss = 0.29903145\n",
      "Iteration 121, loss = 0.29840517\n",
      "Iteration 122, loss = 0.29866219\n",
      "Iteration 123, loss = 0.29857547\n",
      "Iteration 124, loss = 0.29794698\n",
      "Iteration 125, loss = 0.29773362\n",
      "Iteration 126, loss = 0.29831213\n",
      "Iteration 127, loss = 0.29724558\n",
      "Iteration 128, loss = 0.29759414\n",
      "Iteration 129, loss = 0.29662740\n",
      "Iteration 130, loss = 0.29676657\n",
      "Iteration 131, loss = 0.29688445\n",
      "Iteration 132, loss = 0.29647433\n",
      "Iteration 133, loss = 0.29617400\n",
      "Iteration 134, loss = 0.29635895\n",
      "Iteration 135, loss = 0.29589852\n",
      "Iteration 136, loss = 0.29636486\n",
      "Iteration 137, loss = 0.29607702\n",
      "Iteration 138, loss = 0.29511414\n",
      "Iteration 139, loss = 0.29538992\n",
      "Iteration 140, loss = 0.29503068\n",
      "Iteration 141, loss = 0.29502740\n",
      "Iteration 142, loss = 0.29473426\n",
      "Iteration 143, loss = 0.29432508\n",
      "Iteration 144, loss = 0.29464322\n",
      "Iteration 145, loss = 0.29492971\n",
      "Iteration 146, loss = 0.29385158\n",
      "Iteration 147, loss = 0.29379897\n",
      "Iteration 148, loss = 0.29404685\n",
      "Iteration 149, loss = 0.29375890\n",
      "Iteration 150, loss = 0.29326909\n",
      "Iteration 151, loss = 0.29353624\n",
      "Iteration 152, loss = 0.29360242\n",
      "Iteration 153, loss = 0.29292793\n",
      "Iteration 154, loss = 0.29264074\n",
      "Iteration 155, loss = 0.29307105\n",
      "Iteration 156, loss = 0.29205268\n",
      "Iteration 157, loss = 0.29239652\n",
      "Iteration 158, loss = 0.29283308\n",
      "Iteration 159, loss = 0.29193582\n",
      "Iteration 160, loss = 0.29251086\n",
      "Iteration 161, loss = 0.29219769\n",
      "Iteration 162, loss = 0.29148623\n",
      "Iteration 163, loss = 0.29151973\n",
      "Iteration 164, loss = 0.29150548\n",
      "Iteration 165, loss = 0.29115609\n",
      "Iteration 166, loss = 0.29129272\n",
      "Iteration 167, loss = 0.29103224\n",
      "Iteration 168, loss = 0.29111359\n",
      "Iteration 169, loss = 0.29078996\n",
      "Iteration 170, loss = 0.29048737\n",
      "Iteration 171, loss = 0.29082584\n",
      "Iteration 172, loss = 0.29044760\n",
      "Iteration 173, loss = 0.28989125\n",
      "Iteration 174, loss = 0.28988474\n",
      "Iteration 175, loss = 0.28967253\n",
      "Iteration 176, loss = 0.28973314\n",
      "Iteration 177, loss = 0.28991560\n",
      "Iteration 178, loss = 0.28965128\n",
      "Iteration 179, loss = 0.28869220\n",
      "Iteration 180, loss = 0.28885326\n",
      "Iteration 181, loss = 0.28899343\n",
      "Iteration 182, loss = 0.28925476\n",
      "Iteration 183, loss = 0.28890554\n",
      "Iteration 184, loss = 0.28889034\n",
      "Iteration 185, loss = 0.28844069\n",
      "Iteration 186, loss = 0.28902243\n",
      "Iteration 187, loss = 0.28802326\n",
      "Iteration 188, loss = 0.28788309\n",
      "Iteration 189, loss = 0.28765644\n",
      "Iteration 190, loss = 0.28760908\n",
      "Iteration 191, loss = 0.28763704\n",
      "Iteration 192, loss = 0.28764389\n",
      "Iteration 193, loss = 0.28803771\n",
      "Iteration 194, loss = 0.28718269\n",
      "Iteration 195, loss = 0.28722226\n",
      "Iteration 196, loss = 0.28656476\n",
      "Iteration 197, loss = 0.28668777\n",
      "Iteration 198, loss = 0.28668657\n",
      "Iteration 199, loss = 0.28757782\n",
      "Iteration 200, loss = 0.28694638\n",
      "Iteration 201, loss = 0.28672879\n",
      "Iteration 202, loss = 0.28594756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 138, loss = 0.29454818\n",
      "Iteration 139, loss = 0.29460688\n",
      "Iteration 140, loss = 0.29377684\n",
      "Iteration 141, loss = 0.29394910\n",
      "Iteration 142, loss = 0.29457716\n",
      "Iteration 143, loss = 0.29372483\n",
      "Iteration 144, loss = 0.29334186\n",
      "Iteration 145, loss = 0.29350848\n",
      "Iteration 146, loss = 0.29384068\n",
      "Iteration 147, loss = 0.29342100\n",
      "Iteration 148, loss = 0.29319818\n",
      "Iteration 149, loss = 0.29307567\n",
      "Iteration 150, loss = 0.29288304\n",
      "Iteration 151, loss = 0.29261625\n",
      "Iteration 152, loss = 0.29218104\n",
      "Iteration 153, loss = 0.29277430\n",
      "Iteration 154, loss = 0.29255116\n",
      "Iteration 155, loss = 0.29266805\n",
      "Iteration 156, loss = 0.29171362\n",
      "Iteration 157, loss = 0.29173504\n",
      "Iteration 158, loss = 0.29169405\n",
      "Iteration 159, loss = 0.29083848\n",
      "Iteration 160, loss = 0.29136416\n",
      "Iteration 161, loss = 0.29143158\n",
      "Iteration 162, loss = 0.29105505\n",
      "Iteration 163, loss = 0.29057818\n",
      "Iteration 164, loss = 0.29075169\n",
      "Iteration 165, loss = 0.29035454\n",
      "Iteration 166, loss = 0.29075485\n",
      "Iteration 167, loss = 0.29059606\n",
      "Iteration 168, loss = 0.29023651\n",
      "Iteration 169, loss = 0.29017302\n",
      "Iteration 170, loss = 0.29034156\n",
      "Iteration 171, loss = 0.28975730\n",
      "Iteration 172, loss = 0.29018285\n",
      "Iteration 173, loss = 0.28930924\n",
      "Iteration 174, loss = 0.28970521\n",
      "Iteration 175, loss = 0.28938624\n",
      "Iteration 176, loss = 0.28922133\n",
      "Iteration 177, loss = 0.28925072\n",
      "Iteration 178, loss = 0.28917158\n",
      "Iteration 179, loss = 0.28857407\n",
      "Iteration 180, loss = 0.28852516\n",
      "Iteration 181, loss = 0.28825766\n",
      "Iteration 182, loss = 0.28866998\n",
      "Iteration 183, loss = 0.28838869\n",
      "Iteration 184, loss = 0.28854411\n",
      "Iteration 185, loss = 0.28822491\n",
      "Iteration 186, loss = 0.28789968\n",
      "Iteration 187, loss = 0.28754797\n",
      "Iteration 188, loss = 0.28794588\n",
      "Iteration 189, loss = 0.28720801\n",
      "Iteration 190, loss = 0.28716582\n",
      "Iteration 191, loss = 0.28734659\n",
      "Iteration 192, loss = 0.28648424\n",
      "Iteration 193, loss = 0.28652390\n",
      "Iteration 194, loss = 0.28683186\n",
      "Iteration 195, loss = 0.28639429\n",
      "Iteration 196, loss = 0.28711548\n",
      "Iteration 197, loss = 0.28615459\n",
      "Iteration 198, loss = 0.28630454\n",
      "Iteration 199, loss = 0.28611392\n",
      "Iteration 200, loss = 0.28606559\n",
      "Iteration 201, loss = 0.28624403\n",
      "Iteration 202, loss = 0.28522178\n",
      "Iteration 203, loss = 0.28568983\n",
      "Iteration 204, loss = 0.28557908\n",
      "Iteration 205, loss = 0.28568102\n",
      "Iteration 206, loss = 0.28558166\n",
      "Iteration 207, loss = 0.28505295\n",
      "Iteration 208, loss = 0.28508960\n",
      "Iteration 209, loss = 0.28541557\n",
      "Iteration 210, loss = 0.28456470\n",
      "Iteration 211, loss = 0.28518828\n",
      "Iteration 212, loss = 0.28438589\n",
      "Iteration 213, loss = 0.28429901\n",
      "Iteration 214, loss = 0.28514004\n",
      "Iteration 215, loss = 0.28407700\n",
      "Iteration 216, loss = 0.28414541\n",
      "Iteration 217, loss = 0.28399565\n",
      "Iteration 218, loss = 0.28363770\n",
      "Iteration 219, loss = 0.28424249\n",
      "Iteration 220, loss = 0.28377655\n",
      "Iteration 221, loss = 0.28384546\n",
      "Iteration 222, loss = 0.28341574\n",
      "Iteration 223, loss = 0.28333240\n",
      "Iteration 224, loss = 0.28341570\n",
      "Iteration 225, loss = 0.28325293\n",
      "Iteration 226, loss = 0.28228659\n",
      "Iteration 227, loss = 0.28307940\n",
      "Iteration 228, loss = 0.28264468\n",
      "Iteration 229, loss = 0.28294203\n",
      "Iteration 230, loss = 0.28261985\n",
      "Iteration 231, loss = 0.28200832\n",
      "Iteration 232, loss = 0.28235912\n",
      "Iteration 233, loss = 0.28154444\n",
      "Iteration 234, loss = 0.28197103\n",
      "Iteration 235, loss = 0.28248613\n",
      "Iteration 236, loss = 0.28128911\n",
      "Iteration 237, loss = 0.28158837\n",
      "Iteration 238, loss = 0.28135420\n",
      "Iteration 239, loss = 0.28206391\n",
      "Iteration 240, loss = 0.28109213\n",
      "Iteration 241, loss = 0.28092116\n",
      "Iteration 242, loss = 0.28134101\n",
      "Iteration 243, loss = 0.28101644\n",
      "Iteration 244, loss = 0.28037925\n",
      "Iteration 245, loss = 0.28059278\n",
      "Iteration 246, loss = 0.28026935\n",
      "Iteration 247, loss = 0.27981644\n",
      "Iteration 248, loss = 0.28020407\n",
      "Iteration 249, loss = 0.27972450\n",
      "Iteration 250, loss = 0.27959088\n",
      "Iteration 251, loss = 0.27963602\n",
      "Iteration 252, loss = 0.27901136\n",
      "Iteration 253, loss = 0.27943363\n",
      "Iteration 254, loss = 0.27861262\n",
      "Iteration 255, loss = 0.27832728\n",
      "Iteration 256, loss = 0.27872177\n",
      "Iteration 257, loss = 0.27873017\n",
      "Iteration 258, loss = 0.27789838\n",
      "Iteration 259, loss = 0.27807566\n",
      "Iteration 260, loss = 0.27791419\n",
      "Iteration 261, loss = 0.27770916\n",
      "Iteration 262, loss = 0.27733863\n",
      "Iteration 263, loss = 0.27772379\n",
      "Iteration 264, loss = 0.27700412\n",
      "Iteration 265, loss = 0.27644859\n",
      "Iteration 266, loss = 0.27713033\n",
      "Iteration 267, loss = 0.27608766\n",
      "Iteration 268, loss = 0.27614753\n",
      "Iteration 269, loss = 0.27593832\n",
      "Iteration 270, loss = 0.27542772\n",
      "Iteration 271, loss = 0.27532408\n",
      "Iteration 272, loss = 0.27528927\n",
      "Iteration 273, loss = 0.27546467\n",
      "Iteration 274, loss = 0.27474966\n",
      "Iteration 275, loss = 0.27503230\n",
      "Iteration 276, loss = 0.27467860\n",
      "Iteration 277, loss = 0.27431476\n",
      "Iteration 278, loss = 0.27428374\n",
      "Iteration 279, loss = 0.27406389\n",
      "Iteration 280, loss = 0.27404357\n",
      "Iteration 281, loss = 0.27346276\n",
      "Iteration 282, loss = 0.27361864\n",
      "Iteration 283, loss = 0.27357986\n",
      "Iteration 284, loss = 0.27303209\n",
      "Iteration 285, loss = 0.27306597\n",
      "Iteration 286, loss = 0.27267500\n",
      "Iteration 287, loss = 0.27289374\n",
      "Iteration 288, loss = 0.27246662\n",
      "Iteration 289, loss = 0.27241634\n",
      "Iteration 290, loss = 0.27208101\n",
      "Iteration 291, loss = 0.27239308\n",
      "Iteration 292, loss = 0.27230004\n",
      "Iteration 293, loss = 0.27147106\n",
      "Iteration 294, loss = 0.27117665\n",
      "Iteration 295, loss = 0.27172154\n",
      "Iteration 296, loss = 0.27151793\n",
      "Iteration 297, loss = 0.27131426\n",
      "Iteration 298, loss = 0.27168694\n",
      "Iteration 299, loss = 0.27092979\n",
      "Iteration 300, loss = 0.27078061\n",
      "Iteration 1, loss = 0.65656495\n",
      "Iteration 2, loss = 0.60842144\n",
      "Iteration 3, loss = 0.58519548\n",
      "Iteration 4, loss = 0.56456126\n",
      "Iteration 5, loss = 0.54511057\n",
      "Iteration 6, loss = 0.52697450\n",
      "Iteration 7, loss = 0.51036428\n",
      "Iteration 8, loss = 0.49563100\n",
      "Iteration 9, loss = 0.48262876\n",
      "Iteration 10, loss = 0.47114818\n",
      "Iteration 11, loss = 0.46107498\n",
      "Iteration 12, loss = 0.45221502\n",
      "Iteration 13, loss = 0.44427313\n",
      "Iteration 14, loss = 0.43734648\n",
      "Iteration 15, loss = 0.43108577\n",
      "Iteration 16, loss = 0.42549043\n",
      "Iteration 17, loss = 0.42050739\n",
      "Iteration 18, loss = 0.41607235\n",
      "Iteration 19, loss = 0.41207895\n",
      "Iteration 20, loss = 0.40838445\n",
      "Iteration 21, loss = 0.40509279\n",
      "Iteration 22, loss = 0.40210874\n",
      "Iteration 23, loss = 0.39937950\n",
      "Iteration 24, loss = 0.39687354\n",
      "Iteration 25, loss = 0.39454310\n",
      "Iteration 26, loss = 0.39225546\n",
      "Iteration 27, loss = 0.39030458\n",
      "Iteration 28, loss = 0.38847236\n",
      "Iteration 29, loss = 0.38676187\n",
      "Iteration 30, loss = 0.38500983\n",
      "Iteration 31, loss = 0.38347229\n",
      "Iteration 32, loss = 0.38195948\n",
      "Iteration 33, loss = 0.38061721\n",
      "Iteration 34, loss = 0.37928550\n",
      "Iteration 35, loss = 0.37801404\n",
      "Iteration 36, loss = 0.37698455\n",
      "Iteration 37, loss = 0.37576888\n",
      "Iteration 38, loss = 0.37486549\n",
      "Iteration 39, loss = 0.37356515\n",
      "Iteration 40, loss = 0.37273462\n",
      "Iteration 41, loss = 0.37179421\n",
      "Iteration 42, loss = 0.37082010\n",
      "Iteration 43, loss = 0.36999171\n",
      "Iteration 44, loss = 0.36914838\n",
      "Iteration 45, loss = 0.36837548\n",
      "Iteration 46, loss = 0.36752211\n",
      "Iteration 47, loss = 0.36681171\n",
      "Iteration 48, loss = 0.36611764\n",
      "Iteration 49, loss = 0.36542526\n",
      "Iteration 50, loss = 0.36480865\n",
      "Iteration 51, loss = 0.36398927\n",
      "Iteration 52, loss = 0.36348061\n",
      "Iteration 53, loss = 0.36286854\n",
      "Iteration 54, loss = 0.36224537\n",
      "Iteration 55, loss = 0.36173100\n",
      "Iteration 56, loss = 0.36107468\n",
      "Iteration 57, loss = 0.36065310\n",
      "Iteration 58, loss = 0.36007287\n",
      "Iteration 59, loss = 0.35949117\n",
      "Iteration 60, loss = 0.35914970\n",
      "Iteration 61, loss = 0.35862854\n",
      "Iteration 62, loss = 0.35824118\n",
      "Iteration 63, loss = 0.35774946\n",
      "Iteration 64, loss = 0.35718921\n",
      "Iteration 65, loss = 0.35687964\n",
      "Iteration 66, loss = 0.35646499\n",
      "Iteration 67, loss = 0.35605096\n",
      "Iteration 68, loss = 0.35579368\n",
      "Iteration 69, loss = 0.35544948\n",
      "Iteration 70, loss = 0.35499204\n",
      "Iteration 71, loss = 0.35451650\n",
      "Iteration 72, loss = 0.35421020\n",
      "Iteration 73, loss = 0.35392055\n",
      "Iteration 74, loss = 0.35366679\n",
      "Iteration 75, loss = 0.35331049\n",
      "Iteration 76, loss = 0.35291676\n",
      "Iteration 77, loss = 0.35266880\n",
      "Iteration 78, loss = 0.35230314\n",
      "Iteration 79, loss = 0.35206506\n",
      "Iteration 80, loss = 0.35176095\n",
      "Iteration 81, loss = 0.35154382\n",
      "Iteration 82, loss = 0.35134587\n",
      "Iteration 83, loss = 0.35096707\n",
      "Iteration 84, loss = 0.35062641\n",
      "Iteration 85, loss = 0.35040770\n",
      "Iteration 86, loss = 0.35015112\n",
      "Iteration 87, loss = 0.34986764\n",
      "Iteration 88, loss = 0.34974096\n",
      "Iteration 138, loss = 0.29602708\n",
      "Iteration 139, loss = 0.29574175\n",
      "Iteration 140, loss = 0.29507921\n",
      "Iteration 141, loss = 0.29501095\n",
      "Iteration 142, loss = 0.29506165\n",
      "Iteration 143, loss = 0.29479387\n",
      "Iteration 144, loss = 0.29411851\n",
      "Iteration 145, loss = 0.29515876\n",
      "Iteration 146, loss = 0.29412188\n",
      "Iteration 147, loss = 0.29394962\n",
      "Iteration 148, loss = 0.29436458\n",
      "Iteration 149, loss = 0.29383435\n",
      "Iteration 150, loss = 0.29367052\n",
      "Iteration 151, loss = 0.29370371\n",
      "Iteration 152, loss = 0.29347699\n",
      "Iteration 153, loss = 0.29367231\n",
      "Iteration 154, loss = 0.29330478\n",
      "Iteration 155, loss = 0.29309474\n",
      "Iteration 156, loss = 0.29262410\n",
      "Iteration 157, loss = 0.29302137\n",
      "Iteration 158, loss = 0.29280335\n",
      "Iteration 159, loss = 0.29214711\n",
      "Iteration 160, loss = 0.29236827\n",
      "Iteration 161, loss = 0.29212204\n",
      "Iteration 162, loss = 0.29145076\n",
      "Iteration 163, loss = 0.29133511\n",
      "Iteration 164, loss = 0.29104614\n",
      "Iteration 165, loss = 0.29129777\n",
      "Iteration 166, loss = 0.29149907\n",
      "Iteration 167, loss = 0.29123086\n",
      "Iteration 168, loss = 0.29106663\n",
      "Iteration 169, loss = 0.29019072\n",
      "Iteration 170, loss = 0.29076772\n",
      "Iteration 171, loss = 0.29090732\n",
      "Iteration 172, loss = 0.29059578\n",
      "Iteration 173, loss = 0.29033869\n",
      "Iteration 174, loss = 0.28990837\n",
      "Iteration 175, loss = 0.28960415\n",
      "Iteration 176, loss = 0.28988931\n",
      "Iteration 177, loss = 0.28963066\n",
      "Iteration 178, loss = 0.28939584\n",
      "Iteration 179, loss = 0.28895367\n",
      "Iteration 180, loss = 0.28913708\n",
      "Iteration 181, loss = 0.28906593\n",
      "Iteration 182, loss = 0.28858595\n",
      "Iteration 183, loss = 0.28867599\n",
      "Iteration 184, loss = 0.28847202\n",
      "Iteration 185, loss = 0.28844898\n",
      "Iteration 186, loss = 0.28814984\n",
      "Iteration 187, loss = 0.28815773\n",
      "Iteration 188, loss = 0.28812394\n",
      "Iteration 189, loss = 0.28779182\n",
      "Iteration 190, loss = 0.28738617\n",
      "Iteration 191, loss = 0.28753822\n",
      "Iteration 192, loss = 0.28678860\n",
      "Iteration 193, loss = 0.28674606\n",
      "Iteration 194, loss = 0.28699639\n",
      "Iteration 195, loss = 0.28671122\n",
      "Iteration 196, loss = 0.28652408\n",
      "Iteration 197, loss = 0.28612587\n",
      "Iteration 198, loss = 0.28607266\n",
      "Iteration 199, loss = 0.28583261\n",
      "Iteration 200, loss = 0.28557939\n",
      "Iteration 201, loss = 0.28530425\n",
      "Iteration 202, loss = 0.28444438\n",
      "Iteration 203, loss = 0.28461229\n",
      "Iteration 204, loss = 0.28504472\n",
      "Iteration 205, loss = 0.28431171\n",
      "Iteration 206, loss = 0.28436814\n",
      "Iteration 207, loss = 0.28388300\n",
      "Iteration 208, loss = 0.28298956\n",
      "Iteration 209, loss = 0.28352673\n",
      "Iteration 210, loss = 0.28291749\n",
      "Iteration 211, loss = 0.28293124\n",
      "Iteration 212, loss = 0.28270069\n",
      "Iteration 213, loss = 0.28221385\n",
      "Iteration 214, loss = 0.28298933\n",
      "Iteration 215, loss = 0.28287486\n",
      "Iteration 216, loss = 0.28154980\n",
      "Iteration 217, loss = 0.28219842\n",
      "Iteration 218, loss = 0.28103699\n",
      "Iteration 219, loss = 0.28112627\n",
      "Iteration 220, loss = 0.28143140\n",
      "Iteration 221, loss = 0.28180902\n",
      "Iteration 222, loss = 0.28131174\n",
      "Iteration 223, loss = 0.28013636\n",
      "Iteration 224, loss = 0.28068974\n",
      "Iteration 225, loss = 0.28009743\n",
      "Iteration 226, loss = 0.28029419\n",
      "Iteration 227, loss = 0.27989620\n",
      "Iteration 228, loss = 0.27968412\n",
      "Iteration 229, loss = 0.28000507\n",
      "Iteration 230, loss = 0.27967265\n",
      "Iteration 231, loss = 0.27939672\n",
      "Iteration 232, loss = 0.27930152\n",
      "Iteration 233, loss = 0.27893107\n",
      "Iteration 234, loss = 0.27926467\n",
      "Iteration 235, loss = 0.27877272\n",
      "Iteration 236, loss = 0.27928089\n",
      "Iteration 237, loss = 0.27833763\n",
      "Iteration 238, loss = 0.27795230\n",
      "Iteration 239, loss = 0.27834177\n",
      "Iteration 240, loss = 0.27789603\n",
      "Iteration 241, loss = 0.27739414\n",
      "Iteration 242, loss = 0.27806151\n",
      "Iteration 243, loss = 0.27783480\n",
      "Iteration 244, loss = 0.27735096\n",
      "Iteration 245, loss = 0.27728638\n",
      "Iteration 246, loss = 0.27752020\n",
      "Iteration 247, loss = 0.27690621\n",
      "Iteration 248, loss = 0.27741559\n",
      "Iteration 249, loss = 0.27696314\n",
      "Iteration 250, loss = 0.27684694\n",
      "Iteration 251, loss = 0.27618499\n",
      "Iteration 252, loss = 0.27616642\n",
      "Iteration 253, loss = 0.27546263\n",
      "Iteration 254, loss = 0.27622885\n",
      "Iteration 255, loss = 0.27535573\n",
      "Iteration 256, loss = 0.27563556\n",
      "Iteration 257, loss = 0.27532811\n",
      "Iteration 258, loss = 0.27512172\n",
      "Iteration 259, loss = 0.27503191\n",
      "Iteration 260, loss = 0.27557463\n",
      "Iteration 261, loss = 0.27464133\n",
      "Iteration 262, loss = 0.27507075\n",
      "Iteration 263, loss = 0.27429068\n",
      "Iteration 264, loss = 0.27427948\n",
      "Iteration 265, loss = 0.27401112\n",
      "Iteration 266, loss = 0.27446598\n",
      "Iteration 267, loss = 0.27440215\n",
      "Iteration 268, loss = 0.27393737\n",
      "Iteration 269, loss = 0.27320110\n",
      "Iteration 270, loss = 0.27384575\n",
      "Iteration 271, loss = 0.27356074\n",
      "Iteration 272, loss = 0.27342472\n",
      "Iteration 273, loss = 0.27335144\n",
      "Iteration 274, loss = 0.27344725\n",
      "Iteration 275, loss = 0.27259567\n",
      "Iteration 276, loss = 0.27313280\n",
      "Iteration 277, loss = 0.27211281\n",
      "Iteration 278, loss = 0.27238876\n",
      "Iteration 279, loss = 0.27242425\n",
      "Iteration 280, loss = 0.27221529\n",
      "Iteration 281, loss = 0.27196196\n",
      "Iteration 282, loss = 0.27193214\n",
      "Iteration 283, loss = 0.27148003\n",
      "Iteration 284, loss = 0.27156449\n",
      "Iteration 285, loss = 0.27092992\n",
      "Iteration 286, loss = 0.27142525\n",
      "Iteration 287, loss = 0.27210385\n",
      "Iteration 288, loss = 0.27135992\n",
      "Iteration 289, loss = 0.27105593\n",
      "Iteration 290, loss = 0.27043444\n",
      "Iteration 291, loss = 0.27068282\n",
      "Iteration 292, loss = 0.27090551\n",
      "Iteration 293, loss = 0.27016023\n",
      "Iteration 294, loss = 0.27063234\n",
      "Iteration 295, loss = 0.26915619\n",
      "Iteration 296, loss = 0.27053527\n",
      "Iteration 297, loss = 0.26998761\n",
      "Iteration 298, loss = 0.27001114\n",
      "Iteration 299, loss = 0.27042152\n",
      "Iteration 300, loss = 0.26898728\n",
      "Iteration 1, loss = 0.49907723\n",
      "Iteration 2, loss = 0.39389975\n",
      "Iteration 3, loss = 0.37089623\n",
      "Iteration 4, loss = 0.36067761\n",
      "Iteration 5, loss = 0.35471433\n",
      "Iteration 6, loss = 0.35066348\n",
      "Iteration 7, loss = 0.34813629\n",
      "Iteration 8, loss = 0.34583776\n",
      "Iteration 9, loss = 0.34437897\n",
      "Iteration 10, loss = 0.34281927\n",
      "Iteration 11, loss = 0.34159006\n",
      "Iteration 12, loss = 0.34000788\n",
      "Iteration 13, loss = 0.33958026\n",
      "Iteration 14, loss = 0.33823726\n",
      "Iteration 15, loss = 0.33736750\n",
      "Iteration 16, loss = 0.33693670\n",
      "Iteration 17, loss = 0.33645277\n",
      "Iteration 18, loss = 0.33585598\n",
      "Iteration 19, loss = 0.33520450\n",
      "Iteration 20, loss = 0.33411723\n",
      "Iteration 21, loss = 0.33376305\n",
      "Iteration 22, loss = 0.33332379\n",
      "Iteration 23, loss = 0.33256370\n",
      "Iteration 24, loss = 0.33228491\n",
      "Iteration 25, loss = 0.33142207\n",
      "Iteration 26, loss = 0.33051522\n",
      "Iteration 27, loss = 0.32974568\n",
      "Iteration 28, loss = 0.32911183\n",
      "Iteration 29, loss = 0.32875287\n",
      "Iteration 30, loss = 0.32819293\n",
      "Iteration 31, loss = 0.32762334\n",
      "Iteration 32, loss = 0.32743316\n",
      "Iteration 33, loss = 0.32699197\n",
      "Iteration 34, loss = 0.32631048\n",
      "Iteration 35, loss = 0.32606736\n",
      "Iteration 36, loss = 0.32582628\n",
      "Iteration 37, loss = 0.32520734\n",
      "Iteration 38, loss = 0.32528733\n",
      "Iteration 39, loss = 0.32495108\n",
      "Iteration 40, loss = 0.32484887\n",
      "Iteration 41, loss = 0.32441121\n",
      "Iteration 42, loss = 0.32418166\n",
      "Iteration 43, loss = 0.32368075\n",
      "Iteration 44, loss = 0.32340003\n",
      "Iteration 45, loss = 0.32344149\n",
      "Iteration 46, loss = 0.32321690\n",
      "Iteration 47, loss = 0.32302545\n",
      "Iteration 48, loss = 0.32259946\n",
      "Iteration 49, loss = 0.32225733\n",
      "Iteration 50, loss = 0.32223570\n",
      "Iteration 51, loss = 0.32217192\n",
      "Iteration 52, loss = 0.32198032\n",
      "Iteration 53, loss = 0.32159845\n",
      "Iteration 54, loss = 0.32141216\n",
      "Iteration 55, loss = 0.32111041\n",
      "Iteration 56, loss = 0.32075209\n",
      "Iteration 57, loss = 0.32074977\n",
      "Iteration 58, loss = 0.32095215\n",
      "Iteration 59, loss = 0.32060653\n",
      "Iteration 60, loss = 0.32037536\n",
      "Iteration 61, loss = 0.32009253\n",
      "Iteration 62, loss = 0.31995288\n",
      "Iteration 63, loss = 0.31969067\n",
      "Iteration 64, loss = 0.31954988\n",
      "Iteration 65, loss = 0.31936393\n",
      "Iteration 66, loss = 0.31910650\n",
      "Iteration 67, loss = 0.31886293\n",
      "Iteration 68, loss = 0.31879075\n",
      "Iteration 69, loss = 0.31864068\n",
      "Iteration 70, loss = 0.31822125\n",
      "Iteration 71, loss = 0.31808353\n",
      "Iteration 72, loss = 0.31811352\n",
      "Iteration 73, loss = 0.31802051\n",
      "Iteration 74, loss = 0.31771260\n",
      "Iteration 75, loss = 0.31759766\n",
      "Iteration 76, loss = 0.31749871\n",
      "Iteration 77, loss = 0.31728159\n",
      "Iteration 78, loss = 0.31684261\n",
      "Iteration 79, loss = 0.31703172\n",
      "Iteration 80, loss = 0.31691099\n",
      "Iteration 81, loss = 0.31650010\n",
      "Iteration 82, loss = 0.31639846\n",
      "Iteration 83, loss = 0.31655042\n",
      "Iteration 84, loss = 0.31628404\n",
      "Iteration 85, loss = 0.31561396\n",
      "Iteration 86, loss = 0.31606997\n",
      "Iteration 87, loss = 0.31580311\n",
      "Iteration 88, loss = 0.31577955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72, loss = 0.31272416\n",
      "Iteration 73, loss = 0.31203532\n",
      "Iteration 74, loss = 0.31190194\n",
      "Iteration 75, loss = 0.31146338\n",
      "Iteration 76, loss = 0.31129825\n",
      "Iteration 77, loss = 0.31066697\n",
      "Iteration 78, loss = 0.31075881\n",
      "Iteration 79, loss = 0.31038178\n",
      "Iteration 80, loss = 0.30983342\n",
      "Iteration 81, loss = 0.30939032\n",
      "Iteration 82, loss = 0.30924553\n",
      "Iteration 83, loss = 0.30881035\n",
      "Iteration 84, loss = 0.30864497\n",
      "Iteration 85, loss = 0.30834894\n",
      "Iteration 86, loss = 0.30811707\n",
      "Iteration 87, loss = 0.30783303\n",
      "Iteration 88, loss = 0.30776580\n",
      "Iteration 89, loss = 0.30763639\n",
      "Iteration 90, loss = 0.30662990\n",
      "Iteration 91, loss = 0.30654484\n",
      "Iteration 92, loss = 0.30686139\n",
      "Iteration 93, loss = 0.30602141\n",
      "Iteration 94, loss = 0.30530534\n",
      "Iteration 95, loss = 0.30564788\n",
      "Iteration 96, loss = 0.30501211\n",
      "Iteration 97, loss = 0.30431501\n",
      "Iteration 98, loss = 0.30499454\n",
      "Iteration 99, loss = 0.30397760\n",
      "Iteration 100, loss = 0.30407133\n",
      "Iteration 101, loss = 0.30377614\n",
      "Iteration 102, loss = 0.30328356\n",
      "Iteration 103, loss = 0.30316756\n",
      "Iteration 104, loss = 0.30285756\n",
      "Iteration 105, loss = 0.30255628\n",
      "Iteration 106, loss = 0.30245854\n",
      "Iteration 107, loss = 0.30203180\n",
      "Iteration 108, loss = 0.30204636\n",
      "Iteration 109, loss = 0.30144273\n",
      "Iteration 110, loss = 0.30104721\n",
      "Iteration 111, loss = 0.30089593\n",
      "Iteration 112, loss = 0.30121160\n",
      "Iteration 113, loss = 0.30095213\n",
      "Iteration 114, loss = 0.30028914\n",
      "Iteration 115, loss = 0.30019979\n",
      "Iteration 116, loss = 0.29918345\n",
      "Iteration 117, loss = 0.29974916\n",
      "Iteration 118, loss = 0.29913079\n",
      "Iteration 119, loss = 0.29899333\n",
      "Iteration 120, loss = 0.29903145\n",
      "Iteration 121, loss = 0.29840517\n",
      "Iteration 122, loss = 0.29866219\n",
      "Iteration 123, loss = 0.29857547\n",
      "Iteration 124, loss = 0.29794698\n",
      "Iteration 125, loss = 0.29773362\n",
      "Iteration 126, loss = 0.29831213\n",
      "Iteration 127, loss = 0.29724558\n",
      "Iteration 128, loss = 0.29759414\n",
      "Iteration 129, loss = 0.29662740\n",
      "Iteration 130, loss = 0.29676657\n",
      "Iteration 131, loss = 0.29688445\n",
      "Iteration 132, loss = 0.29647433\n",
      "Iteration 133, loss = 0.29617400\n",
      "Iteration 134, loss = 0.29635895\n",
      "Iteration 135, loss = 0.29589852\n",
      "Iteration 136, loss = 0.29636486\n",
      "Iteration 137, loss = 0.29607702\n",
      "Iteration 138, loss = 0.29511414\n",
      "Iteration 139, loss = 0.29538992\n",
      "Iteration 140, loss = 0.29503068\n",
      "Iteration 141, loss = 0.29502740\n",
      "Iteration 142, loss = 0.29473426\n",
      "Iteration 143, loss = 0.29432508\n",
      "Iteration 144, loss = 0.29464322\n",
      "Iteration 145, loss = 0.29492971\n",
      "Iteration 146, loss = 0.29385158\n",
      "Iteration 147, loss = 0.29379897\n",
      "Iteration 148, loss = 0.29404685\n",
      "Iteration 149, loss = 0.29375890\n",
      "Iteration 150, loss = 0.29326909\n",
      "Iteration 151, loss = 0.29353624\n",
      "Iteration 152, loss = 0.29360242\n",
      "Iteration 153, loss = 0.29292793\n",
      "Iteration 154, loss = 0.29264074\n",
      "Iteration 155, loss = 0.29307105\n",
      "Iteration 156, loss = 0.29205268\n",
      "Iteration 157, loss = 0.29239652\n",
      "Iteration 158, loss = 0.29283308\n",
      "Iteration 159, loss = 0.29193582\n",
      "Iteration 160, loss = 0.29251086\n",
      "Iteration 161, loss = 0.29219769\n",
      "Iteration 162, loss = 0.29148623\n",
      "Iteration 163, loss = 0.29151973\n",
      "Iteration 164, loss = 0.29150548\n",
      "Iteration 165, loss = 0.29115609\n",
      "Iteration 166, loss = 0.29129272\n",
      "Iteration 167, loss = 0.29103224\n",
      "Iteration 168, loss = 0.29111359\n",
      "Iteration 169, loss = 0.29078996\n",
      "Iteration 170, loss = 0.29048737\n",
      "Iteration 171, loss = 0.29082584\n",
      "Iteration 172, loss = 0.29044760\n",
      "Iteration 173, loss = 0.28989125\n",
      "Iteration 174, loss = 0.28988474\n",
      "Iteration 175, loss = 0.28967253\n",
      "Iteration 176, loss = 0.28973314\n",
      "Iteration 177, loss = 0.28991560\n",
      "Iteration 178, loss = 0.28965128\n",
      "Iteration 179, loss = 0.28869220\n",
      "Iteration 180, loss = 0.28885326\n",
      "Iteration 181, loss = 0.28899343\n",
      "Iteration 182, loss = 0.28925476\n",
      "Iteration 183, loss = 0.28890554\n",
      "Iteration 184, loss = 0.28889034\n",
      "Iteration 185, loss = 0.28844069\n",
      "Iteration 186, loss = 0.28902243\n",
      "Iteration 187, loss = 0.28802326\n",
      "Iteration 188, loss = 0.28788309\n",
      "Iteration 189, loss = 0.28765644\n",
      "Iteration 190, loss = 0.28760908\n",
      "Iteration 191, loss = 0.28763704\n",
      "Iteration 192, loss = 0.28764389\n",
      "Iteration 193, loss = 0.28803771\n",
      "Iteration 194, loss = 0.28718269\n",
      "Iteration 195, loss = 0.28722226\n",
      "Iteration 196, loss = 0.28656476\n",
      "Iteration 197, loss = 0.28668777\n",
      "Iteration 198, loss = 0.28668657\n",
      "Iteration 199, loss = 0.28757782\n",
      "Iteration 200, loss = 0.28694638\n",
      "Iteration 201, loss = 0.28672879\n",
      "Iteration 202, loss = 0.28594756\n",
      "Iteration 203, loss = 0.28668204\n",
      "Iteration 204, loss = 0.28616208\n",
      "Iteration 205, loss = 0.28644718\n",
      "Iteration 206, loss = 0.28612833\n",
      "Iteration 207, loss = 0.28543821\n",
      "Iteration 208, loss = 0.28529237\n",
      "Iteration 209, loss = 0.28563367\n",
      "Iteration 210, loss = 0.28561797\n",
      "Iteration 211, loss = 0.28469339\n",
      "Iteration 212, loss = 0.28481788\n",
      "Iteration 213, loss = 0.28425929\n",
      "Iteration 214, loss = 0.28521602\n",
      "Iteration 215, loss = 0.28503797\n",
      "Iteration 216, loss = 0.28471606\n",
      "Iteration 217, loss = 0.28435678\n",
      "Iteration 218, loss = 0.28426285\n",
      "Iteration 219, loss = 0.28431564\n",
      "Iteration 220, loss = 0.28412935\n",
      "Iteration 221, loss = 0.28406867\n",
      "Iteration 222, loss = 0.28421799\n",
      "Iteration 223, loss = 0.28426551\n",
      "Iteration 224, loss = 0.28376101\n",
      "Iteration 225, loss = 0.28329466\n",
      "Iteration 226, loss = 0.28295443\n",
      "Iteration 227, loss = 0.28335339\n",
      "Iteration 228, loss = 0.28400764\n",
      "Iteration 229, loss = 0.28300013\n",
      "Iteration 230, loss = 0.28277587\n",
      "Iteration 231, loss = 0.28285036\n",
      "Iteration 232, loss = 0.28231718\n",
      "Iteration 233, loss = 0.28270310\n",
      "Iteration 234, loss = 0.28279462\n",
      "Iteration 235, loss = 0.28292381\n",
      "Iteration 236, loss = 0.28248751\n",
      "Iteration 237, loss = 0.28219764\n",
      "Iteration 238, loss = 0.28171495\n",
      "Iteration 239, loss = 0.28195418\n",
      "Iteration 240, loss = 0.28151037\n",
      "Iteration 241, loss = 0.28096131\n",
      "Iteration 242, loss = 0.28127207\n",
      "Iteration 243, loss = 0.28126559\n",
      "Iteration 244, loss = 0.28106039\n",
      "Iteration 245, loss = 0.28081120\n",
      "Iteration 246, loss = 0.28025642\n",
      "Iteration 247, loss = 0.28031916\n",
      "Iteration 248, loss = 0.28030639\n",
      "Iteration 249, loss = 0.27980683\n",
      "Iteration 250, loss = 0.28067731\n",
      "Iteration 251, loss = 0.28005574\n",
      "Iteration 252, loss = 0.28029615\n",
      "Iteration 253, loss = 0.27987650\n",
      "Iteration 254, loss = 0.27995411\n",
      "Iteration 255, loss = 0.27944604\n",
      "Iteration 256, loss = 0.27904611\n",
      "Iteration 257, loss = 0.27869603\n",
      "Iteration 258, loss = 0.27808641\n",
      "Iteration 259, loss = 0.27800934\n",
      "Iteration 260, loss = 0.27846597\n",
      "Iteration 261, loss = 0.27840895\n",
      "Iteration 262, loss = 0.27789772\n",
      "Iteration 263, loss = 0.27831633\n",
      "Iteration 264, loss = 0.27705484\n",
      "Iteration 265, loss = 0.27747746\n",
      "Iteration 266, loss = 0.27744475\n",
      "Iteration 267, loss = 0.27674099\n",
      "Iteration 268, loss = 0.27684886\n",
      "Iteration 269, loss = 0.27651997\n",
      "Iteration 270, loss = 0.27675547\n",
      "Iteration 271, loss = 0.27566486\n",
      "Iteration 272, loss = 0.27599453\n",
      "Iteration 273, loss = 0.27587175\n",
      "Iteration 274, loss = 0.27530656\n",
      "Iteration 275, loss = 0.27520970\n",
      "Iteration 276, loss = 0.27535673\n",
      "Iteration 277, loss = 0.27511754\n",
      "Iteration 278, loss = 0.27478887\n",
      "Iteration 279, loss = 0.27471276\n",
      "Iteration 280, loss = 0.27438884\n",
      "Iteration 281, loss = 0.27437826\n",
      "Iteration 282, loss = 0.27438225\n",
      "Iteration 283, loss = 0.27434053\n",
      "Iteration 284, loss = 0.27432258\n",
      "Iteration 285, loss = 0.27415523\n",
      "Iteration 286, loss = 0.27332087\n",
      "Iteration 287, loss = 0.27373275\n",
      "Iteration 288, loss = 0.27315918\n",
      "Iteration 289, loss = 0.27301048\n",
      "Iteration 290, loss = 0.27338991\n",
      "Iteration 291, loss = 0.27286310\n",
      "Iteration 292, loss = 0.27257457\n",
      "Iteration 293, loss = 0.27231545\n",
      "Iteration 294, loss = 0.27179617\n",
      "Iteration 295, loss = 0.27220502\n",
      "Iteration 296, loss = 0.27153987\n",
      "Iteration 297, loss = 0.27191835\n",
      "Iteration 298, loss = 0.27184903\n",
      "Iteration 299, loss = 0.27152959\n",
      "Iteration 300, loss = 0.27158764\n",
      "Iteration 1, loss = 0.49829481\n",
      "Iteration 2, loss = 0.39377489\n",
      "Iteration 3, loss = 0.37168628\n",
      "Iteration 4, loss = 0.36108422\n",
      "Iteration 5, loss = 0.35515609\n",
      "Iteration 6, loss = 0.35173882\n",
      "Iteration 7, loss = 0.34869852\n",
      "Iteration 8, loss = 0.34651212\n",
      "Iteration 9, loss = 0.34499630\n",
      "Iteration 10, loss = 0.34286633\n",
      "Iteration 11, loss = 0.34211205\n",
      "Iteration 12, loss = 0.34059955\n",
      "Iteration 13, loss = 0.33961868\n",
      "Iteration 14, loss = 0.33893069\n",
      "Iteration 15, loss = 0.33791072\n",
      "Iteration 16, loss = 0.33730634\n",
      "Iteration 17, loss = 0.33630566\n",
      "Iteration 18, loss = 0.33585786\n",
      "Iteration 19, loss = 0.33529501\n",
      "Iteration 20, loss = 0.33423614\n",
      "Iteration 21, loss = 0.33359600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 159, loss = 0.33937009\n",
      "Iteration 160, loss = 0.33926251\n",
      "Iteration 161, loss = 0.33914310\n",
      "Iteration 162, loss = 0.33900870\n",
      "Iteration 163, loss = 0.33891775\n",
      "Iteration 164, loss = 0.33870212\n",
      "Iteration 165, loss = 0.33872430\n",
      "Iteration 166, loss = 0.33857349\n",
      "Iteration 167, loss = 0.33835747\n",
      "Iteration 168, loss = 0.33826044\n",
      "Iteration 169, loss = 0.33831052\n",
      "Iteration 170, loss = 0.33809112\n",
      "Iteration 171, loss = 0.33818645\n",
      "Iteration 172, loss = 0.33789179\n",
      "Iteration 173, loss = 0.33796274\n",
      "Iteration 174, loss = 0.33774396\n",
      "Iteration 175, loss = 0.33777199\n",
      "Iteration 176, loss = 0.33767553\n",
      "Iteration 177, loss = 0.33756893\n",
      "Iteration 178, loss = 0.33741470\n",
      "Iteration 179, loss = 0.33723166\n",
      "Iteration 180, loss = 0.33717151\n",
      "Iteration 181, loss = 0.33713609\n",
      "Iteration 182, loss = 0.33704695\n",
      "Iteration 183, loss = 0.33687282\n",
      "Iteration 184, loss = 0.33680482\n",
      "Iteration 185, loss = 0.33670757\n",
      "Iteration 186, loss = 0.33663875\n",
      "Iteration 187, loss = 0.33647226\n",
      "Iteration 188, loss = 0.33651546\n",
      "Iteration 189, loss = 0.33622980\n",
      "Iteration 190, loss = 0.33630480\n",
      "Iteration 191, loss = 0.33605360\n",
      "Iteration 192, loss = 0.33611265\n",
      "Iteration 193, loss = 0.33594129\n",
      "Iteration 194, loss = 0.33606511\n",
      "Iteration 195, loss = 0.33581658\n",
      "Iteration 196, loss = 0.33572463\n",
      "Iteration 197, loss = 0.33551731\n",
      "Iteration 198, loss = 0.33565224\n",
      "Iteration 199, loss = 0.33538331\n",
      "Iteration 200, loss = 0.33540469\n",
      "Iteration 201, loss = 0.33512980\n",
      "Iteration 202, loss = 0.33507712\n",
      "Iteration 203, loss = 0.33488958\n",
      "Iteration 204, loss = 0.33493552\n",
      "Iteration 205, loss = 0.33472454\n",
      "Iteration 206, loss = 0.33474389\n",
      "Iteration 207, loss = 0.33458560\n",
      "Iteration 208, loss = 0.33443905\n",
      "Iteration 209, loss = 0.33451204\n",
      "Iteration 210, loss = 0.33437957\n",
      "Iteration 211, loss = 0.33430035\n",
      "Iteration 212, loss = 0.33421595\n",
      "Iteration 213, loss = 0.33401716\n",
      "Iteration 214, loss = 0.33396070\n",
      "Iteration 215, loss = 0.33382776\n",
      "Iteration 216, loss = 0.33390888\n",
      "Iteration 217, loss = 0.33376578\n",
      "Iteration 218, loss = 0.33361642\n",
      "Iteration 219, loss = 0.33351652\n",
      "Iteration 220, loss = 0.33349867\n",
      "Iteration 221, loss = 0.33325434\n",
      "Iteration 222, loss = 0.33331049\n",
      "Iteration 223, loss = 0.33336617\n",
      "Iteration 224, loss = 0.33321236\n",
      "Iteration 225, loss = 0.33302970\n",
      "Iteration 226, loss = 0.33292708\n",
      "Iteration 227, loss = 0.33277560\n",
      "Iteration 228, loss = 0.33264934\n",
      "Iteration 229, loss = 0.33268577\n",
      "Iteration 230, loss = 0.33268114\n",
      "Iteration 231, loss = 0.33252575\n",
      "Iteration 232, loss = 0.33240376\n",
      "Iteration 233, loss = 0.33235297\n",
      "Iteration 234, loss = 0.33214037\n",
      "Iteration 235, loss = 0.33220747\n",
      "Iteration 236, loss = 0.33214199\n",
      "Iteration 237, loss = 0.33201907\n",
      "Iteration 238, loss = 0.33193973\n",
      "Iteration 239, loss = 0.33190690\n",
      "Iteration 240, loss = 0.33183118\n",
      "Iteration 241, loss = 0.33179058\n",
      "Iteration 242, loss = 0.33182603\n",
      "Iteration 243, loss = 0.33155736\n",
      "Iteration 244, loss = 0.33136737\n",
      "Iteration 245, loss = 0.33142253\n",
      "Iteration 246, loss = 0.33129911\n",
      "Iteration 247, loss = 0.33141145\n",
      "Iteration 248, loss = 0.33128937\n",
      "Iteration 249, loss = 0.33122502\n",
      "Iteration 250, loss = 0.33101887\n",
      "Iteration 251, loss = 0.33100410\n",
      "Iteration 252, loss = 0.33096483\n",
      "Iteration 253, loss = 0.33078412\n",
      "Iteration 254, loss = 0.33073379\n",
      "Iteration 255, loss = 0.33078352\n",
      "Iteration 256, loss = 0.33065522\n",
      "Iteration 257, loss = 0.33060865\n",
      "Iteration 258, loss = 0.33067348\n",
      "Iteration 259, loss = 0.33043775\n",
      "Iteration 260, loss = 0.33052990\n",
      "Iteration 261, loss = 0.33033853\n",
      "Iteration 262, loss = 0.33018242\n",
      "Iteration 263, loss = 0.33027416\n",
      "Iteration 264, loss = 0.33019184\n",
      "Iteration 265, loss = 0.33018279\n",
      "Iteration 266, loss = 0.33003249\n",
      "Iteration 267, loss = 0.32998882\n",
      "Iteration 268, loss = 0.32986238\n",
      "Iteration 269, loss = 0.32988147\n",
      "Iteration 270, loss = 0.32987717\n",
      "Iteration 271, loss = 0.32961041\n",
      "Iteration 272, loss = 0.32980256\n",
      "Iteration 273, loss = 0.32966044\n",
      "Iteration 274, loss = 0.32951356\n",
      "Iteration 275, loss = 0.32958436\n",
      "Iteration 276, loss = 0.32941028\n",
      "Iteration 277, loss = 0.32937520\n",
      "Iteration 278, loss = 0.32936204\n",
      "Iteration 279, loss = 0.32924640\n",
      "Iteration 280, loss = 0.32922512\n",
      "Iteration 281, loss = 0.32927259\n",
      "Iteration 282, loss = 0.32926857\n",
      "Iteration 283, loss = 0.32903983\n",
      "Iteration 284, loss = 0.32914348\n",
      "Iteration 285, loss = 0.32898334\n",
      "Iteration 286, loss = 0.32897506\n",
      "Iteration 287, loss = 0.32883776\n",
      "Iteration 288, loss = 0.32891512\n",
      "Iteration 289, loss = 0.32886530\n",
      "Iteration 290, loss = 0.32858695\n",
      "Iteration 291, loss = 0.32873356\n",
      "Iteration 292, loss = 0.32863496\n",
      "Iteration 293, loss = 0.32848903\n",
      "Iteration 294, loss = 0.32856618\n",
      "Iteration 295, loss = 0.32852860\n",
      "Iteration 296, loss = 0.32843357\n",
      "Iteration 297, loss = 0.32836018\n",
      "Iteration 298, loss = 0.32834760\n",
      "Iteration 299, loss = 0.32835268\n",
      "Iteration 300, loss = 0.32831134\n",
      "Iteration 1, loss = 0.65660380\n",
      "Iteration 2, loss = 0.60839806\n",
      "Iteration 3, loss = 0.58516878\n",
      "Iteration 4, loss = 0.56449964\n",
      "Iteration 5, loss = 0.54511558\n",
      "Iteration 6, loss = 0.52686999\n",
      "Iteration 7, loss = 0.51038824\n",
      "Iteration 8, loss = 0.49561857\n",
      "Iteration 9, loss = 0.48264207\n",
      "Iteration 10, loss = 0.47119211\n",
      "Iteration 11, loss = 0.46108903\n",
      "Iteration 12, loss = 0.45207266\n",
      "Iteration 13, loss = 0.44425936\n",
      "Iteration 14, loss = 0.43709802\n",
      "Iteration 15, loss = 0.43091201\n",
      "Iteration 16, loss = 0.42528115\n",
      "Iteration 17, loss = 0.42038534\n",
      "Iteration 18, loss = 0.41586271\n",
      "Iteration 19, loss = 0.41182686\n",
      "Iteration 20, loss = 0.40819603\n",
      "Iteration 21, loss = 0.40498715\n",
      "Iteration 22, loss = 0.40188973\n",
      "Iteration 23, loss = 0.39917108\n",
      "Iteration 24, loss = 0.39669406\n",
      "Iteration 25, loss = 0.39435932\n",
      "Iteration 26, loss = 0.39217190\n",
      "Iteration 27, loss = 0.39012897\n",
      "Iteration 28, loss = 0.38831211\n",
      "Iteration 29, loss = 0.38656448\n",
      "Iteration 30, loss = 0.38493512\n",
      "Iteration 31, loss = 0.38334378\n",
      "Iteration 32, loss = 0.38196578\n",
      "Iteration 33, loss = 0.38056441\n",
      "Iteration 34, loss = 0.37925441\n",
      "Iteration 35, loss = 0.37800035\n",
      "Iteration 36, loss = 0.37686671\n",
      "Iteration 37, loss = 0.37571513\n",
      "Iteration 38, loss = 0.37469994\n",
      "Iteration 39, loss = 0.37359073\n",
      "Iteration 40, loss = 0.37264753\n",
      "Iteration 41, loss = 0.37172520\n",
      "Iteration 42, loss = 0.37085415\n",
      "Iteration 43, loss = 0.36989337\n",
      "Iteration 44, loss = 0.36905861\n",
      "Iteration 45, loss = 0.36837822\n",
      "Iteration 46, loss = 0.36751630\n",
      "Iteration 47, loss = 0.36684143\n",
      "Iteration 48, loss = 0.36601267\n",
      "Iteration 49, loss = 0.36520412\n",
      "Iteration 50, loss = 0.36460325\n",
      "Iteration 51, loss = 0.36411318\n",
      "Iteration 52, loss = 0.36335983\n",
      "Iteration 53, loss = 0.36263213\n",
      "Iteration 54, loss = 0.36221404\n",
      "Iteration 55, loss = 0.36154996\n",
      "Iteration 56, loss = 0.36094001\n",
      "Iteration 57, loss = 0.36044017\n",
      "Iteration 58, loss = 0.36001628\n",
      "Iteration 59, loss = 0.35950879\n",
      "Iteration 60, loss = 0.35900848\n",
      "Iteration 61, loss = 0.35852419\n",
      "Iteration 62, loss = 0.35804133\n",
      "Iteration 63, loss = 0.35761515\n",
      "Iteration 64, loss = 0.35717024\n",
      "Iteration 65, loss = 0.35675586\n",
      "Iteration 66, loss = 0.35631100\n",
      "Iteration 67, loss = 0.35593043\n",
      "Iteration 68, loss = 0.35560263\n",
      "Iteration 69, loss = 0.35521940\n",
      "Iteration 70, loss = 0.35471070\n",
      "Iteration 71, loss = 0.35435786\n",
      "Iteration 72, loss = 0.35408979\n",
      "Iteration 73, loss = 0.35380461\n",
      "Iteration 74, loss = 0.35336591\n",
      "Iteration 75, loss = 0.35306507\n",
      "Iteration 76, loss = 0.35282310\n",
      "Iteration 77, loss = 0.35242750\n",
      "Iteration 78, loss = 0.35214164\n",
      "Iteration 79, loss = 0.35182532\n",
      "Iteration 80, loss = 0.35167278\n",
      "Iteration 81, loss = 0.35122414\n",
      "Iteration 82, loss = 0.35103899\n",
      "Iteration 83, loss = 0.35075229\n",
      "Iteration 84, loss = 0.35050070\n",
      "Iteration 85, loss = 0.35012767\n",
      "Iteration 86, loss = 0.35006951\n",
      "Iteration 87, loss = 0.34977488\n",
      "Iteration 88, loss = 0.34955117\n",
      "Iteration 89, loss = 0.34925456\n",
      "Iteration 90, loss = 0.34889932\n",
      "Iteration 91, loss = 0.34884149\n",
      "Iteration 92, loss = 0.34855372\n",
      "Iteration 93, loss = 0.34830870\n",
      "Iteration 94, loss = 0.34810359\n",
      "Iteration 95, loss = 0.34792723\n",
      "Iteration 96, loss = 0.34772414\n",
      "Iteration 97, loss = 0.34750312\n",
      "Iteration 98, loss = 0.34723773\n",
      "Iteration 99, loss = 0.34723952\n",
      "Iteration 100, loss = 0.34706688\n",
      "Iteration 101, loss = 0.34687985\n",
      "Iteration 102, loss = 0.34656405\n",
      "Iteration 103, loss = 0.34636148\n",
      "Iteration 104, loss = 0.34617299\n",
      "Iteration 105, loss = 0.34611512\n",
      "Iteration 106, loss = 0.34583195\n",
      "Iteration 107, loss = 0.34565643\n",
      "Iteration 108, loss = 0.34546606\n",
      "Iteration 109, loss = 0.34528613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 115, loss = 0.34445522\n",
      "Iteration 116, loss = 0.34429934\n",
      "Iteration 117, loss = 0.34398676\n",
      "Iteration 118, loss = 0.34398389\n",
      "Iteration 119, loss = 0.34393496\n",
      "Iteration 120, loss = 0.34376099\n",
      "Iteration 121, loss = 0.34347222\n",
      "Iteration 122, loss = 0.34345191\n",
      "Iteration 123, loss = 0.34322040\n",
      "Iteration 124, loss = 0.34308122\n",
      "Iteration 125, loss = 0.34305010\n",
      "Iteration 126, loss = 0.34295058\n",
      "Iteration 127, loss = 0.34276990\n",
      "Iteration 128, loss = 0.34261965\n",
      "Iteration 129, loss = 0.34256295\n",
      "Iteration 130, loss = 0.34240487\n",
      "Iteration 131, loss = 0.34232739\n",
      "Iteration 132, loss = 0.34228589\n",
      "Iteration 133, loss = 0.34208287\n",
      "Iteration 134, loss = 0.34201362\n",
      "Iteration 135, loss = 0.34181887\n",
      "Iteration 136, loss = 0.34169405\n",
      "Iteration 137, loss = 0.34155116\n",
      "Iteration 138, loss = 0.34152433\n",
      "Iteration 139, loss = 0.34131405\n",
      "Iteration 140, loss = 0.34115381\n",
      "Iteration 141, loss = 0.34116126\n",
      "Iteration 142, loss = 0.34106996\n",
      "Iteration 143, loss = 0.34077620\n",
      "Iteration 144, loss = 0.34080656\n",
      "Iteration 145, loss = 0.34078890\n",
      "Iteration 146, loss = 0.34046321\n",
      "Iteration 147, loss = 0.34049927\n",
      "Iteration 148, loss = 0.34034129\n",
      "Iteration 149, loss = 0.34031336\n",
      "Iteration 150, loss = 0.34016650\n",
      "Iteration 151, loss = 0.34004997\n",
      "Iteration 152, loss = 0.33995126\n",
      "Iteration 153, loss = 0.33990643\n",
      "Iteration 154, loss = 0.33973459\n",
      "Iteration 155, loss = 0.33955931\n",
      "Iteration 156, loss = 0.33957341\n",
      "Iteration 157, loss = 0.33952093\n",
      "Iteration 158, loss = 0.33935921\n",
      "Iteration 159, loss = 0.33928791\n",
      "Iteration 160, loss = 0.33923858\n",
      "Iteration 161, loss = 0.33917455\n",
      "Iteration 162, loss = 0.33899503\n",
      "Iteration 163, loss = 0.33898588\n",
      "Iteration 164, loss = 0.33881486\n",
      "Iteration 165, loss = 0.33866162\n",
      "Iteration 166, loss = 0.33867270\n",
      "Iteration 167, loss = 0.33858869\n",
      "Iteration 168, loss = 0.33849794\n",
      "Iteration 169, loss = 0.33833164\n",
      "Iteration 170, loss = 0.33828759\n",
      "Iteration 171, loss = 0.33815898\n",
      "Iteration 172, loss = 0.33806529\n",
      "Iteration 173, loss = 0.33799178\n",
      "Iteration 174, loss = 0.33793578\n",
      "Iteration 175, loss = 0.33788026\n",
      "Iteration 176, loss = 0.33771244\n",
      "Iteration 177, loss = 0.33776004\n",
      "Iteration 178, loss = 0.33750591\n",
      "Iteration 179, loss = 0.33745468\n",
      "Iteration 180, loss = 0.33742715\n",
      "Iteration 181, loss = 0.33732580\n",
      "Iteration 182, loss = 0.33731481\n",
      "Iteration 183, loss = 0.33708316\n",
      "Iteration 184, loss = 0.33713953\n",
      "Iteration 185, loss = 0.33702149\n",
      "Iteration 186, loss = 0.33687598\n",
      "Iteration 187, loss = 0.33675350\n",
      "Iteration 188, loss = 0.33686619\n",
      "Iteration 189, loss = 0.33660118\n",
      "Iteration 190, loss = 0.33655894\n",
      "Iteration 191, loss = 0.33654810\n",
      "Iteration 192, loss = 0.33641614\n",
      "Iteration 193, loss = 0.33633957\n",
      "Iteration 194, loss = 0.33631312\n",
      "Iteration 195, loss = 0.33625634\n",
      "Iteration 196, loss = 0.33611223\n",
      "Iteration 197, loss = 0.33600558\n",
      "Iteration 198, loss = 0.33594274\n",
      "Iteration 199, loss = 0.33593508\n",
      "Iteration 200, loss = 0.33593128\n",
      "Iteration 201, loss = 0.33570706\n",
      "Iteration 202, loss = 0.33568897\n",
      "Iteration 203, loss = 0.33549717\n",
      "Iteration 204, loss = 0.33542437\n",
      "Iteration 205, loss = 0.33538583\n",
      "Iteration 206, loss = 0.33529117\n",
      "Iteration 207, loss = 0.33531381\n",
      "Iteration 208, loss = 0.33511323\n",
      "Iteration 209, loss = 0.33526311\n",
      "Iteration 210, loss = 0.33510111\n",
      "Iteration 211, loss = 0.33499245\n",
      "Iteration 212, loss = 0.33497714\n",
      "Iteration 213, loss = 0.33484628\n",
      "Iteration 214, loss = 0.33483292\n",
      "Iteration 215, loss = 0.33475018\n",
      "Iteration 216, loss = 0.33455902\n",
      "Iteration 217, loss = 0.33442990\n",
      "Iteration 218, loss = 0.33433133\n",
      "Iteration 219, loss = 0.33439006\n",
      "Iteration 220, loss = 0.33429677\n",
      "Iteration 221, loss = 0.33423221\n",
      "Iteration 222, loss = 0.33403584\n",
      "Iteration 223, loss = 0.33402995\n",
      "Iteration 224, loss = 0.33398783\n",
      "Iteration 225, loss = 0.33393233\n",
      "Iteration 226, loss = 0.33389210\n",
      "Iteration 227, loss = 0.33366435\n",
      "Iteration 228, loss = 0.33368052\n",
      "Iteration 229, loss = 0.33366617\n",
      "Iteration 230, loss = 0.33354609\n",
      "Iteration 231, loss = 0.33349837\n",
      "Iteration 232, loss = 0.33327210\n",
      "Iteration 233, loss = 0.33323111\n",
      "Iteration 234, loss = 0.33326777\n",
      "Iteration 235, loss = 0.33310082\n",
      "Iteration 236, loss = 0.33317179\n",
      "Iteration 237, loss = 0.33295812\n",
      "Iteration 238, loss = 0.33296309\n",
      "Iteration 239, loss = 0.33292359\n",
      "Iteration 240, loss = 0.33263740\n",
      "Iteration 241, loss = 0.33268251\n",
      "Iteration 242, loss = 0.33272251\n",
      "Iteration 243, loss = 0.33238270\n",
      "Iteration 244, loss = 0.33241866\n",
      "Iteration 245, loss = 0.33238973\n",
      "Iteration 246, loss = 0.33238227\n",
      "Iteration 247, loss = 0.33225147\n",
      "Iteration 248, loss = 0.33216148\n",
      "Iteration 249, loss = 0.33215943\n",
      "Iteration 250, loss = 0.33195601\n",
      "Iteration 251, loss = 0.33192258\n",
      "Iteration 252, loss = 0.33201188\n",
      "Iteration 253, loss = 0.33174066\n",
      "Iteration 254, loss = 0.33160874\n",
      "Iteration 255, loss = 0.33172686\n",
      "Iteration 256, loss = 0.33158279\n",
      "Iteration 257, loss = 0.33144783\n",
      "Iteration 258, loss = 0.33139240\n",
      "Iteration 259, loss = 0.33133303\n",
      "Iteration 260, loss = 0.33113634\n",
      "Iteration 261, loss = 0.33124013\n",
      "Iteration 262, loss = 0.33114185\n",
      "Iteration 263, loss = 0.33094893\n",
      "Iteration 264, loss = 0.33085594\n",
      "Iteration 265, loss = 0.33074047\n",
      "Iteration 266, loss = 0.33078076\n",
      "Iteration 267, loss = 0.33074050\n",
      "Iteration 268, loss = 0.33060858\n",
      "Iteration 269, loss = 0.33056126\n",
      "Iteration 270, loss = 0.33055801\n",
      "Iteration 271, loss = 0.33039124\n",
      "Iteration 272, loss = 0.33037992\n",
      "Iteration 273, loss = 0.33022107\n",
      "Iteration 274, loss = 0.33030056\n",
      "Iteration 275, loss = 0.33028366\n",
      "Iteration 276, loss = 0.32987395\n",
      "Iteration 277, loss = 0.33000911\n",
      "Iteration 278, loss = 0.32996319\n",
      "Iteration 279, loss = 0.32991837\n",
      "Iteration 280, loss = 0.32987872\n",
      "Iteration 281, loss = 0.32983711\n",
      "Iteration 282, loss = 0.32969087\n",
      "Iteration 283, loss = 0.32962832\n",
      "Iteration 284, loss = 0.32940612\n",
      "Iteration 285, loss = 0.32937950\n",
      "Iteration 286, loss = 0.32947131\n",
      "Iteration 287, loss = 0.32944369\n",
      "Iteration 288, loss = 0.32924469\n",
      "Iteration 289, loss = 0.32914808\n",
      "Iteration 290, loss = 0.32913321\n",
      "Iteration 291, loss = 0.32892841\n",
      "Iteration 292, loss = 0.32898799\n",
      "Iteration 293, loss = 0.32892057\n",
      "Iteration 294, loss = 0.32890504\n",
      "Iteration 295, loss = 0.32888346\n",
      "Iteration 296, loss = 0.32870124\n",
      "Iteration 297, loss = 0.32864270\n",
      "Iteration 298, loss = 0.32857909\n",
      "Iteration 299, loss = 0.32869603\n",
      "Iteration 300, loss = 0.32854497\n",
      "Iteration 1, loss = 0.49907723\n",
      "Iteration 2, loss = 0.39389975\n",
      "Iteration 3, loss = 0.37089623\n",
      "Iteration 4, loss = 0.36067761\n",
      "Iteration 5, loss = 0.35471433\n",
      "Iteration 6, loss = 0.35066348\n",
      "Iteration 7, loss = 0.34813629\n",
      "Iteration 8, loss = 0.34583776\n",
      "Iteration 9, loss = 0.34437897\n",
      "Iteration 10, loss = 0.34281927\n",
      "Iteration 11, loss = 0.34159006\n",
      "Iteration 12, loss = 0.34000788\n",
      "Iteration 13, loss = 0.33958026\n",
      "Iteration 14, loss = 0.33823726\n",
      "Iteration 15, loss = 0.33736750\n",
      "Iteration 16, loss = 0.33693670\n",
      "Iteration 17, loss = 0.33645277\n",
      "Iteration 18, loss = 0.33585598\n",
      "Iteration 19, loss = 0.33520450\n",
      "Iteration 20, loss = 0.33411723\n",
      "Iteration 21, loss = 0.33376305\n",
      "Iteration 22, loss = 0.33332379\n",
      "Iteration 23, loss = 0.33256370\n",
      "Iteration 24, loss = 0.33228491\n",
      "Iteration 25, loss = 0.33142207\n",
      "Iteration 26, loss = 0.33051522\n",
      "Iteration 27, loss = 0.32974568\n",
      "Iteration 28, loss = 0.32911183\n",
      "Iteration 29, loss = 0.32875287\n",
      "Iteration 30, loss = 0.32819293\n",
      "Iteration 31, loss = 0.32762334\n",
      "Iteration 32, loss = 0.32743316\n",
      "Iteration 33, loss = 0.32699197\n",
      "Iteration 34, loss = 0.32631048\n",
      "Iteration 35, loss = 0.32606736\n",
      "Iteration 36, loss = 0.32582628\n",
      "Iteration 37, loss = 0.32520734\n",
      "Iteration 38, loss = 0.32528733\n",
      "Iteration 39, loss = 0.32495108\n",
      "Iteration 40, loss = 0.32484887\n",
      "Iteration 41, loss = 0.32441121\n",
      "Iteration 42, loss = 0.32418166\n",
      "Iteration 43, loss = 0.32368075\n",
      "Iteration 44, loss = 0.32340003\n",
      "Iteration 45, loss = 0.32344149\n",
      "Iteration 46, loss = 0.32321690\n",
      "Iteration 47, loss = 0.32302545\n",
      "Iteration 48, loss = 0.32259946\n",
      "Iteration 49, loss = 0.32225733\n",
      "Iteration 50, loss = 0.32223570\n",
      "Iteration 51, loss = 0.32217192\n",
      "Iteration 52, loss = 0.32198032\n",
      "Iteration 53, loss = 0.32159845\n",
      "Iteration 54, loss = 0.32141216\n",
      "Iteration 55, loss = 0.32111041\n",
      "Iteration 56, loss = 0.32075209\n",
      "Iteration 57, loss = 0.32074977\n",
      "Iteration 58, loss = 0.32095215\n",
      "Iteration 59, loss = 0.32060653\n",
      "Iteration 60, loss = 0.32037536\n",
      "Iteration 61, loss = 0.32009253\n",
      "Iteration 62, loss = 0.31995288\n",
      "Iteration 63, loss = 0.31969067\n",
      "Iteration 64, loss = 0.31954988\n",
      "Iteration 156, loss = 0.33857479\n",
      "Iteration 157, loss = 0.33848341\n",
      "Iteration 158, loss = 0.33841256\n",
      "Iteration 159, loss = 0.33825253\n",
      "Iteration 160, loss = 0.33817325\n",
      "Iteration 161, loss = 0.33821034\n",
      "Iteration 162, loss = 0.33794774\n",
      "Iteration 163, loss = 0.33799810\n",
      "Iteration 164, loss = 0.33767806\n",
      "Iteration 165, loss = 0.33761246\n",
      "Iteration 166, loss = 0.33755671\n",
      "Iteration 167, loss = 0.33753739\n",
      "Iteration 168, loss = 0.33744493\n",
      "Iteration 169, loss = 0.33730502\n",
      "Iteration 170, loss = 0.33729360\n",
      "Iteration 171, loss = 0.33715038\n",
      "Iteration 172, loss = 0.33703869\n",
      "Iteration 173, loss = 0.33710880\n",
      "Iteration 174, loss = 0.33686287\n",
      "Iteration 175, loss = 0.33693825\n",
      "Iteration 176, loss = 0.33671577\n",
      "Iteration 177, loss = 0.33675385\n",
      "Iteration 178, loss = 0.33663180\n",
      "Iteration 179, loss = 0.33649345\n",
      "Iteration 180, loss = 0.33646910\n",
      "Iteration 181, loss = 0.33640895\n",
      "Iteration 182, loss = 0.33632235\n",
      "Iteration 183, loss = 0.33616108\n",
      "Iteration 184, loss = 0.33597261\n",
      "Iteration 185, loss = 0.33608691\n",
      "Iteration 186, loss = 0.33603002\n",
      "Iteration 187, loss = 0.33588988\n",
      "Iteration 188, loss = 0.33588257\n",
      "Iteration 189, loss = 0.33554247\n",
      "Iteration 190, loss = 0.33570158\n",
      "Iteration 191, loss = 0.33545776\n",
      "Iteration 192, loss = 0.33550101\n",
      "Iteration 193, loss = 0.33535299\n",
      "Iteration 194, loss = 0.33545356\n",
      "Iteration 195, loss = 0.33523000\n",
      "Iteration 196, loss = 0.33511131\n",
      "Iteration 197, loss = 0.33500279\n",
      "Iteration 198, loss = 0.33513217\n",
      "Iteration 199, loss = 0.33497197\n",
      "Iteration 200, loss = 0.33489802\n",
      "Iteration 201, loss = 0.33481105\n",
      "Iteration 202, loss = 0.33474456\n",
      "Iteration 203, loss = 0.33464121\n",
      "Iteration 204, loss = 0.33442584\n",
      "Iteration 205, loss = 0.33446421\n",
      "Iteration 206, loss = 0.33442167\n",
      "Iteration 207, loss = 0.33430426\n",
      "Iteration 208, loss = 0.33409123\n",
      "Iteration 209, loss = 0.33416862\n",
      "Iteration 210, loss = 0.33390642\n",
      "Iteration 211, loss = 0.33399894\n",
      "Iteration 212, loss = 0.33391124\n",
      "Iteration 213, loss = 0.33380190\n",
      "Iteration 214, loss = 0.33375367\n",
      "Iteration 215, loss = 0.33357471\n",
      "Iteration 216, loss = 0.33354389\n",
      "Iteration 217, loss = 0.33353126\n",
      "Iteration 218, loss = 0.33328191\n",
      "Iteration 219, loss = 0.33334153\n",
      "Iteration 220, loss = 0.33328426\n",
      "Iteration 221, loss = 0.33307991\n",
      "Iteration 222, loss = 0.33315944\n",
      "Iteration 223, loss = 0.33306517\n",
      "Iteration 224, loss = 0.33283706\n",
      "Iteration 225, loss = 0.33274176\n",
      "Iteration 226, loss = 0.33279705\n",
      "Iteration 227, loss = 0.33263148\n",
      "Iteration 228, loss = 0.33250278\n",
      "Iteration 229, loss = 0.33258420\n",
      "Iteration 230, loss = 0.33243050\n",
      "Iteration 231, loss = 0.33235994\n",
      "Iteration 232, loss = 0.33224913\n",
      "Iteration 233, loss = 0.33212995\n",
      "Iteration 234, loss = 0.33196181\n",
      "Iteration 235, loss = 0.33194637\n",
      "Iteration 236, loss = 0.33182423\n",
      "Iteration 237, loss = 0.33181867\n",
      "Iteration 238, loss = 0.33181309\n",
      "Iteration 239, loss = 0.33167905\n",
      "Iteration 240, loss = 0.33152820\n",
      "Iteration 241, loss = 0.33146008\n",
      "Iteration 242, loss = 0.33147531\n",
      "Iteration 243, loss = 0.33117027\n",
      "Iteration 244, loss = 0.33121590\n",
      "Iteration 245, loss = 0.33115447\n",
      "Iteration 246, loss = 0.33108727\n",
      "Iteration 247, loss = 0.33099419\n",
      "Iteration 248, loss = 0.33092561\n",
      "Iteration 249, loss = 0.33078305\n",
      "Iteration 250, loss = 0.33063121\n",
      "Iteration 251, loss = 0.33067082\n",
      "Iteration 252, loss = 0.33068580\n",
      "Iteration 253, loss = 0.33044505\n",
      "Iteration 254, loss = 0.33024447\n",
      "Iteration 255, loss = 0.33050189\n",
      "Iteration 256, loss = 0.33019223\n",
      "Iteration 257, loss = 0.33010411\n",
      "Iteration 258, loss = 0.33014938\n",
      "Iteration 259, loss = 0.32996145\n",
      "Iteration 260, loss = 0.32983360\n",
      "Iteration 261, loss = 0.32984625\n",
      "Iteration 262, loss = 0.32976859\n",
      "Iteration 263, loss = 0.32953899\n",
      "Iteration 264, loss = 0.32959078\n",
      "Iteration 265, loss = 0.32963102\n",
      "Iteration 266, loss = 0.32953044\n",
      "Iteration 267, loss = 0.32938818\n",
      "Iteration 268, loss = 0.32931922\n",
      "Iteration 269, loss = 0.32920536\n",
      "Iteration 270, loss = 0.32934745\n",
      "Iteration 271, loss = 0.32910419\n",
      "Iteration 272, loss = 0.32911962\n",
      "Iteration 273, loss = 0.32887277\n",
      "Iteration 274, loss = 0.32890583\n",
      "Iteration 275, loss = 0.32886763\n",
      "Iteration 276, loss = 0.32867580\n",
      "Iteration 277, loss = 0.32877146\n",
      "Iteration 278, loss = 0.32861886\n",
      "Iteration 279, loss = 0.32848843\n",
      "Iteration 280, loss = 0.32847707\n",
      "Iteration 281, loss = 0.32850757\n",
      "Iteration 282, loss = 0.32850094\n",
      "Iteration 283, loss = 0.32825333\n",
      "Iteration 284, loss = 0.32824106\n",
      "Iteration 285, loss = 0.32816669\n",
      "Iteration 286, loss = 0.32825521\n",
      "Iteration 287, loss = 0.32813194\n",
      "Iteration 288, loss = 0.32816763\n",
      "Iteration 289, loss = 0.32800204\n",
      "Iteration 290, loss = 0.32776985\n",
      "Iteration 291, loss = 0.32777831\n",
      "Iteration 292, loss = 0.32769953\n",
      "Iteration 293, loss = 0.32765615\n",
      "Iteration 294, loss = 0.32766996\n",
      "Iteration 295, loss = 0.32766219\n",
      "Iteration 296, loss = 0.32757938\n",
      "Iteration 297, loss = 0.32747234\n",
      "Iteration 298, loss = 0.32737168\n",
      "Iteration 299, loss = 0.32741571\n",
      "Iteration 300, loss = 0.32726588\n",
      "Iteration 1, loss = 0.49825320\n",
      "Iteration 2, loss = 0.39289127\n",
      "Iteration 3, loss = 0.36996266\n",
      "Iteration 4, loss = 0.35973032\n",
      "Iteration 5, loss = 0.35410431\n",
      "Iteration 6, loss = 0.34997211\n",
      "Iteration 7, loss = 0.34731359\n",
      "Iteration 8, loss = 0.34518130\n",
      "Iteration 9, loss = 0.34338341\n",
      "Iteration 10, loss = 0.34158647\n",
      "Iteration 11, loss = 0.34110960\n",
      "Iteration 12, loss = 0.33974364\n",
      "Iteration 13, loss = 0.33858253\n",
      "Iteration 14, loss = 0.33789763\n",
      "Iteration 15, loss = 0.33708279\n",
      "Iteration 16, loss = 0.33640220\n",
      "Iteration 17, loss = 0.33578181\n",
      "Iteration 18, loss = 0.33519330\n",
      "Iteration 19, loss = 0.33435492\n",
      "Iteration 20, loss = 0.33367376\n",
      "Iteration 21, loss = 0.33314117\n",
      "Iteration 22, loss = 0.33266577\n",
      "Iteration 23, loss = 0.33213679\n",
      "Iteration 24, loss = 0.33187969\n",
      "Iteration 25, loss = 0.33080080\n",
      "Iteration 26, loss = 0.33047000\n",
      "Iteration 27, loss = 0.32951753\n",
      "Iteration 28, loss = 0.32879450\n",
      "Iteration 29, loss = 0.32832789\n",
      "Iteration 30, loss = 0.32797988\n",
      "Iteration 31, loss = 0.32749473\n",
      "Iteration 32, loss = 0.32714818\n",
      "Iteration 33, loss = 0.32656866\n",
      "Iteration 34, loss = 0.32644693\n",
      "Iteration 35, loss = 0.32568402\n",
      "Iteration 36, loss = 0.32561041\n",
      "Iteration 37, loss = 0.32513735\n",
      "Iteration 38, loss = 0.32469471\n",
      "Iteration 39, loss = 0.32430780\n",
      "Iteration 40, loss = 0.32430518\n",
      "Iteration 41, loss = 0.32373050\n",
      "Iteration 42, loss = 0.32323644\n",
      "Iteration 43, loss = 0.32325064\n",
      "Iteration 44, loss = 0.32287460\n",
      "Iteration 45, loss = 0.32252779\n",
      "Iteration 46, loss = 0.32241052\n",
      "Iteration 47, loss = 0.32229741\n",
      "Iteration 48, loss = 0.32201030\n",
      "Iteration 49, loss = 0.32179306\n",
      "Iteration 50, loss = 0.32145865\n",
      "Iteration 51, loss = 0.32125229\n",
      "Iteration 52, loss = 0.32135631\n",
      "Iteration 53, loss = 0.32102208\n",
      "Iteration 54, loss = 0.32081484\n",
      "Iteration 55, loss = 0.32026708\n",
      "Iteration 56, loss = 0.32038619\n",
      "Iteration 57, loss = 0.32035780\n",
      "Iteration 58, loss = 0.32024581\n",
      "Iteration 59, loss = 0.32004934\n",
      "Iteration 60, loss = 0.32007126\n",
      "Iteration 61, loss = 0.31929584\n",
      "Iteration 62, loss = 0.31933585\n",
      "Iteration 63, loss = 0.31903409\n",
      "Iteration 64, loss = 0.31887833\n",
      "Iteration 65, loss = 0.31846888\n",
      "Iteration 66, loss = 0.31826910\n",
      "Iteration 67, loss = 0.31821413\n",
      "Iteration 68, loss = 0.31833642\n",
      "Iteration 69, loss = 0.31813335\n",
      "Iteration 70, loss = 0.31741478\n",
      "Iteration 71, loss = 0.31768053\n",
      "Iteration 72, loss = 0.31751443\n",
      "Iteration 73, loss = 0.31731134\n",
      "Iteration 74, loss = 0.31727201\n",
      "Iteration 75, loss = 0.31708553\n",
      "Iteration 76, loss = 0.31716190\n",
      "Iteration 77, loss = 0.31665076\n",
      "Iteration 78, loss = 0.31669724\n",
      "Iteration 79, loss = 0.31624592\n",
      "Iteration 80, loss = 0.31645405\n",
      "Iteration 81, loss = 0.31621953\n",
      "Iteration 82, loss = 0.31639771\n",
      "Iteration 83, loss = 0.31595444\n",
      "Iteration 84, loss = 0.31557064\n",
      "Iteration 85, loss = 0.31535536\n",
      "Iteration 86, loss = 0.31559340\n",
      "Iteration 87, loss = 0.31513527\n",
      "Iteration 88, loss = 0.31543188\n",
      "Iteration 89, loss = 0.31503692\n",
      "Iteration 90, loss = 0.31485891\n",
      "Iteration 91, loss = 0.31462650\n",
      "Iteration 92, loss = 0.31458897\n",
      "Iteration 93, loss = 0.31467571\n",
      "Iteration 94, loss = 0.31447775\n",
      "Iteration 95, loss = 0.31386892\n",
      "Iteration 96, loss = 0.31421432\n",
      "Iteration 97, loss = 0.31388257\n",
      "Iteration 98, loss = 0.31399723\n",
      "Iteration 99, loss = 0.31369137\n",
      "Iteration 100, loss = 0.31359914\n",
      "Iteration 101, loss = 0.31353297\n",
      "Iteration 102, loss = 0.31334404\n",
      "Iteration 103, loss = 0.31324627\n",
      "Iteration 104, loss = 0.31312711\n",
      "Iteration 105, loss = 0.31303532\n",
      "Iteration 106, loss = 0.31297383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 0.37461783\n",
      "Iteration 38, loss = 0.37351878\n",
      "Iteration 39, loss = 0.37234627\n",
      "Iteration 40, loss = 0.37149119\n",
      "Iteration 41, loss = 0.37045994\n",
      "Iteration 42, loss = 0.36954752\n",
      "Iteration 43, loss = 0.36870732\n",
      "Iteration 44, loss = 0.36781391\n",
      "Iteration 45, loss = 0.36697454\n",
      "Iteration 46, loss = 0.36622966\n",
      "Iteration 47, loss = 0.36549643\n",
      "Iteration 48, loss = 0.36475688\n",
      "Iteration 49, loss = 0.36399957\n",
      "Iteration 50, loss = 0.36328215\n",
      "Iteration 51, loss = 0.36269524\n",
      "Iteration 52, loss = 0.36203304\n",
      "Iteration 53, loss = 0.36139561\n",
      "Iteration 54, loss = 0.36078175\n",
      "Iteration 55, loss = 0.36020856\n",
      "Iteration 56, loss = 0.35965520\n",
      "Iteration 57, loss = 0.35920780\n",
      "Iteration 58, loss = 0.35860019\n",
      "Iteration 59, loss = 0.35818077\n",
      "Iteration 60, loss = 0.35780487\n",
      "Iteration 61, loss = 0.35713066\n",
      "Iteration 62, loss = 0.35670663\n",
      "Iteration 63, loss = 0.35628245\n",
      "Iteration 64, loss = 0.35587962\n",
      "Iteration 65, loss = 0.35537927\n",
      "Iteration 66, loss = 0.35491714\n",
      "Iteration 67, loss = 0.35461741\n",
      "Iteration 68, loss = 0.35429175\n",
      "Iteration 69, loss = 0.35398123\n",
      "Iteration 70, loss = 0.35342137\n",
      "Iteration 71, loss = 0.35306114\n",
      "Iteration 72, loss = 0.35281905\n",
      "Iteration 73, loss = 0.35242666\n",
      "Iteration 74, loss = 0.35221960\n",
      "Iteration 75, loss = 0.35188000\n",
      "Iteration 76, loss = 0.35166950\n",
      "Iteration 77, loss = 0.35117186\n",
      "Iteration 78, loss = 0.35094540\n",
      "Iteration 79, loss = 0.35059498\n",
      "Iteration 80, loss = 0.35038380\n",
      "Iteration 81, loss = 0.35003898\n",
      "Iteration 82, loss = 0.35003257\n",
      "Iteration 83, loss = 0.34961374\n",
      "Iteration 84, loss = 0.34932957\n",
      "Iteration 85, loss = 0.34897860\n",
      "Iteration 86, loss = 0.34888479\n",
      "Iteration 87, loss = 0.34848125\n",
      "Iteration 88, loss = 0.34848213\n",
      "Iteration 89, loss = 0.34810985\n",
      "Iteration 90, loss = 0.34786424\n",
      "Iteration 91, loss = 0.34758798\n",
      "Iteration 92, loss = 0.34747482\n",
      "Iteration 93, loss = 0.34734842\n",
      "Iteration 94, loss = 0.34709709\n",
      "Iteration 95, loss = 0.34679885\n",
      "Iteration 96, loss = 0.34671531\n",
      "Iteration 97, loss = 0.34633570\n",
      "Iteration 98, loss = 0.34623477\n",
      "Iteration 99, loss = 0.34600488\n",
      "Iteration 100, loss = 0.34594194\n",
      "Iteration 101, loss = 0.34576054\n",
      "Iteration 102, loss = 0.34552364\n",
      "Iteration 103, loss = 0.34538052\n",
      "Iteration 104, loss = 0.34518875\n",
      "Iteration 105, loss = 0.34513194\n",
      "Iteration 106, loss = 0.34494197\n",
      "Iteration 107, loss = 0.34460675\n",
      "Iteration 108, loss = 0.34442635\n",
      "Iteration 109, loss = 0.34436826\n",
      "Iteration 110, loss = 0.34418819\n",
      "Iteration 111, loss = 0.34409172\n",
      "Iteration 112, loss = 0.34385003\n",
      "Iteration 113, loss = 0.34375745\n",
      "Iteration 114, loss = 0.34351020\n",
      "Iteration 115, loss = 0.34333549\n",
      "Iteration 116, loss = 0.34328777\n",
      "Iteration 117, loss = 0.34307785\n",
      "Iteration 118, loss = 0.34296493\n",
      "Iteration 119, loss = 0.34276795\n",
      "Iteration 120, loss = 0.34267020\n",
      "Iteration 121, loss = 0.34246510\n",
      "Iteration 122, loss = 0.34234821\n",
      "Iteration 123, loss = 0.34222448\n",
      "Iteration 124, loss = 0.34217559\n",
      "Iteration 125, loss = 0.34199894\n",
      "Iteration 126, loss = 0.34197969\n",
      "Iteration 127, loss = 0.34181494\n",
      "Iteration 128, loss = 0.34159828\n",
      "Iteration 129, loss = 0.34145873\n",
      "Iteration 130, loss = 0.34145222\n",
      "Iteration 131, loss = 0.34125325\n",
      "Iteration 132, loss = 0.34121503\n",
      "Iteration 133, loss = 0.34100241\n",
      "Iteration 134, loss = 0.34097339\n",
      "Iteration 135, loss = 0.34076889\n",
      "Iteration 136, loss = 0.34061939\n",
      "Iteration 137, loss = 0.34058047\n",
      "Iteration 138, loss = 0.34046164\n",
      "Iteration 139, loss = 0.34039157\n",
      "Iteration 140, loss = 0.34017161\n",
      "Iteration 141, loss = 0.34026941\n",
      "Iteration 142, loss = 0.33994995\n",
      "Iteration 143, loss = 0.33995325\n",
      "Iteration 144, loss = 0.33984701\n",
      "Iteration 145, loss = 0.33963783\n",
      "Iteration 146, loss = 0.33954184\n",
      "Iteration 147, loss = 0.33942530\n",
      "Iteration 148, loss = 0.33932320\n",
      "Iteration 149, loss = 0.33909751\n",
      "Iteration 150, loss = 0.33911222\n",
      "Iteration 151, loss = 0.33902491\n",
      "Iteration 152, loss = 0.33908178\n",
      "Iteration 153, loss = 0.33889714\n",
      "Iteration 154, loss = 0.33878191\n",
      "Iteration 155, loss = 0.33865980\n",
      "Iteration 156, loss = 0.33857479\n",
      "Iteration 157, loss = 0.33848341\n",
      "Iteration 158, loss = 0.33841256\n",
      "Iteration 159, loss = 0.33825253\n",
      "Iteration 160, loss = 0.33817325\n",
      "Iteration 161, loss = 0.33821034\n",
      "Iteration 162, loss = 0.33794774\n",
      "Iteration 163, loss = 0.33799810\n",
      "Iteration 164, loss = 0.33767806\n",
      "Iteration 165, loss = 0.33761246\n",
      "Iteration 166, loss = 0.33755671\n",
      "Iteration 167, loss = 0.33753739\n",
      "Iteration 168, loss = 0.33744493\n",
      "Iteration 169, loss = 0.33730502\n",
      "Iteration 170, loss = 0.33729360\n",
      "Iteration 171, loss = 0.33715038\n",
      "Iteration 172, loss = 0.33703869\n",
      "Iteration 173, loss = 0.33710880\n",
      "Iteration 174, loss = 0.33686287\n",
      "Iteration 175, loss = 0.33693825\n",
      "Iteration 176, loss = 0.33671577\n",
      "Iteration 177, loss = 0.33675385\n",
      "Iteration 178, loss = 0.33663180\n",
      "Iteration 179, loss = 0.33649345\n",
      "Iteration 180, loss = 0.33646910\n",
      "Iteration 181, loss = 0.33640895\n",
      "Iteration 182, loss = 0.33632235\n",
      "Iteration 183, loss = 0.33616108\n",
      "Iteration 184, loss = 0.33597261\n",
      "Iteration 185, loss = 0.33608691\n",
      "Iteration 186, loss = 0.33603002\n",
      "Iteration 187, loss = 0.33588988\n",
      "Iteration 188, loss = 0.33588257\n",
      "Iteration 189, loss = 0.33554247\n",
      "Iteration 190, loss = 0.33570158\n",
      "Iteration 191, loss = 0.33545776\n",
      "Iteration 192, loss = 0.33550101\n",
      "Iteration 193, loss = 0.33535299\n",
      "Iteration 194, loss = 0.33545356\n",
      "Iteration 195, loss = 0.33523000\n",
      "Iteration 196, loss = 0.33511131\n",
      "Iteration 197, loss = 0.33500279\n",
      "Iteration 198, loss = 0.33513217\n",
      "Iteration 199, loss = 0.33497197\n",
      "Iteration 200, loss = 0.33489802\n",
      "Iteration 201, loss = 0.33481105\n",
      "Iteration 202, loss = 0.33474456\n",
      "Iteration 203, loss = 0.33464121\n",
      "Iteration 204, loss = 0.33442584\n",
      "Iteration 205, loss = 0.33446421\n",
      "Iteration 206, loss = 0.33442167\n",
      "Iteration 207, loss = 0.33430426\n",
      "Iteration 208, loss = 0.33409123\n",
      "Iteration 209, loss = 0.33416862\n",
      "Iteration 210, loss = 0.33390642\n",
      "Iteration 211, loss = 0.33399894\n",
      "Iteration 212, loss = 0.33391124\n",
      "Iteration 213, loss = 0.33380190\n",
      "Iteration 214, loss = 0.33375367\n",
      "Iteration 215, loss = 0.33357471\n",
      "Iteration 216, loss = 0.33354389\n",
      "Iteration 217, loss = 0.33353126\n",
      "Iteration 218, loss = 0.33328191\n",
      "Iteration 219, loss = 0.33334153\n",
      "Iteration 220, loss = 0.33328426\n",
      "Iteration 221, loss = 0.33307991\n",
      "Iteration 222, loss = 0.33315944\n",
      "Iteration 223, loss = 0.33306517\n",
      "Iteration 224, loss = 0.33283706\n",
      "Iteration 225, loss = 0.33274176\n",
      "Iteration 226, loss = 0.33279705\n",
      "Iteration 227, loss = 0.33263148\n",
      "Iteration 228, loss = 0.33250278\n",
      "Iteration 229, loss = 0.33258420\n",
      "Iteration 230, loss = 0.33243050\n",
      "Iteration 231, loss = 0.33235994\n",
      "Iteration 232, loss = 0.33224913\n",
      "Iteration 233, loss = 0.33212995\n",
      "Iteration 234, loss = 0.33196181\n",
      "Iteration 235, loss = 0.33194637\n",
      "Iteration 236, loss = 0.33182423\n",
      "Iteration 237, loss = 0.33181867\n",
      "Iteration 238, loss = 0.33181309\n",
      "Iteration 239, loss = 0.33167905\n",
      "Iteration 240, loss = 0.33152820\n",
      "Iteration 241, loss = 0.33146008\n",
      "Iteration 242, loss = 0.33147531\n",
      "Iteration 243, loss = 0.33117027\n",
      "Iteration 244, loss = 0.33121590\n",
      "Iteration 245, loss = 0.33115447\n",
      "Iteration 246, loss = 0.33108727\n",
      "Iteration 247, loss = 0.33099419\n",
      "Iteration 248, loss = 0.33092561\n",
      "Iteration 249, loss = 0.33078305\n",
      "Iteration 250, loss = 0.33063121\n",
      "Iteration 251, loss = 0.33067082\n",
      "Iteration 252, loss = 0.33068580\n",
      "Iteration 253, loss = 0.33044505\n",
      "Iteration 254, loss = 0.33024447\n",
      "Iteration 255, loss = 0.33050189\n",
      "Iteration 256, loss = 0.33019223\n",
      "Iteration 257, loss = 0.33010411\n",
      "Iteration 258, loss = 0.33014938\n",
      "Iteration 259, loss = 0.32996145\n",
      "Iteration 260, loss = 0.32983360\n",
      "Iteration 261, loss = 0.32984625\n",
      "Iteration 262, loss = 0.32976859\n",
      "Iteration 263, loss = 0.32953899\n",
      "Iteration 264, loss = 0.32959078\n",
      "Iteration 265, loss = 0.32963102\n",
      "Iteration 266, loss = 0.32953044\n",
      "Iteration 267, loss = 0.32938818\n",
      "Iteration 268, loss = 0.32931922\n",
      "Iteration 269, loss = 0.32920536\n",
      "Iteration 270, loss = 0.32934745\n",
      "Iteration 271, loss = 0.32910419\n",
      "Iteration 272, loss = 0.32911962\n",
      "Iteration 273, loss = 0.32887277\n",
      "Iteration 274, loss = 0.32890583\n",
      "Iteration 275, loss = 0.32886763\n",
      "Iteration 276, loss = 0.32867580\n",
      "Iteration 277, loss = 0.32877146\n",
      "Iteration 278, loss = 0.32861886\n",
      "Iteration 279, loss = 0.32848843\n",
      "Iteration 280, loss = 0.32847707\n",
      "Iteration 281, loss = 0.32850757\n",
      "Iteration 282, loss = 0.32850094\n",
      "Iteration 283, loss = 0.32825333\n",
      "Iteration 284, loss = 0.32824106\n",
      "Iteration 285, loss = 0.32816669\n",
      "Iteration 286, loss = 0.32825521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72, loss = 0.31751443\n",
      "Iteration 73, loss = 0.31731134\n",
      "Iteration 74, loss = 0.31727201\n",
      "Iteration 75, loss = 0.31708553\n",
      "Iteration 76, loss = 0.31716190\n",
      "Iteration 77, loss = 0.31665076\n",
      "Iteration 78, loss = 0.31669724\n",
      "Iteration 79, loss = 0.31624592\n",
      "Iteration 80, loss = 0.31645405\n",
      "Iteration 81, loss = 0.31621953\n",
      "Iteration 82, loss = 0.31639771\n",
      "Iteration 83, loss = 0.31595444\n",
      "Iteration 84, loss = 0.31557064\n",
      "Iteration 85, loss = 0.31535536\n",
      "Iteration 86, loss = 0.31559340\n",
      "Iteration 87, loss = 0.31513527\n",
      "Iteration 88, loss = 0.31543188\n",
      "Iteration 89, loss = 0.31503692\n",
      "Iteration 90, loss = 0.31485891\n",
      "Iteration 91, loss = 0.31462650\n",
      "Iteration 92, loss = 0.31458897\n",
      "Iteration 93, loss = 0.31467571\n",
      "Iteration 94, loss = 0.31447775\n",
      "Iteration 95, loss = 0.31386892\n",
      "Iteration 96, loss = 0.31421432\n",
      "Iteration 97, loss = 0.31388257\n",
      "Iteration 98, loss = 0.31399723\n",
      "Iteration 99, loss = 0.31369137\n",
      "Iteration 100, loss = 0.31359914\n",
      "Iteration 101, loss = 0.31353297\n",
      "Iteration 102, loss = 0.31334404\n",
      "Iteration 103, loss = 0.31324627\n",
      "Iteration 104, loss = 0.31312711\n",
      "Iteration 105, loss = 0.31303532\n",
      "Iteration 106, loss = 0.31297383\n",
      "Iteration 107, loss = 0.31266644\n",
      "Iteration 108, loss = 0.31247393\n",
      "Iteration 109, loss = 0.31260068\n",
      "Iteration 110, loss = 0.31249015\n",
      "Iteration 111, loss = 0.31260805\n",
      "Iteration 112, loss = 0.31222928\n",
      "Iteration 113, loss = 0.31203132\n",
      "Iteration 114, loss = 0.31151214\n",
      "Iteration 115, loss = 0.31172691\n",
      "Iteration 116, loss = 0.31169700\n",
      "Iteration 117, loss = 0.31158565\n",
      "Iteration 118, loss = 0.31106325\n",
      "Iteration 119, loss = 0.31116905\n",
      "Iteration 120, loss = 0.31115543\n",
      "Iteration 121, loss = 0.31087272\n",
      "Iteration 122, loss = 0.31090630\n",
      "Iteration 123, loss = 0.31070668\n",
      "Iteration 124, loss = 0.31087891\n",
      "Iteration 125, loss = 0.31062629\n",
      "Iteration 126, loss = 0.31063076\n",
      "Iteration 127, loss = 0.31058083\n",
      "Iteration 128, loss = 0.31011699\n",
      "Iteration 129, loss = 0.31011877\n",
      "Iteration 130, loss = 0.31029715\n",
      "Iteration 131, loss = 0.30999920\n",
      "Iteration 132, loss = 0.30986160\n",
      "Iteration 133, loss = 0.30975063\n",
      "Iteration 134, loss = 0.30976748\n",
      "Iteration 135, loss = 0.30942223\n",
      "Iteration 136, loss = 0.30940170\n",
      "Iteration 137, loss = 0.30945445\n",
      "Iteration 138, loss = 0.30918746\n",
      "Iteration 139, loss = 0.30912187\n",
      "Iteration 140, loss = 0.30877860\n",
      "Iteration 141, loss = 0.30905128\n",
      "Iteration 142, loss = 0.30851108\n",
      "Iteration 143, loss = 0.30885323\n",
      "Iteration 144, loss = 0.30861678\n",
      "Iteration 145, loss = 0.30828420\n",
      "Iteration 146, loss = 0.30825743\n",
      "Iteration 147, loss = 0.30818892\n",
      "Iteration 148, loss = 0.30800206\n",
      "Iteration 149, loss = 0.30756614\n",
      "Iteration 150, loss = 0.30778273\n",
      "Iteration 151, loss = 0.30769967\n",
      "Iteration 152, loss = 0.30773056\n",
      "Iteration 153, loss = 0.30746575\n",
      "Iteration 154, loss = 0.30758079\n",
      "Iteration 155, loss = 0.30724709\n",
      "Iteration 156, loss = 0.30728780\n",
      "Iteration 157, loss = 0.30716718\n",
      "Iteration 158, loss = 0.30702189\n",
      "Iteration 159, loss = 0.30689924\n",
      "Iteration 160, loss = 0.30678309\n",
      "Iteration 161, loss = 0.30711164\n",
      "Iteration 162, loss = 0.30662353\n",
      "Iteration 163, loss = 0.30694330\n",
      "Iteration 164, loss = 0.30612069\n",
      "Iteration 165, loss = 0.30626078\n",
      "Iteration 166, loss = 0.30608584\n",
      "Iteration 167, loss = 0.30606202\n",
      "Iteration 168, loss = 0.30607736\n",
      "Iteration 169, loss = 0.30559690\n",
      "Iteration 170, loss = 0.30591418\n",
      "Iteration 171, loss = 0.30567258\n",
      "Iteration 172, loss = 0.30547802\n",
      "Iteration 173, loss = 0.30558780\n",
      "Iteration 174, loss = 0.30528942\n",
      "Iteration 175, loss = 0.30545993\n",
      "Iteration 176, loss = 0.30518783\n",
      "Iteration 177, loss = 0.30529681\n",
      "Iteration 178, loss = 0.30509426\n",
      "Iteration 179, loss = 0.30479586\n",
      "Iteration 180, loss = 0.30510635\n",
      "Iteration 181, loss = 0.30491829\n",
      "Iteration 182, loss = 0.30448256\n",
      "Iteration 183, loss = 0.30449702\n",
      "Iteration 184, loss = 0.30405100\n",
      "Iteration 185, loss = 0.30436153\n",
      "Iteration 186, loss = 0.30417559\n",
      "Iteration 187, loss = 0.30414778\n",
      "Iteration 188, loss = 0.30444434\n",
      "Iteration 189, loss = 0.30378015\n",
      "Iteration 190, loss = 0.30394914\n",
      "Iteration 191, loss = 0.30354493\n",
      "Iteration 192, loss = 0.30376459\n",
      "Iteration 193, loss = 0.30327701\n",
      "Iteration 194, loss = 0.30376818\n",
      "Iteration 195, loss = 0.30320497\n",
      "Iteration 196, loss = 0.30333296\n",
      "Iteration 197, loss = 0.30293034\n",
      "Iteration 198, loss = 0.30324189\n",
      "Iteration 199, loss = 0.30291194\n",
      "Iteration 200, loss = 0.30287239\n",
      "Iteration 201, loss = 0.30300973\n",
      "Iteration 202, loss = 0.30270264\n",
      "Iteration 203, loss = 0.30253183\n",
      "Iteration 204, loss = 0.30220485\n",
      "Iteration 205, loss = 0.30228660\n",
      "Iteration 206, loss = 0.30247168\n",
      "Iteration 207, loss = 0.30217215\n",
      "Iteration 208, loss = 0.30177125\n",
      "Iteration 209, loss = 0.30185670\n",
      "Iteration 210, loss = 0.30167170\n",
      "Iteration 211, loss = 0.30194787\n",
      "Iteration 212, loss = 0.30167309\n",
      "Iteration 213, loss = 0.30150112\n",
      "Iteration 214, loss = 0.30155955\n",
      "Iteration 215, loss = 0.30128920\n",
      "Iteration 216, loss = 0.30101251\n",
      "Iteration 217, loss = 0.30110581\n",
      "Iteration 218, loss = 0.30082286\n",
      "Iteration 219, loss = 0.30104074\n",
      "Iteration 220, loss = 0.30095545\n",
      "Iteration 221, loss = 0.30055973\n",
      "Iteration 222, loss = 0.30089134\n",
      "Iteration 223, loss = 0.30062968\n",
      "Iteration 224, loss = 0.30037992\n",
      "Iteration 225, loss = 0.30017744\n",
      "Iteration 226, loss = 0.30044384\n",
      "Iteration 227, loss = 0.30016499\n",
      "Iteration 228, loss = 0.29999350\n",
      "Iteration 229, loss = 0.30021691\n",
      "Iteration 230, loss = 0.29997576\n",
      "Iteration 231, loss = 0.29985230\n",
      "Iteration 232, loss = 0.29981418\n",
      "Iteration 233, loss = 0.29948127\n",
      "Iteration 234, loss = 0.29938343\n",
      "Iteration 235, loss = 0.29937269\n",
      "Iteration 236, loss = 0.29929083\n",
      "Iteration 237, loss = 0.29972112\n",
      "Iteration 238, loss = 0.29948002\n",
      "Iteration 239, loss = 0.29914733\n",
      "Iteration 240, loss = 0.29897987\n",
      "Iteration 241, loss = 0.29895865\n",
      "Iteration 242, loss = 0.29898550\n",
      "Iteration 243, loss = 0.29858526\n",
      "Iteration 244, loss = 0.29886910\n",
      "Iteration 245, loss = 0.29876554\n",
      "Iteration 246, loss = 0.29857379\n",
      "Iteration 247, loss = 0.29860026\n",
      "Iteration 248, loss = 0.29857330\n",
      "Iteration 249, loss = 0.29818179\n",
      "Iteration 250, loss = 0.29798528\n",
      "Iteration 251, loss = 0.29812347\n",
      "Iteration 252, loss = 0.29830228\n",
      "Iteration 253, loss = 0.29789137\n",
      "Iteration 254, loss = 0.29777755\n",
      "Iteration 255, loss = 0.29797133\n",
      "Iteration 256, loss = 0.29779972\n",
      "Iteration 257, loss = 0.29749390\n",
      "Iteration 258, loss = 0.29765661\n",
      "Iteration 259, loss = 0.29740627\n",
      "Iteration 260, loss = 0.29721576\n",
      "Iteration 261, loss = 0.29745140\n",
      "Iteration 262, loss = 0.29726967\n",
      "Iteration 263, loss = 0.29696993\n",
      "Iteration 264, loss = 0.29704037\n",
      "Iteration 265, loss = 0.29729837\n",
      "Iteration 266, loss = 0.29699883\n",
      "Iteration 267, loss = 0.29681961\n",
      "Iteration 268, loss = 0.29692602\n",
      "Iteration 269, loss = 0.29672112\n",
      "Iteration 270, loss = 0.29693833\n",
      "Iteration 271, loss = 0.29649029\n",
      "Iteration 272, loss = 0.29673870\n",
      "Iteration 273, loss = 0.29650256\n",
      "Iteration 274, loss = 0.29632270\n",
      "Iteration 275, loss = 0.29643164\n",
      "Iteration 276, loss = 0.29611993\n",
      "Iteration 277, loss = 0.29627591\n",
      "Iteration 278, loss = 0.29603294\n",
      "Iteration 279, loss = 0.29578815\n",
      "Iteration 280, loss = 0.29600212\n",
      "Iteration 281, loss = 0.29598374\n",
      "Iteration 282, loss = 0.29572151\n",
      "Iteration 283, loss = 0.29538452\n",
      "Iteration 284, loss = 0.29572660\n",
      "Iteration 285, loss = 0.29559154\n",
      "Iteration 286, loss = 0.29571761\n",
      "Iteration 287, loss = 0.29564867\n",
      "Iteration 288, loss = 0.29560192\n",
      "Iteration 289, loss = 0.29525850\n",
      "Iteration 290, loss = 0.29496180\n",
      "Iteration 291, loss = 0.29492321\n",
      "Iteration 292, loss = 0.29494179\n",
      "Iteration 293, loss = 0.29477127\n",
      "Iteration 294, loss = 0.29471637\n",
      "Iteration 295, loss = 0.29480000\n",
      "Iteration 296, loss = 0.29476176\n",
      "Iteration 297, loss = 0.29471788\n",
      "Iteration 298, loss = 0.29453410\n",
      "Iteration 299, loss = 0.29424662\n",
      "Iteration 300, loss = 0.29420667\n",
      "Iteration 1, loss = 0.64759182\n",
      "Iteration 2, loss = 0.56286254\n",
      "Iteration 3, loss = 0.47276083\n",
      "Iteration 4, loss = 0.41837977\n",
      "Iteration 5, loss = 0.39727359\n",
      "Iteration 6, loss = 0.38860096\n",
      "Iteration 7, loss = 0.38385821\n",
      "Iteration 8, loss = 0.38051173\n",
      "Iteration 9, loss = 0.37799647\n",
      "Iteration 10, loss = 0.37608736\n",
      "Iteration 11, loss = 0.37488073\n",
      "Iteration 12, loss = 0.37345581\n",
      "Iteration 13, loss = 0.37256526\n",
      "Iteration 14, loss = 0.37144393\n",
      "Iteration 15, loss = 0.37077600\n",
      "Iteration 16, loss = 0.36960796\n",
      "Iteration 17, loss = 0.36889257\n",
      "Iteration 18, loss = 0.36757018\n",
      "Iteration 19, loss = 0.36697640\n",
      "Iteration 20, loss = 0.36640522\n",
      "Iteration 21, loss = 0.36587161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 116, loss = 0.31249222\n",
      "Iteration 117, loss = 0.31196467\n",
      "Iteration 118, loss = 0.31141109\n",
      "Iteration 119, loss = 0.31158020\n",
      "Iteration 120, loss = 0.31164469\n",
      "Iteration 121, loss = 0.31154876\n",
      "Iteration 122, loss = 0.31110570\n",
      "Iteration 123, loss = 0.31086531\n",
      "Iteration 124, loss = 0.31089524\n",
      "Iteration 125, loss = 0.31092037\n",
      "Iteration 126, loss = 0.31053366\n",
      "Iteration 127, loss = 0.31054480\n",
      "Iteration 128, loss = 0.31042617\n",
      "Iteration 129, loss = 0.31051928\n",
      "Iteration 130, loss = 0.31042666\n",
      "Iteration 131, loss = 0.31001182\n",
      "Iteration 132, loss = 0.31003630\n",
      "Iteration 133, loss = 0.30979987\n",
      "Iteration 134, loss = 0.30950395\n",
      "Iteration 135, loss = 0.30960451\n",
      "Iteration 136, loss = 0.30946620\n",
      "Iteration 137, loss = 0.30939964\n",
      "Iteration 138, loss = 0.30908222\n",
      "Iteration 139, loss = 0.30927306\n",
      "Iteration 140, loss = 0.30875772\n",
      "Iteration 141, loss = 0.30883996\n",
      "Iteration 142, loss = 0.30838597\n",
      "Iteration 143, loss = 0.30853086\n",
      "Iteration 144, loss = 0.30840206\n",
      "Iteration 145, loss = 0.30805454\n",
      "Iteration 146, loss = 0.30819720\n",
      "Iteration 147, loss = 0.30812933\n",
      "Iteration 148, loss = 0.30796818\n",
      "Iteration 149, loss = 0.30726498\n",
      "Iteration 150, loss = 0.30756838\n",
      "Iteration 151, loss = 0.30755117\n",
      "Iteration 152, loss = 0.30731416\n",
      "Iteration 153, loss = 0.30710015\n",
      "Iteration 154, loss = 0.30711559\n",
      "Iteration 155, loss = 0.30693443\n",
      "Iteration 156, loss = 0.30673469\n",
      "Iteration 157, loss = 0.30675971\n",
      "Iteration 158, loss = 0.30656999\n",
      "Iteration 159, loss = 0.30651322\n",
      "Iteration 160, loss = 0.30618707\n",
      "Iteration 161, loss = 0.30621253\n",
      "Iteration 162, loss = 0.30591586\n",
      "Iteration 163, loss = 0.30604615\n",
      "Iteration 164, loss = 0.30546608\n",
      "Iteration 165, loss = 0.30569390\n",
      "Iteration 166, loss = 0.30537663\n",
      "Iteration 167, loss = 0.30515467\n",
      "Iteration 168, loss = 0.30489459\n",
      "Iteration 169, loss = 0.30499301\n",
      "Iteration 170, loss = 0.30488959\n",
      "Iteration 171, loss = 0.30482830\n",
      "Iteration 172, loss = 0.30446399\n",
      "Iteration 173, loss = 0.30481879\n",
      "Iteration 174, loss = 0.30435623\n",
      "Iteration 175, loss = 0.30451431\n",
      "Iteration 176, loss = 0.30424050\n",
      "Iteration 177, loss = 0.30433542\n",
      "Iteration 178, loss = 0.30385751\n",
      "Iteration 179, loss = 0.30368520\n",
      "Iteration 180, loss = 0.30340388\n",
      "Iteration 181, loss = 0.30357914\n",
      "Iteration 182, loss = 0.30324181\n",
      "Iteration 183, loss = 0.30314507\n",
      "Iteration 184, loss = 0.30297051\n",
      "Iteration 185, loss = 0.30293003\n",
      "Iteration 186, loss = 0.30298763\n",
      "Iteration 187, loss = 0.30270846\n",
      "Iteration 188, loss = 0.30277885\n",
      "Iteration 189, loss = 0.30214386\n",
      "Iteration 190, loss = 0.30236455\n",
      "Iteration 191, loss = 0.30199944\n",
      "Iteration 192, loss = 0.30206674\n",
      "Iteration 193, loss = 0.30195097\n",
      "Iteration 194, loss = 0.30224439\n",
      "Iteration 195, loss = 0.30162330\n",
      "Iteration 196, loss = 0.30182853\n",
      "Iteration 197, loss = 0.30141743\n",
      "Iteration 198, loss = 0.30195220\n",
      "Iteration 199, loss = 0.30119160\n",
      "Iteration 200, loss = 0.30135581\n",
      "Iteration 201, loss = 0.30097237\n",
      "Iteration 202, loss = 0.30086495\n",
      "Iteration 203, loss = 0.30069304\n",
      "Iteration 204, loss = 0.30066594\n",
      "Iteration 205, loss = 0.30035249\n",
      "Iteration 206, loss = 0.30048564\n",
      "Iteration 207, loss = 0.30025380\n",
      "Iteration 208, loss = 0.30012594\n",
      "Iteration 209, loss = 0.29986739\n",
      "Iteration 210, loss = 0.29993317\n",
      "Iteration 211, loss = 0.29987661\n",
      "Iteration 212, loss = 0.29969576\n",
      "Iteration 213, loss = 0.29958474\n",
      "Iteration 214, loss = 0.29934611\n",
      "Iteration 215, loss = 0.29920407\n",
      "Iteration 216, loss = 0.29947927\n",
      "Iteration 217, loss = 0.29926831\n",
      "Iteration 218, loss = 0.29888819\n",
      "Iteration 219, loss = 0.29886504\n",
      "Iteration 220, loss = 0.29894406\n",
      "Iteration 221, loss = 0.29852550\n",
      "Iteration 222, loss = 0.29863800\n",
      "Iteration 223, loss = 0.29906275\n",
      "Iteration 224, loss = 0.29858353\n",
      "Iteration 225, loss = 0.29827981\n",
      "Iteration 226, loss = 0.29808763\n",
      "Iteration 227, loss = 0.29795437\n",
      "Iteration 228, loss = 0.29777531\n",
      "Iteration 229, loss = 0.29795726\n",
      "Iteration 230, loss = 0.29797153\n",
      "Iteration 231, loss = 0.29771320\n",
      "Iteration 232, loss = 0.29764571\n",
      "Iteration 233, loss = 0.29753357\n",
      "Iteration 234, loss = 0.29710760\n",
      "Iteration 235, loss = 0.29749414\n",
      "Iteration 236, loss = 0.29716998\n",
      "Iteration 237, loss = 0.29721230\n",
      "Iteration 238, loss = 0.29690416\n",
      "Iteration 239, loss = 0.29728340\n",
      "Iteration 240, loss = 0.29691572\n",
      "Iteration 241, loss = 0.29708704\n",
      "Iteration 242, loss = 0.29683814\n",
      "Iteration 243, loss = 0.29662618\n",
      "Iteration 244, loss = 0.29628130\n",
      "Iteration 245, loss = 0.29651337\n",
      "Iteration 246, loss = 0.29611133\n",
      "Iteration 247, loss = 0.29647228\n",
      "Iteration 248, loss = 0.29630176\n",
      "Iteration 249, loss = 0.29595914\n",
      "Iteration 250, loss = 0.29577263\n",
      "Iteration 251, loss = 0.29586609\n",
      "Iteration 252, loss = 0.29568806\n",
      "Iteration 253, loss = 0.29554718\n",
      "Iteration 254, loss = 0.29550427\n",
      "Iteration 255, loss = 0.29564255\n",
      "Iteration 256, loss = 0.29513472\n",
      "Iteration 257, loss = 0.29543448\n",
      "Iteration 258, loss = 0.29551491\n",
      "Iteration 259, loss = 0.29516410\n",
      "Iteration 260, loss = 0.29513971\n",
      "Iteration 261, loss = 0.29505761\n",
      "Iteration 262, loss = 0.29472684\n",
      "Iteration 263, loss = 0.29496321\n",
      "Iteration 264, loss = 0.29460648\n",
      "Iteration 265, loss = 0.29477032\n",
      "Iteration 266, loss = 0.29465952\n",
      "Iteration 267, loss = 0.29456752\n",
      "Iteration 268, loss = 0.29431561\n",
      "Iteration 269, loss = 0.29409605\n",
      "Iteration 270, loss = 0.29423078\n",
      "Iteration 271, loss = 0.29415444\n",
      "Iteration 272, loss = 0.29435901\n",
      "Iteration 273, loss = 0.29410417\n",
      "Iteration 274, loss = 0.29376973\n",
      "Iteration 275, loss = 0.29395257\n",
      "Iteration 276, loss = 0.29377176\n",
      "Iteration 277, loss = 0.29355691\n",
      "Iteration 278, loss = 0.29358424\n",
      "Iteration 279, loss = 0.29356096\n",
      "Iteration 280, loss = 0.29343659\n",
      "Iteration 281, loss = 0.29363120\n",
      "Iteration 282, loss = 0.29347629\n",
      "Iteration 283, loss = 0.29320441\n",
      "Iteration 284, loss = 0.29328156\n",
      "Iteration 285, loss = 0.29315073\n",
      "Iteration 286, loss = 0.29314771\n",
      "Iteration 287, loss = 0.29283206\n",
      "Iteration 288, loss = 0.29320878\n",
      "Iteration 289, loss = 0.29288010\n",
      "Iteration 290, loss = 0.29259551\n",
      "Iteration 291, loss = 0.29276808\n",
      "Iteration 292, loss = 0.29242576\n",
      "Iteration 293, loss = 0.29229191\n",
      "Iteration 294, loss = 0.29255318\n",
      "Iteration 295, loss = 0.29263991\n",
      "Iteration 296, loss = 0.29237566\n",
      "Iteration 297, loss = 0.29222631\n",
      "Iteration 298, loss = 0.29206613\n",
      "Iteration 299, loss = 0.29217167\n",
      "Iteration 300, loss = 0.29202353\n",
      "Iteration 1, loss = 0.64778531\n",
      "Iteration 2, loss = 0.56276947\n",
      "Iteration 3, loss = 0.47263363\n",
      "Iteration 4, loss = 0.41883679\n",
      "Iteration 5, loss = 0.39824841\n",
      "Iteration 6, loss = 0.38945414\n",
      "Iteration 7, loss = 0.38461294\n",
      "Iteration 8, loss = 0.38149727\n",
      "Iteration 9, loss = 0.37861380\n",
      "Iteration 10, loss = 0.37704818\n",
      "Iteration 11, loss = 0.37561795\n",
      "Iteration 12, loss = 0.37409057\n",
      "Iteration 13, loss = 0.37287508\n",
      "Iteration 14, loss = 0.37211181\n",
      "Iteration 15, loss = 0.37080635\n",
      "Iteration 16, loss = 0.37035542\n",
      "Iteration 17, loss = 0.36911106\n",
      "Iteration 18, loss = 0.36809234\n",
      "Iteration 19, loss = 0.36736731\n",
      "Iteration 20, loss = 0.36679712\n",
      "Iteration 21, loss = 0.36608099\n",
      "Iteration 22, loss = 0.36528871\n",
      "Iteration 23, loss = 0.36480581\n",
      "Iteration 24, loss = 0.36408206\n",
      "Iteration 25, loss = 0.36342445\n",
      "Iteration 26, loss = 0.36256736\n",
      "Iteration 27, loss = 0.36216833\n",
      "Iteration 28, loss = 0.36172808\n",
      "Iteration 29, loss = 0.36081722\n",
      "Iteration 30, loss = 0.36064161\n",
      "Iteration 31, loss = 0.35969615\n",
      "Iteration 32, loss = 0.35953937\n",
      "Iteration 33, loss = 0.35933509\n",
      "Iteration 34, loss = 0.35873667\n",
      "Iteration 35, loss = 0.35805670\n",
      "Iteration 36, loss = 0.35755513\n",
      "Iteration 37, loss = 0.35706620\n",
      "Iteration 38, loss = 0.35647926\n",
      "Iteration 39, loss = 0.35594871\n",
      "Iteration 40, loss = 0.35603484\n",
      "Iteration 41, loss = 0.35541670\n",
      "Iteration 42, loss = 0.35515881\n",
      "Iteration 43, loss = 0.35489434\n",
      "Iteration 44, loss = 0.35449791\n",
      "Iteration 45, loss = 0.35418469\n",
      "Iteration 46, loss = 0.35348440\n",
      "Iteration 47, loss = 0.35305684\n",
      "Iteration 48, loss = 0.35281747\n",
      "Iteration 49, loss = 0.35221988\n",
      "Iteration 50, loss = 0.35229016\n",
      "Iteration 51, loss = 0.35175043\n",
      "Iteration 52, loss = 0.35174901\n",
      "Iteration 53, loss = 0.35142082\n",
      "Iteration 54, loss = 0.35118100\n",
      "Iteration 55, loss = 0.35079908\n",
      "Iteration 56, loss = 0.35076665\n",
      "Iteration 57, loss = 0.35009604\n",
      "Iteration 58, loss = 0.35012290\n",
      "Iteration 59, loss = 0.34973452\n",
      "Iteration 60, loss = 0.34955463\n",
      "Iteration 61, loss = 0.34903202\n",
      "Iteration 62, loss = 0.34870335\n",
      "Iteration 63, loss = 0.34829118\n",
      "Iteration 64, loss = 0.34850417\n",
      "Iteration 65, loss = 0.34798674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 89, loss = 0.34936567\n",
      "Iteration 90, loss = 0.34935817\n",
      "Iteration 91, loss = 0.34891883\n",
      "Iteration 92, loss = 0.34883452\n",
      "Iteration 93, loss = 0.34852635\n",
      "Iteration 94, loss = 0.34849799\n",
      "Iteration 95, loss = 0.34814407\n",
      "Iteration 96, loss = 0.34780084\n",
      "Iteration 97, loss = 0.34757585\n",
      "Iteration 98, loss = 0.34751090\n",
      "Iteration 99, loss = 0.34726537\n",
      "Iteration 100, loss = 0.34715553\n",
      "Iteration 101, loss = 0.34700683\n",
      "Iteration 102, loss = 0.34687738\n",
      "Iteration 103, loss = 0.34644771\n",
      "Iteration 104, loss = 0.34652748\n",
      "Iteration 105, loss = 0.34618680\n",
      "Iteration 106, loss = 0.34603850\n",
      "Iteration 107, loss = 0.34583801\n",
      "Iteration 108, loss = 0.34572471\n",
      "Iteration 109, loss = 0.34548470\n",
      "Iteration 110, loss = 0.34534053\n",
      "Iteration 111, loss = 0.34525901\n",
      "Iteration 112, loss = 0.34508351\n",
      "Iteration 113, loss = 0.34498193\n",
      "Iteration 114, loss = 0.34473048\n",
      "Iteration 115, loss = 0.34454339\n",
      "Iteration 116, loss = 0.34448277\n",
      "Iteration 117, loss = 0.34429999\n",
      "Iteration 118, loss = 0.34400643\n",
      "Iteration 119, loss = 0.34395358\n",
      "Iteration 120, loss = 0.34382983\n",
      "Iteration 121, loss = 0.34374625\n",
      "Iteration 122, loss = 0.34349584\n",
      "Iteration 123, loss = 0.34328593\n",
      "Iteration 124, loss = 0.34331017\n",
      "Iteration 125, loss = 0.34317038\n",
      "Iteration 126, loss = 0.34297866\n",
      "Iteration 127, loss = 0.34289053\n",
      "Iteration 128, loss = 0.34276024\n",
      "Iteration 129, loss = 0.34271924\n",
      "Iteration 130, loss = 0.34267638\n",
      "Iteration 131, loss = 0.34244557\n",
      "Iteration 132, loss = 0.34235394\n",
      "Iteration 133, loss = 0.34210662\n",
      "Iteration 134, loss = 0.34196996\n",
      "Iteration 135, loss = 0.34191595\n",
      "Iteration 136, loss = 0.34175035\n",
      "Iteration 137, loss = 0.34173615\n",
      "Iteration 138, loss = 0.34163829\n",
      "Iteration 139, loss = 0.34158208\n",
      "Iteration 140, loss = 0.34127171\n",
      "Iteration 141, loss = 0.34127973\n",
      "Iteration 142, loss = 0.34102581\n",
      "Iteration 143, loss = 0.34098914\n",
      "Iteration 144, loss = 0.34089658\n",
      "Iteration 145, loss = 0.34071586\n",
      "Iteration 146, loss = 0.34062943\n",
      "Iteration 147, loss = 0.34060132\n",
      "Iteration 148, loss = 0.34038191\n",
      "Iteration 149, loss = 0.34023285\n",
      "Iteration 150, loss = 0.34014631\n",
      "Iteration 151, loss = 0.34019775\n",
      "Iteration 152, loss = 0.34002935\n",
      "Iteration 153, loss = 0.33995555\n",
      "Iteration 154, loss = 0.33982777\n",
      "Iteration 155, loss = 0.33967037\n",
      "Iteration 156, loss = 0.33961596\n",
      "Iteration 157, loss = 0.33959382\n",
      "Iteration 158, loss = 0.33939995\n",
      "Iteration 159, loss = 0.33937009\n",
      "Iteration 160, loss = 0.33926251\n",
      "Iteration 161, loss = 0.33914310\n",
      "Iteration 162, loss = 0.33900870\n",
      "Iteration 163, loss = 0.33891775\n",
      "Iteration 164, loss = 0.33870212\n",
      "Iteration 165, loss = 0.33872430\n",
      "Iteration 166, loss = 0.33857349\n",
      "Iteration 167, loss = 0.33835747\n",
      "Iteration 168, loss = 0.33826044\n",
      "Iteration 169, loss = 0.33831052\n",
      "Iteration 170, loss = 0.33809112\n",
      "Iteration 171, loss = 0.33818645\n",
      "Iteration 172, loss = 0.33789179\n",
      "Iteration 173, loss = 0.33796274\n",
      "Iteration 174, loss = 0.33774396\n",
      "Iteration 175, loss = 0.33777199\n",
      "Iteration 176, loss = 0.33767553\n",
      "Iteration 177, loss = 0.33756893\n",
      "Iteration 178, loss = 0.33741470\n",
      "Iteration 179, loss = 0.33723166\n",
      "Iteration 180, loss = 0.33717151\n",
      "Iteration 181, loss = 0.33713609\n",
      "Iteration 182, loss = 0.33704695\n",
      "Iteration 183, loss = 0.33687282\n",
      "Iteration 184, loss = 0.33680482\n",
      "Iteration 185, loss = 0.33670757\n",
      "Iteration 186, loss = 0.33663875\n",
      "Iteration 187, loss = 0.33647226\n",
      "Iteration 188, loss = 0.33651546\n",
      "Iteration 189, loss = 0.33622980\n",
      "Iteration 190, loss = 0.33630480\n",
      "Iteration 191, loss = 0.33605360\n",
      "Iteration 192, loss = 0.33611265\n",
      "Iteration 193, loss = 0.33594129\n",
      "Iteration 194, loss = 0.33606511\n",
      "Iteration 195, loss = 0.33581658\n",
      "Iteration 196, loss = 0.33572463\n",
      "Iteration 197, loss = 0.33551731\n",
      "Iteration 198, loss = 0.33565224\n",
      "Iteration 199, loss = 0.33538331\n",
      "Iteration 200, loss = 0.33540469\n",
      "Iteration 201, loss = 0.33512980\n",
      "Iteration 202, loss = 0.33507712\n",
      "Iteration 203, loss = 0.33488958\n",
      "Iteration 204, loss = 0.33493552\n",
      "Iteration 205, loss = 0.33472454\n",
      "Iteration 206, loss = 0.33474389\n",
      "Iteration 207, loss = 0.33458560\n",
      "Iteration 208, loss = 0.33443905\n",
      "Iteration 209, loss = 0.33451204\n",
      "Iteration 210, loss = 0.33437957\n",
      "Iteration 211, loss = 0.33430035\n",
      "Iteration 212, loss = 0.33421595\n",
      "Iteration 213, loss = 0.33401716\n",
      "Iteration 214, loss = 0.33396070\n",
      "Iteration 215, loss = 0.33382776\n",
      "Iteration 216, loss = 0.33390888\n",
      "Iteration 217, loss = 0.33376578\n",
      "Iteration 218, loss = 0.33361642\n",
      "Iteration 219, loss = 0.33351652\n",
      "Iteration 220, loss = 0.33349867\n",
      "Iteration 221, loss = 0.33325434\n",
      "Iteration 222, loss = 0.33331049\n",
      "Iteration 223, loss = 0.33336617\n",
      "Iteration 224, loss = 0.33321236\n",
      "Iteration 225, loss = 0.33302970\n",
      "Iteration 226, loss = 0.33292708\n",
      "Iteration 227, loss = 0.33277560\n",
      "Iteration 228, loss = 0.33264934\n",
      "Iteration 229, loss = 0.33268577\n",
      "Iteration 230, loss = 0.33268114\n",
      "Iteration 231, loss = 0.33252575\n",
      "Iteration 232, loss = 0.33240376\n",
      "Iteration 233, loss = 0.33235297\n",
      "Iteration 234, loss = 0.33214037\n",
      "Iteration 235, loss = 0.33220747\n",
      "Iteration 236, loss = 0.33214199\n",
      "Iteration 237, loss = 0.33201907\n",
      "Iteration 238, loss = 0.33193973\n",
      "Iteration 239, loss = 0.33190690\n",
      "Iteration 240, loss = 0.33183118\n",
      "Iteration 241, loss = 0.33179058\n",
      "Iteration 242, loss = 0.33182603\n",
      "Iteration 243, loss = 0.33155736\n",
      "Iteration 244, loss = 0.33136737\n",
      "Iteration 245, loss = 0.33142253\n",
      "Iteration 246, loss = 0.33129911\n",
      "Iteration 247, loss = 0.33141145\n",
      "Iteration 248, loss = 0.33128937\n",
      "Iteration 249, loss = 0.33122502\n",
      "Iteration 250, loss = 0.33101887\n",
      "Iteration 251, loss = 0.33100410\n",
      "Iteration 252, loss = 0.33096483\n",
      "Iteration 253, loss = 0.33078412\n",
      "Iteration 254, loss = 0.33073379\n",
      "Iteration 255, loss = 0.33078352\n",
      "Iteration 256, loss = 0.33065522\n",
      "Iteration 257, loss = 0.33060865\n",
      "Iteration 258, loss = 0.33067348\n",
      "Iteration 259, loss = 0.33043775\n",
      "Iteration 260, loss = 0.33052990\n",
      "Iteration 261, loss = 0.33033853\n",
      "Iteration 262, loss = 0.33018242\n",
      "Iteration 263, loss = 0.33027416\n",
      "Iteration 264, loss = 0.33019184\n",
      "Iteration 265, loss = 0.33018279\n",
      "Iteration 266, loss = 0.33003249\n",
      "Iteration 267, loss = 0.32998882\n",
      "Iteration 268, loss = 0.32986238\n",
      "Iteration 269, loss = 0.32988147\n",
      "Iteration 270, loss = 0.32987717\n",
      "Iteration 271, loss = 0.32961041\n",
      "Iteration 272, loss = 0.32980256\n",
      "Iteration 273, loss = 0.32966044\n",
      "Iteration 274, loss = 0.32951356\n",
      "Iteration 275, loss = 0.32958436\n",
      "Iteration 276, loss = 0.32941028\n",
      "Iteration 277, loss = 0.32937520\n",
      "Iteration 278, loss = 0.32936204\n",
      "Iteration 279, loss = 0.32924640\n",
      "Iteration 280, loss = 0.32922512\n",
      "Iteration 281, loss = 0.32927259\n",
      "Iteration 282, loss = 0.32926857\n",
      "Iteration 283, loss = 0.32903983\n",
      "Iteration 284, loss = 0.32914348\n",
      "Iteration 285, loss = 0.32898334\n",
      "Iteration 286, loss = 0.32897506\n",
      "Iteration 287, loss = 0.32883776\n",
      "Iteration 288, loss = 0.32891512\n",
      "Iteration 289, loss = 0.32886530\n",
      "Iteration 290, loss = 0.32858695\n",
      "Iteration 291, loss = 0.32873356\n",
      "Iteration 292, loss = 0.32863496\n",
      "Iteration 293, loss = 0.32848903\n",
      "Iteration 294, loss = 0.32856618\n",
      "Iteration 295, loss = 0.32852860\n",
      "Iteration 296, loss = 0.32843357\n",
      "Iteration 297, loss = 0.32836018\n",
      "Iteration 298, loss = 0.32834760\n",
      "Iteration 299, loss = 0.32835268\n",
      "Iteration 300, loss = 0.32831134\n",
      "Iteration 1, loss = 0.43808010\n",
      "Iteration 2, loss = 0.38103407\n",
      "Iteration 3, loss = 0.37033767\n",
      "Iteration 4, loss = 0.36507058\n",
      "Iteration 5, loss = 0.35931544\n",
      "Iteration 6, loss = 0.35617748\n",
      "Iteration 7, loss = 0.35340711\n",
      "Iteration 8, loss = 0.35214363\n",
      "Iteration 9, loss = 0.34983551\n",
      "Iteration 10, loss = 0.34752779\n",
      "Iteration 11, loss = 0.34621818\n",
      "Iteration 12, loss = 0.34419733\n",
      "Iteration 13, loss = 0.34265857\n",
      "Iteration 14, loss = 0.34072738\n",
      "Iteration 15, loss = 0.34059279\n",
      "Iteration 16, loss = 0.33849019\n",
      "Iteration 17, loss = 0.33829951\n",
      "Iteration 18, loss = 0.33745596\n",
      "Iteration 19, loss = 0.33656242\n",
      "Iteration 20, loss = 0.33669052\n",
      "Iteration 21, loss = 0.33522424\n",
      "Iteration 22, loss = 0.33488018\n",
      "Iteration 23, loss = 0.33382085\n",
      "Iteration 24, loss = 0.33385306\n",
      "Iteration 25, loss = 0.33339278\n",
      "Iteration 26, loss = 0.33186823\n",
      "Iteration 27, loss = 0.33218675\n",
      "Iteration 28, loss = 0.33180760\n",
      "Iteration 29, loss = 0.33176854\n",
      "Iteration 30, loss = 0.33098802\n",
      "Iteration 31, loss = 0.33056000\n",
      "Iteration 32, loss = 0.33152561\n",
      "Iteration 33, loss = 0.33099720\n",
      "Iteration 34, loss = 0.33025607\n",
      "Iteration 35, loss = 0.33086773\n",
      "Iteration 36, loss = 0.32967911\n",
      "Iteration 37, loss = 0.32916717\n",
      "Iteration 38, loss = 0.32935317\n",
      "Iteration 89, loss = 0.31546612\n",
      "Iteration 90, loss = 0.31493698\n",
      "Iteration 91, loss = 0.31512836\n",
      "Iteration 92, loss = 0.31493347\n",
      "Iteration 93, loss = 0.31465539\n",
      "Iteration 94, loss = 0.31467032\n",
      "Iteration 95, loss = 0.31448217\n",
      "Iteration 96, loss = 0.31432035\n",
      "Iteration 97, loss = 0.31413725\n",
      "Iteration 98, loss = 0.31382969\n",
      "Iteration 99, loss = 0.31430003\n",
      "Iteration 100, loss = 0.31395871\n",
      "Iteration 101, loss = 0.31400860\n",
      "Iteration 102, loss = 0.31355990\n",
      "Iteration 103, loss = 0.31321745\n",
      "Iteration 104, loss = 0.31325011\n",
      "Iteration 105, loss = 0.31324665\n",
      "Iteration 106, loss = 0.31284220\n",
      "Iteration 107, loss = 0.31289767\n",
      "Iteration 108, loss = 0.31236383\n",
      "Iteration 109, loss = 0.31249315\n",
      "Iteration 110, loss = 0.31271804\n",
      "Iteration 111, loss = 0.31282471\n",
      "Iteration 112, loss = 0.31217900\n",
      "Iteration 113, loss = 0.31211565\n",
      "Iteration 114, loss = 0.31186470\n",
      "Iteration 115, loss = 0.31209780\n",
      "Iteration 116, loss = 0.31170478\n",
      "Iteration 117, loss = 0.31119219\n",
      "Iteration 118, loss = 0.31149957\n",
      "Iteration 119, loss = 0.31125726\n",
      "Iteration 120, loss = 0.31146857\n",
      "Iteration 121, loss = 0.31093935\n",
      "Iteration 122, loss = 0.31124535\n",
      "Iteration 123, loss = 0.31071773\n",
      "Iteration 124, loss = 0.31083454\n",
      "Iteration 125, loss = 0.31083437\n",
      "Iteration 126, loss = 0.31069919\n",
      "Iteration 127, loss = 0.31052612\n",
      "Iteration 128, loss = 0.31042295\n",
      "Iteration 129, loss = 0.31031179\n",
      "Iteration 130, loss = 0.31033492\n",
      "Iteration 131, loss = 0.31003481\n",
      "Iteration 132, loss = 0.31004734\n",
      "Iteration 133, loss = 0.31001922\n",
      "Iteration 134, loss = 0.30981093\n",
      "Iteration 135, loss = 0.30957001\n",
      "Iteration 136, loss = 0.30947727\n",
      "Iteration 137, loss = 0.30909523\n",
      "Iteration 138, loss = 0.30952344\n",
      "Iteration 139, loss = 0.30895424\n",
      "Iteration 140, loss = 0.30874814\n",
      "Iteration 141, loss = 0.30893666\n",
      "Iteration 142, loss = 0.30886781\n",
      "Iteration 143, loss = 0.30847655\n",
      "Iteration 144, loss = 0.30839210\n",
      "Iteration 145, loss = 0.30881392\n",
      "Iteration 146, loss = 0.30793681\n",
      "Iteration 147, loss = 0.30810811\n",
      "Iteration 148, loss = 0.30798365\n",
      "Iteration 149, loss = 0.30772343\n",
      "Iteration 150, loss = 0.30761791\n",
      "Iteration 151, loss = 0.30750946\n",
      "Iteration 152, loss = 0.30763099\n",
      "Iteration 153, loss = 0.30736711\n",
      "Iteration 154, loss = 0.30708885\n",
      "Iteration 155, loss = 0.30676906\n",
      "Iteration 156, loss = 0.30688053\n",
      "Iteration 157, loss = 0.30702823\n",
      "Iteration 158, loss = 0.30683321\n",
      "Iteration 159, loss = 0.30665313\n",
      "Iteration 160, loss = 0.30652525\n",
      "Iteration 161, loss = 0.30655812\n",
      "Iteration 162, loss = 0.30641147\n",
      "Iteration 163, loss = 0.30635098\n",
      "Iteration 164, loss = 0.30598888\n",
      "Iteration 165, loss = 0.30564081\n",
      "Iteration 166, loss = 0.30566683\n",
      "Iteration 167, loss = 0.30548031\n",
      "Iteration 168, loss = 0.30543885\n",
      "Iteration 169, loss = 0.30512523\n",
      "Iteration 170, loss = 0.30518859\n",
      "Iteration 171, loss = 0.30489592\n",
      "Iteration 172, loss = 0.30478753\n",
      "Iteration 173, loss = 0.30457891\n",
      "Iteration 174, loss = 0.30491132\n",
      "Iteration 175, loss = 0.30446639\n",
      "Iteration 176, loss = 0.30407715\n",
      "Iteration 177, loss = 0.30427058\n",
      "Iteration 178, loss = 0.30397765\n",
      "Iteration 179, loss = 0.30353986\n",
      "Iteration 180, loss = 0.30382296\n",
      "Iteration 181, loss = 0.30366274\n",
      "Iteration 182, loss = 0.30363113\n",
      "Iteration 183, loss = 0.30303533\n",
      "Iteration 184, loss = 0.30325098\n",
      "Iteration 185, loss = 0.30296274\n",
      "Iteration 186, loss = 0.30277199\n",
      "Iteration 187, loss = 0.30257101\n",
      "Iteration 188, loss = 0.30309030\n",
      "Iteration 189, loss = 0.30250303\n",
      "Iteration 190, loss = 0.30225795\n",
      "Iteration 191, loss = 0.30202362\n",
      "Iteration 192, loss = 0.30217618\n",
      "Iteration 193, loss = 0.30186883\n",
      "Iteration 194, loss = 0.30212804\n",
      "Iteration 195, loss = 0.30168620\n",
      "Iteration 196, loss = 0.30154455\n",
      "Iteration 197, loss = 0.30145838\n",
      "Iteration 198, loss = 0.30114743\n",
      "Iteration 199, loss = 0.30112887\n",
      "Iteration 200, loss = 0.30126451\n",
      "Iteration 201, loss = 0.30085474\n",
      "Iteration 202, loss = 0.30082838\n",
      "Iteration 203, loss = 0.30045875\n",
      "Iteration 204, loss = 0.30031347\n",
      "Iteration 205, loss = 0.30038285\n",
      "Iteration 206, loss = 0.30017907\n",
      "Iteration 207, loss = 0.30018627\n",
      "Iteration 208, loss = 0.29977867\n",
      "Iteration 209, loss = 0.30015792\n",
      "Iteration 210, loss = 0.29969701\n",
      "Iteration 211, loss = 0.29974293\n",
      "Iteration 212, loss = 0.29956371\n",
      "Iteration 213, loss = 0.29955785\n",
      "Iteration 214, loss = 0.29914294\n",
      "Iteration 215, loss = 0.29928555\n",
      "Iteration 216, loss = 0.29903524\n",
      "Iteration 217, loss = 0.29881353\n",
      "Iteration 218, loss = 0.29865186\n",
      "Iteration 219, loss = 0.29851361\n",
      "Iteration 220, loss = 0.29851712\n",
      "Iteration 221, loss = 0.29855420\n",
      "Iteration 222, loss = 0.29807255\n",
      "Iteration 223, loss = 0.29810114\n",
      "Iteration 224, loss = 0.29811344\n",
      "Iteration 225, loss = 0.29797568\n",
      "Iteration 226, loss = 0.29799648\n",
      "Iteration 227, loss = 0.29770972\n",
      "Iteration 228, loss = 0.29774302\n",
      "Iteration 229, loss = 0.29765555\n",
      "Iteration 230, loss = 0.29764412\n",
      "Iteration 231, loss = 0.29733459\n",
      "Iteration 232, loss = 0.29707443\n",
      "Iteration 233, loss = 0.29703275\n",
      "Iteration 234, loss = 0.29694769\n",
      "Iteration 235, loss = 0.29673447\n",
      "Iteration 236, loss = 0.29704686\n",
      "Iteration 237, loss = 0.29699722\n",
      "Iteration 238, loss = 0.29680133\n",
      "Iteration 239, loss = 0.29643827\n",
      "Iteration 240, loss = 0.29618426\n",
      "Iteration 241, loss = 0.29633983\n",
      "Iteration 242, loss = 0.29641668\n",
      "Iteration 243, loss = 0.29570532\n",
      "Iteration 244, loss = 0.29599281\n",
      "Iteration 245, loss = 0.29602837\n",
      "Iteration 246, loss = 0.29583119\n",
      "Iteration 247, loss = 0.29553179\n",
      "Iteration 248, loss = 0.29567525\n",
      "Iteration 249, loss = 0.29556117\n",
      "Iteration 250, loss = 0.29538424\n",
      "Iteration 251, loss = 0.29533735\n",
      "Iteration 252, loss = 0.29539438\n",
      "Iteration 253, loss = 0.29496337\n",
      "Iteration 254, loss = 0.29493083\n",
      "Iteration 255, loss = 0.29506844\n",
      "Iteration 256, loss = 0.29495391\n",
      "Iteration 257, loss = 0.29487103\n",
      "Iteration 258, loss = 0.29489355\n",
      "Iteration 259, loss = 0.29447925\n",
      "Iteration 260, loss = 0.29425992\n",
      "Iteration 261, loss = 0.29456789\n",
      "Iteration 262, loss = 0.29431883\n",
      "Iteration 263, loss = 0.29405515\n",
      "Iteration 264, loss = 0.29383324\n",
      "Iteration 265, loss = 0.29384085\n",
      "Iteration 266, loss = 0.29379210\n",
      "Iteration 267, loss = 0.29388916\n",
      "Iteration 268, loss = 0.29379611\n",
      "Iteration 269, loss = 0.29385210\n",
      "Iteration 270, loss = 0.29360792\n",
      "Iteration 271, loss = 0.29323025\n",
      "Iteration 272, loss = 0.29350536\n",
      "Iteration 273, loss = 0.29314731\n",
      "Iteration 274, loss = 0.29341686\n",
      "Iteration 275, loss = 0.29346289\n",
      "Iteration 276, loss = 0.29268919\n",
      "Iteration 277, loss = 0.29297789\n",
      "Iteration 278, loss = 0.29303213\n",
      "Iteration 279, loss = 0.29291047\n",
      "Iteration 280, loss = 0.29273633\n",
      "Iteration 281, loss = 0.29276747\n",
      "Iteration 282, loss = 0.29274370\n",
      "Iteration 283, loss = 0.29238649\n",
      "Iteration 284, loss = 0.29206622\n",
      "Iteration 285, loss = 0.29217412\n",
      "Iteration 286, loss = 0.29251354\n",
      "Iteration 287, loss = 0.29224280\n",
      "Iteration 288, loss = 0.29213379\n",
      "Iteration 289, loss = 0.29182084\n",
      "Iteration 290, loss = 0.29201130\n",
      "Iteration 291, loss = 0.29167623\n",
      "Iteration 292, loss = 0.29181979\n",
      "Iteration 293, loss = 0.29163418\n",
      "Iteration 294, loss = 0.29155000\n",
      "Iteration 295, loss = 0.29168063\n",
      "Iteration 296, loss = 0.29134749\n",
      "Iteration 297, loss = 0.29127722\n",
      "Iteration 298, loss = 0.29126793\n",
      "Iteration 299, loss = 0.29160322\n",
      "Iteration 300, loss = 0.29108689\n",
      "Iteration 1, loss = 0.64759182\n",
      "Iteration 2, loss = 0.56286254\n",
      "Iteration 3, loss = 0.47276083\n",
      "Iteration 4, loss = 0.41837977\n",
      "Iteration 5, loss = 0.39727359\n",
      "Iteration 6, loss = 0.38860096\n",
      "Iteration 7, loss = 0.38385821\n",
      "Iteration 8, loss = 0.38051173\n",
      "Iteration 9, loss = 0.37799647\n",
      "Iteration 10, loss = 0.37608736\n",
      "Iteration 11, loss = 0.37488073\n",
      "Iteration 12, loss = 0.37345581\n",
      "Iteration 13, loss = 0.37256526\n",
      "Iteration 14, loss = 0.37144393\n",
      "Iteration 15, loss = 0.37077600\n",
      "Iteration 16, loss = 0.36960796\n",
      "Iteration 17, loss = 0.36889257\n",
      "Iteration 18, loss = 0.36757018\n",
      "Iteration 19, loss = 0.36697640\n",
      "Iteration 20, loss = 0.36640522\n",
      "Iteration 21, loss = 0.36587161\n",
      "Iteration 22, loss = 0.36499447\n",
      "Iteration 23, loss = 0.36433075\n",
      "Iteration 24, loss = 0.36363912\n",
      "Iteration 25, loss = 0.36274416\n",
      "Iteration 26, loss = 0.36248532\n",
      "Iteration 27, loss = 0.36208659\n",
      "Iteration 28, loss = 0.36089953\n",
      "Iteration 29, loss = 0.36098734\n",
      "Iteration 30, loss = 0.36010788\n",
      "Iteration 31, loss = 0.35960997\n",
      "Iteration 32, loss = 0.35913245\n",
      "Iteration 33, loss = 0.35881634\n",
      "Iteration 34, loss = 0.35786153\n",
      "Iteration 35, loss = 0.35754560\n",
      "Iteration 36, loss = 0.35725239\n",
      "Iteration 37, loss = 0.35665336\n",
      "Iteration 38, loss = 0.35595531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.33287917\n",
      "Iteration 23, loss = 0.33241103\n",
      "Iteration 24, loss = 0.33159133\n",
      "Iteration 25, loss = 0.33058386\n",
      "Iteration 26, loss = 0.33030558\n",
      "Iteration 27, loss = 0.32952108\n",
      "Iteration 28, loss = 0.32931366\n",
      "Iteration 29, loss = 0.32871757\n",
      "Iteration 30, loss = 0.32815200\n",
      "Iteration 31, loss = 0.32752365\n",
      "Iteration 32, loss = 0.32729716\n",
      "Iteration 33, loss = 0.32699396\n",
      "Iteration 34, loss = 0.32671493\n",
      "Iteration 35, loss = 0.32627727\n",
      "Iteration 36, loss = 0.32622242\n",
      "Iteration 37, loss = 0.32551509\n",
      "Iteration 38, loss = 0.32567609\n",
      "Iteration 39, loss = 0.32491139\n",
      "Iteration 40, loss = 0.32517080\n",
      "Iteration 41, loss = 0.32458965\n",
      "Iteration 42, loss = 0.32408763\n",
      "Iteration 43, loss = 0.32410776\n",
      "Iteration 44, loss = 0.32351722\n",
      "Iteration 45, loss = 0.32348461\n",
      "Iteration 46, loss = 0.32327623\n",
      "Iteration 47, loss = 0.32325355\n",
      "Iteration 48, loss = 0.32301928\n",
      "Iteration 49, loss = 0.32264440\n",
      "Iteration 50, loss = 0.32245815\n",
      "Iteration 51, loss = 0.32221924\n",
      "Iteration 52, loss = 0.32210757\n",
      "Iteration 53, loss = 0.32186081\n",
      "Iteration 54, loss = 0.32159258\n",
      "Iteration 55, loss = 0.32158877\n",
      "Iteration 56, loss = 0.32122097\n",
      "Iteration 57, loss = 0.32114709\n",
      "Iteration 58, loss = 0.32084694\n",
      "Iteration 59, loss = 0.32055781\n",
      "Iteration 60, loss = 0.32059373\n",
      "Iteration 61, loss = 0.32034499\n",
      "Iteration 62, loss = 0.32032881\n",
      "Iteration 63, loss = 0.31990387\n",
      "Iteration 64, loss = 0.31944008\n",
      "Iteration 65, loss = 0.31952941\n",
      "Iteration 66, loss = 0.31932340\n",
      "Iteration 67, loss = 0.31894166\n",
      "Iteration 68, loss = 0.31919258\n",
      "Iteration 69, loss = 0.31915098\n",
      "Iteration 70, loss = 0.31856777\n",
      "Iteration 71, loss = 0.31840616\n",
      "Iteration 72, loss = 0.31835601\n",
      "Iteration 73, loss = 0.31827084\n",
      "Iteration 74, loss = 0.31817040\n",
      "Iteration 75, loss = 0.31788901\n",
      "Iteration 76, loss = 0.31770860\n",
      "Iteration 77, loss = 0.31769428\n",
      "Iteration 78, loss = 0.31735790\n",
      "Iteration 79, loss = 0.31750909\n",
      "Iteration 80, loss = 0.31713604\n",
      "Iteration 81, loss = 0.31727864\n",
      "Iteration 82, loss = 0.31693321\n",
      "Iteration 83, loss = 0.31658319\n",
      "Iteration 84, loss = 0.31648117\n",
      "Iteration 85, loss = 0.31642907\n",
      "Iteration 86, loss = 0.31621821\n",
      "Iteration 87, loss = 0.31603466\n",
      "Iteration 88, loss = 0.31630464\n",
      "Iteration 89, loss = 0.31571775\n",
      "Iteration 90, loss = 0.31588462\n",
      "Iteration 91, loss = 0.31518306\n",
      "Iteration 92, loss = 0.31532576\n",
      "Iteration 93, loss = 0.31498080\n",
      "Iteration 94, loss = 0.31536934\n",
      "Iteration 95, loss = 0.31493202\n",
      "Iteration 96, loss = 0.31459357\n",
      "Iteration 97, loss = 0.31445598\n",
      "Iteration 98, loss = 0.31462045\n",
      "Iteration 99, loss = 0.31426513\n",
      "Iteration 100, loss = 0.31414812\n",
      "Iteration 101, loss = 0.31412950\n",
      "Iteration 102, loss = 0.31417293\n",
      "Iteration 103, loss = 0.31372883\n",
      "Iteration 104, loss = 0.31382866\n",
      "Iteration 105, loss = 0.31344754\n",
      "Iteration 106, loss = 0.31332646\n",
      "Iteration 107, loss = 0.31329048\n",
      "Iteration 108, loss = 0.31316333\n",
      "Iteration 109, loss = 0.31299155\n",
      "Iteration 110, loss = 0.31279932\n",
      "Iteration 111, loss = 0.31279972\n",
      "Iteration 112, loss = 0.31258828\n",
      "Iteration 113, loss = 0.31262721\n",
      "Iteration 114, loss = 0.31214870\n",
      "Iteration 115, loss = 0.31202438\n",
      "Iteration 116, loss = 0.31249222\n",
      "Iteration 117, loss = 0.31196467\n",
      "Iteration 118, loss = 0.31141109\n",
      "Iteration 119, loss = 0.31158020\n",
      "Iteration 120, loss = 0.31164469\n",
      "Iteration 121, loss = 0.31154876\n",
      "Iteration 122, loss = 0.31110570\n",
      "Iteration 123, loss = 0.31086531\n",
      "Iteration 124, loss = 0.31089524\n",
      "Iteration 125, loss = 0.31092037\n",
      "Iteration 126, loss = 0.31053366\n",
      "Iteration 127, loss = 0.31054480\n",
      "Iteration 128, loss = 0.31042617\n",
      "Iteration 129, loss = 0.31051928\n",
      "Iteration 130, loss = 0.31042666\n",
      "Iteration 131, loss = 0.31001182\n",
      "Iteration 132, loss = 0.31003630\n",
      "Iteration 133, loss = 0.30979987\n",
      "Iteration 134, loss = 0.30950395\n",
      "Iteration 135, loss = 0.30960451\n",
      "Iteration 136, loss = 0.30946620\n",
      "Iteration 137, loss = 0.30939964\n",
      "Iteration 138, loss = 0.30908222\n",
      "Iteration 139, loss = 0.30927306\n",
      "Iteration 140, loss = 0.30875772\n",
      "Iteration 141, loss = 0.30883996\n",
      "Iteration 142, loss = 0.30838597\n",
      "Iteration 143, loss = 0.30853086\n",
      "Iteration 144, loss = 0.30840206\n",
      "Iteration 145, loss = 0.30805454\n",
      "Iteration 146, loss = 0.30819720\n",
      "Iteration 147, loss = 0.30812933\n",
      "Iteration 148, loss = 0.30796818\n",
      "Iteration 149, loss = 0.30726498\n",
      "Iteration 150, loss = 0.30756838\n",
      "Iteration 151, loss = 0.30755117\n",
      "Iteration 152, loss = 0.30731416\n",
      "Iteration 153, loss = 0.30710015\n",
      "Iteration 154, loss = 0.30711559\n",
      "Iteration 155, loss = 0.30693443\n",
      "Iteration 156, loss = 0.30673469\n",
      "Iteration 157, loss = 0.30675971\n",
      "Iteration 158, loss = 0.30656999\n",
      "Iteration 159, loss = 0.30651322\n",
      "Iteration 160, loss = 0.30618707\n",
      "Iteration 161, loss = 0.30621253\n",
      "Iteration 162, loss = 0.30591586\n",
      "Iteration 163, loss = 0.30604615\n",
      "Iteration 164, loss = 0.30546608\n",
      "Iteration 165, loss = 0.30569390\n",
      "Iteration 166, loss = 0.30537663\n",
      "Iteration 167, loss = 0.30515467\n",
      "Iteration 168, loss = 0.30489459\n",
      "Iteration 169, loss = 0.30499301\n",
      "Iteration 170, loss = 0.30488959\n",
      "Iteration 171, loss = 0.30482830\n",
      "Iteration 172, loss = 0.30446399\n",
      "Iteration 173, loss = 0.30481879\n",
      "Iteration 174, loss = 0.30435623\n",
      "Iteration 175, loss = 0.30451431\n",
      "Iteration 176, loss = 0.30424050\n",
      "Iteration 177, loss = 0.30433542\n",
      "Iteration 178, loss = 0.30385751\n",
      "Iteration 179, loss = 0.30368520\n",
      "Iteration 180, loss = 0.30340388\n",
      "Iteration 181, loss = 0.30357914\n",
      "Iteration 182, loss = 0.30324181\n",
      "Iteration 183, loss = 0.30314507\n",
      "Iteration 184, loss = 0.30297051\n",
      "Iteration 185, loss = 0.30293003\n",
      "Iteration 186, loss = 0.30298763\n",
      "Iteration 187, loss = 0.30270846\n",
      "Iteration 188, loss = 0.30277885\n",
      "Iteration 189, loss = 0.30214386\n",
      "Iteration 190, loss = 0.30236455\n",
      "Iteration 191, loss = 0.30199944\n",
      "Iteration 192, loss = 0.30206674\n",
      "Iteration 193, loss = 0.30195097\n",
      "Iteration 194, loss = 0.30224439\n",
      "Iteration 195, loss = 0.30162330\n",
      "Iteration 196, loss = 0.30182853\n",
      "Iteration 197, loss = 0.30141743\n",
      "Iteration 198, loss = 0.30195220\n",
      "Iteration 199, loss = 0.30119160\n",
      "Iteration 200, loss = 0.30135581\n",
      "Iteration 201, loss = 0.30097237\n",
      "Iteration 202, loss = 0.30086495\n",
      "Iteration 203, loss = 0.30069304\n",
      "Iteration 204, loss = 0.30066594\n",
      "Iteration 205, loss = 0.30035249\n",
      "Iteration 206, loss = 0.30048564\n",
      "Iteration 207, loss = 0.30025380\n",
      "Iteration 208, loss = 0.30012594\n",
      "Iteration 209, loss = 0.29986739\n",
      "Iteration 210, loss = 0.29993317\n",
      "Iteration 211, loss = 0.29987661\n",
      "Iteration 212, loss = 0.29969576\n",
      "Iteration 213, loss = 0.29958474\n",
      "Iteration 214, loss = 0.29934611\n",
      "Iteration 215, loss = 0.29920407\n",
      "Iteration 216, loss = 0.29947927\n",
      "Iteration 217, loss = 0.29926831\n",
      "Iteration 218, loss = 0.29888819\n",
      "Iteration 219, loss = 0.29886504\n",
      "Iteration 220, loss = 0.29894406\n",
      "Iteration 221, loss = 0.29852550\n",
      "Iteration 222, loss = 0.29863800\n",
      "Iteration 223, loss = 0.29906275\n",
      "Iteration 224, loss = 0.29858353\n",
      "Iteration 225, loss = 0.29827981\n",
      "Iteration 226, loss = 0.29808763\n",
      "Iteration 227, loss = 0.29795437\n",
      "Iteration 228, loss = 0.29777531\n",
      "Iteration 229, loss = 0.29795726\n",
      "Iteration 230, loss = 0.29797153\n",
      "Iteration 231, loss = 0.29771320\n",
      "Iteration 232, loss = 0.29764571\n",
      "Iteration 233, loss = 0.29753357\n",
      "Iteration 234, loss = 0.29710760\n",
      "Iteration 235, loss = 0.29749414\n",
      "Iteration 236, loss = 0.29716998\n",
      "Iteration 237, loss = 0.29721230\n",
      "Iteration 238, loss = 0.29690416\n",
      "Iteration 239, loss = 0.29728340\n",
      "Iteration 240, loss = 0.29691572\n",
      "Iteration 241, loss = 0.29708704\n",
      "Iteration 242, loss = 0.29683814\n",
      "Iteration 243, loss = 0.29662618\n",
      "Iteration 244, loss = 0.29628130\n",
      "Iteration 245, loss = 0.29651337\n",
      "Iteration 246, loss = 0.29611133\n",
      "Iteration 247, loss = 0.29647228\n",
      "Iteration 248, loss = 0.29630176\n",
      "Iteration 249, loss = 0.29595914\n",
      "Iteration 250, loss = 0.29577263\n",
      "Iteration 251, loss = 0.29586609\n",
      "Iteration 252, loss = 0.29568806\n",
      "Iteration 253, loss = 0.29554718\n",
      "Iteration 254, loss = 0.29550427\n",
      "Iteration 255, loss = 0.29564255\n",
      "Iteration 256, loss = 0.29513472\n",
      "Iteration 257, loss = 0.29543448\n",
      "Iteration 258, loss = 0.29551491\n",
      "Iteration 259, loss = 0.29516410\n",
      "Iteration 260, loss = 0.29513971\n",
      "Iteration 261, loss = 0.29505761\n",
      "Iteration 262, loss = 0.29472684\n",
      "Iteration 263, loss = 0.29496321\n",
      "Iteration 264, loss = 0.29460648\n",
      "Iteration 265, loss = 0.29477032\n",
      "Iteration 266, loss = 0.29465952\n",
      "Iteration 267, loss = 0.29456752\n",
      "Iteration 268, loss = 0.29431561\n",
      "Iteration 269, loss = 0.29409605\n",
      "Iteration 270, loss = 0.29423078\n",
      "Iteration 271, loss = 0.29415444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/james/miniconda3/envs/ydmads-modeltraining/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 110, loss = 0.34532155\n",
      "Iteration 111, loss = 0.34522948\n",
      "Iteration 112, loss = 0.34483830\n",
      "Iteration 113, loss = 0.34469803\n",
      "Iteration 114, loss = 0.34464542\n",
      "Iteration 115, loss = 0.34445522\n",
      "Iteration 116, loss = 0.34429934\n",
      "Iteration 117, loss = 0.34398676\n",
      "Iteration 118, loss = 0.34398389\n",
      "Iteration 119, loss = 0.34393496\n",
      "Iteration 120, loss = 0.34376099\n",
      "Iteration 121, loss = 0.34347222\n",
      "Iteration 122, loss = 0.34345191\n",
      "Iteration 123, loss = 0.34322040\n",
      "Iteration 124, loss = 0.34308122\n",
      "Iteration 125, loss = 0.34305010\n",
      "Iteration 126, loss = 0.34295058\n",
      "Iteration 127, loss = 0.34276990\n",
      "Iteration 128, loss = 0.34261965\n",
      "Iteration 129, loss = 0.34256295\n",
      "Iteration 130, loss = 0.34240487\n",
      "Iteration 131, loss = 0.34232739\n",
      "Iteration 132, loss = 0.34228589\n",
      "Iteration 133, loss = 0.34208287\n",
      "Iteration 134, loss = 0.34201362\n",
      "Iteration 135, loss = 0.34181887\n",
      "Iteration 136, loss = 0.34169405\n",
      "Iteration 137, loss = 0.34155116\n",
      "Iteration 138, loss = 0.34152433\n",
      "Iteration 139, loss = 0.34131405\n",
      "Iteration 140, loss = 0.34115381\n",
      "Iteration 141, loss = 0.34116126\n",
      "Iteration 142, loss = 0.34106996\n",
      "Iteration 143, loss = 0.34077620\n",
      "Iteration 144, loss = 0.34080656\n",
      "Iteration 145, loss = 0.34078890\n",
      "Iteration 146, loss = 0.34046321\n",
      "Iteration 147, loss = 0.34049927\n",
      "Iteration 148, loss = 0.34034129\n",
      "Iteration 149, loss = 0.34031336\n",
      "Iteration 150, loss = 0.34016650\n",
      "Iteration 151, loss = 0.34004997\n",
      "Iteration 152, loss = 0.33995126\n",
      "Iteration 153, loss = 0.33990643\n",
      "Iteration 154, loss = 0.33973459\n",
      "Iteration 155, loss = 0.33955931\n",
      "Iteration 156, loss = 0.33957341\n",
      "Iteration 157, loss = 0.33952093\n",
      "Iteration 158, loss = 0.33935921\n",
      "Iteration 159, loss = 0.33928791\n",
      "Iteration 160, loss = 0.33923858\n",
      "Iteration 161, loss = 0.33917455\n",
      "Iteration 162, loss = 0.33899503\n",
      "Iteration 163, loss = 0.33898588\n",
      "Iteration 164, loss = 0.33881486\n",
      "Iteration 165, loss = 0.33866162\n",
      "Iteration 166, loss = 0.33867270\n",
      "Iteration 167, loss = 0.33858869\n",
      "Iteration 168, loss = 0.33849794\n",
      "Iteration 169, loss = 0.33833164\n",
      "Iteration 170, loss = 0.33828759\n",
      "Iteration 171, loss = 0.33815898\n",
      "Iteration 172, loss = 0.33806529\n",
      "Iteration 173, loss = 0.33799178\n",
      "Iteration 174, loss = 0.33793578\n",
      "Iteration 175, loss = 0.33788026\n",
      "Iteration 176, loss = 0.33771244\n",
      "Iteration 177, loss = 0.33776004\n",
      "Iteration 178, loss = 0.33750591\n",
      "Iteration 179, loss = 0.33745468\n",
      "Iteration 180, loss = 0.33742715\n",
      "Iteration 181, loss = 0.33732580\n",
      "Iteration 182, loss = 0.33731481\n",
      "Iteration 183, loss = 0.33708316\n",
      "Iteration 184, loss = 0.33713953\n",
      "Iteration 185, loss = 0.33702149\n",
      "Iteration 186, loss = 0.33687598\n",
      "Iteration 187, loss = 0.33675350\n",
      "Iteration 188, loss = 0.33686619\n",
      "Iteration 189, loss = 0.33660118\n",
      "Iteration 190, loss = 0.33655894\n",
      "Iteration 191, loss = 0.33654810\n",
      "Iteration 192, loss = 0.33641614\n",
      "Iteration 193, loss = 0.33633957\n",
      "Iteration 194, loss = 0.33631312\n",
      "Iteration 195, loss = 0.33625634\n",
      "Iteration 196, loss = 0.33611223\n",
      "Iteration 197, loss = 0.33600558\n",
      "Iteration 198, loss = 0.33594274\n",
      "Iteration 199, loss = 0.33593508\n",
      "Iteration 200, loss = 0.33593128\n",
      "Iteration 201, loss = 0.33570706\n",
      "Iteration 202, loss = 0.33568897\n",
      "Iteration 203, loss = 0.33549717\n",
      "Iteration 204, loss = 0.33542437\n",
      "Iteration 205, loss = 0.33538583\n",
      "Iteration 206, loss = 0.33529117\n",
      "Iteration 207, loss = 0.33531381\n",
      "Iteration 208, loss = 0.33511323\n",
      "Iteration 209, loss = 0.33526311\n",
      "Iteration 210, loss = 0.33510111\n",
      "Iteration 211, loss = 0.33499245\n",
      "Iteration 212, loss = 0.33497714\n",
      "Iteration 213, loss = 0.33484628\n",
      "Iteration 214, loss = 0.33483292\n",
      "Iteration 215, loss = 0.33475018\n",
      "Iteration 216, loss = 0.33455902\n",
      "Iteration 217, loss = 0.33442990\n",
      "Iteration 218, loss = 0.33433133\n",
      "Iteration 219, loss = 0.33439006\n",
      "Iteration 220, loss = 0.33429677\n",
      "Iteration 221, loss = 0.33423221\n",
      "Iteration 222, loss = 0.33403584\n",
      "Iteration 223, loss = 0.33402995\n",
      "Iteration 224, loss = 0.33398783\n",
      "Iteration 225, loss = 0.33393233\n",
      "Iteration 226, loss = 0.33389210\n",
      "Iteration 227, loss = 0.33366435\n",
      "Iteration 228, loss = 0.33368052\n",
      "Iteration 229, loss = 0.33366617\n",
      "Iteration 230, loss = 0.33354609\n",
      "Iteration 231, loss = 0.33349837\n",
      "Iteration 232, loss = 0.33327210\n",
      "Iteration 233, loss = 0.33323111\n",
      "Iteration 234, loss = 0.33326777\n",
      "Iteration 235, loss = 0.33310082\n",
      "Iteration 236, loss = 0.33317179\n",
      "Iteration 237, loss = 0.33295812\n",
      "Iteration 238, loss = 0.33296309\n",
      "Iteration 239, loss = 0.33292359\n",
      "Iteration 240, loss = 0.33263740\n",
      "Iteration 241, loss = 0.33268251\n",
      "Iteration 242, loss = 0.33272251\n",
      "Iteration 243, loss = 0.33238270\n",
      "Iteration 244, loss = 0.33241866\n",
      "Iteration 245, loss = 0.33238973\n",
      "Iteration 246, loss = 0.33238227\n",
      "Iteration 247, loss = 0.33225147\n",
      "Iteration 248, loss = 0.33216148\n",
      "Iteration 249, loss = 0.33215943\n",
      "Iteration 250, loss = 0.33195601\n",
      "Iteration 251, loss = 0.33192258\n",
      "Iteration 252, loss = 0.33201188\n",
      "Iteration 253, loss = 0.33174066\n",
      "Iteration 254, loss = 0.33160874\n",
      "Iteration 255, loss = 0.33172686\n",
      "Iteration 256, loss = 0.33158279\n",
      "Iteration 257, loss = 0.33144783\n",
      "Iteration 258, loss = 0.33139240\n",
      "Iteration 259, loss = 0.33133303\n",
      "Iteration 260, loss = 0.33113634\n",
      "Iteration 261, loss = 0.33124013\n",
      "Iteration 262, loss = 0.33114185\n",
      "Iteration 263, loss = 0.33094893\n",
      "Iteration 264, loss = 0.33085594\n",
      "Iteration 265, loss = 0.33074047\n",
      "Iteration 266, loss = 0.33078076\n",
      "Iteration 267, loss = 0.33074050\n",
      "Iteration 268, loss = 0.33060858\n",
      "Iteration 269, loss = 0.33056126\n",
      "Iteration 270, loss = 0.33055801\n",
      "Iteration 271, loss = 0.33039124\n",
      "Iteration 272, loss = 0.33037992\n",
      "Iteration 273, loss = 0.33022107\n",
      "Iteration 274, loss = 0.33030056\n",
      "Iteration 275, loss = 0.33028366\n",
      "Iteration 276, loss = 0.32987395\n",
      "Iteration 277, loss = 0.33000911\n",
      "Iteration 278, loss = 0.32996319\n",
      "Iteration 279, loss = 0.32991837\n",
      "Iteration 280, loss = 0.32987872\n",
      "Iteration 281, loss = 0.32983711\n",
      "Iteration 282, loss = 0.32969087\n",
      "Iteration 283, loss = 0.32962832\n",
      "Iteration 284, loss = 0.32940612\n",
      "Iteration 285, loss = 0.32937950\n",
      "Iteration 286, loss = 0.32947131\n",
      "Iteration 287, loss = 0.32944369\n",
      "Iteration 288, loss = 0.32924469\n",
      "Iteration 289, loss = 0.32914808\n",
      "Iteration 290, loss = 0.32913321\n",
      "Iteration 291, loss = 0.32892841\n",
      "Iteration 292, loss = 0.32898799\n",
      "Iteration 293, loss = 0.32892057\n",
      "Iteration 294, loss = 0.32890504\n",
      "Iteration 295, loss = 0.32888346\n",
      "Iteration 296, loss = 0.32870124\n",
      "Iteration 297, loss = 0.32864270\n",
      "Iteration 298, loss = 0.32857909\n",
      "Iteration 299, loss = 0.32869603\n",
      "Iteration 300, loss = 0.32854497\n",
      "Iteration 1, loss = 0.64783975\n",
      "Iteration 2, loss = 0.56274622\n",
      "Iteration 3, loss = 0.47248678\n",
      "Iteration 4, loss = 0.41846756\n",
      "Iteration 5, loss = 0.39724487\n",
      "Iteration 6, loss = 0.38906207\n",
      "Iteration 7, loss = 0.38412843\n",
      "Iteration 8, loss = 0.38076998\n",
      "Iteration 9, loss = 0.37856662\n",
      "Iteration 10, loss = 0.37716321\n",
      "Iteration 11, loss = 0.37496933\n",
      "Iteration 12, loss = 0.37393988\n",
      "Iteration 13, loss = 0.37251126\n",
      "Iteration 14, loss = 0.37160981\n",
      "Iteration 15, loss = 0.37050838\n",
      "Iteration 16, loss = 0.36978495\n",
      "Iteration 17, loss = 0.36898242\n",
      "Iteration 18, loss = 0.36809561\n",
      "Iteration 19, loss = 0.36718180\n",
      "Iteration 20, loss = 0.36645204\n",
      "Iteration 21, loss = 0.36599397\n",
      "Iteration 22, loss = 0.36509651\n",
      "Iteration 23, loss = 0.36464273\n",
      "Iteration 24, loss = 0.36397818\n",
      "Iteration 25, loss = 0.36266023\n",
      "Iteration 26, loss = 0.36263332\n",
      "Iteration 27, loss = 0.36221936\n",
      "Iteration 28, loss = 0.36156231\n",
      "Iteration 29, loss = 0.36063435\n",
      "Iteration 30, loss = 0.35998124\n",
      "Iteration 31, loss = 0.35949837\n",
      "Iteration 32, loss = 0.35913678\n",
      "Iteration 33, loss = 0.35863333\n",
      "Iteration 34, loss = 0.35774740\n",
      "Iteration 35, loss = 0.35751735\n",
      "Iteration 36, loss = 0.35707094\n",
      "Iteration 37, loss = 0.35674284\n",
      "Iteration 38, loss = 0.35628565\n",
      "Iteration 39, loss = 0.35581471\n",
      "Iteration 40, loss = 0.35551228\n",
      "Iteration 41, loss = 0.35521366\n",
      "Iteration 42, loss = 0.35462268\n",
      "Iteration 43, loss = 0.35412856\n",
      "Iteration 44, loss = 0.35362962\n",
      "Iteration 45, loss = 0.35344997\n",
      "Iteration 46, loss = 0.35304672\n",
      "Iteration 47, loss = 0.35242556\n",
      "Iteration 48, loss = 0.35223518\n",
      "Iteration 49, loss = 0.35218081\n",
      "Iteration 50, loss = 0.35144322\n",
      "Iteration 51, loss = 0.35138632\n",
      "Iteration 52, loss = 0.35091856\n",
      "Iteration 53, loss = 0.35072915\n",
      "Iteration 54, loss = 0.35039506\n",
      "Iteration 55, loss = 0.34999926\n",
      "Iteration 56, loss = 0.34972591\n",
      "Iteration 57, loss = 0.34935031\n",
      "Iteration 58, loss = 0.34948668\n",
      "Iteration 59, loss = 0.34919869\n"
     ]
    }
   ],
   "source": [
    "# Gridsearch MLP\n",
    "# Inspired by https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "mlp_base = MLPClassifier(random_state=random_state, max_iter=300, verbose=10)\n",
    "mlp_clf_grid = GridSearchCV(mlp_base, parameter_space, n_jobs=-1, cv=3)\n",
    "mlp_clf_grid.fit(training_df_scaled_X,training_df[y_col])\n",
    "\n",
    "mlp_scaled_best_params = mlp_clf_grid.best_params_\n",
    "print(\"MLP Scaled Best Params found through grid search cv are:\")\n",
    "print(mlp_scaled_best_params)\n",
    "\n",
    "# Testing model\n",
    "mlp_acc_grid,mlp_f1_grid = test_model_metrics(mlp_clf_grid,\"MLP-scaled-Gridsearch\",testing_df_scaled_X,testing_df[y_col])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    mlp_clf_grid,\n",
    "    testing_df_scaled_X,\n",
    "    testing_df[y_col],\n",
    "    cmap=plt.cm.Blues,\n",
    "    normalize=\"true\",\n",
    ")\n",
    "disp.ax_.set_title(\"MLP Confusion Matrix-scaled-Gridsearch\")\n",
    "\n",
    "print(\"MLP Confusion Matrix-scaled-Gridsearch\")\n",
    "print(disp.confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f689cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'age_limit',\n",
       " 'view_count',\n",
       " 'like_count',\n",
       " 'view_like_ratio',\n",
       " 'is_comments_enabled',\n",
       " 'is_live_content',\n",
       " 'cat_codes',\n",
       " 'neu',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'compound']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341b6b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ld_score_ohe'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d564e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'age_limit',\n",
       " 'view_count',\n",
       " 'like_count',\n",
       " 'view_like_ratio',\n",
       " 'is_comments_enabled',\n",
       " 'is_live_content',\n",
       " 'cat_codes',\n",
       " 'neu',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'compound',\n",
       " 'ld_score_ohe']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_related_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "5011b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with all required columns\n",
    "tab_df = training_df[all_related_cols].copy()\n",
    "\n",
    "def remap_target(value):\n",
    "    if value == -1:\n",
    "        return \"Negative\"\n",
    "    elif value == 0:\n",
    "        return \"Neutral\"\n",
    "    elif value == 1:\n",
    "        return \"Positive\"\n",
    "\n",
    "tab_df[\"ld_score_ohe\"] = tab_df[\"ld_score_ohe\"].apply(remap_target)\n",
    "\n",
    "# Create tabular object and dataloaders\n",
    "cat_names = ['age_limit','is_comments_enabled','is_live_content','cat_codes']\n",
    "cont_names = ['duration', 'view_count', 'like_count','view_like_ratio','neu','neg','pos','compound']\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "\n",
    "# Creates splits\n",
    "splits = RandomSplitter(valid_pct=0.1)(range_of(tab_df))\n",
    "\n",
    "to = TabularPandas(tab_df, procs=procs,\n",
    "                   cat_names = cat_names,\n",
    "                   cont_names = cont_names,\n",
    "                   y_names=y_col,\n",
    "                   y_block=CategoryBlock,\n",
    "                   splits=splits)\n",
    "\n",
    "dls = to.dataloaders(bs=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ea930b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted f1 score by modifying fastai f1 score\n",
    "# def F1Score(axis=-1, labels=None, pos_label=1, average='weighted', sample_weight=None):\n",
    "#     \"F1 score for single-label classification problems\"\n",
    "#     return skm_to_fastai(skm.f1_score, axis=axis,\n",
    "#                          labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4c735dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss_flat = CrossEntropyLossFlat()\n",
    "f1_score_fai = F1Score()\n",
    "f1_score_multi = F1ScoreMulti(average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a40ba7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tabular learner\n",
    "learn_tab = tabular_learner(dls, layers=[100,200,100], metrics=accuracy,loss_func=cross_entropy_loss_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9a7bda29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.0008317637839354575)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fastai has a useful function to estimate the best learning rate to use.\n",
    "learn_tab.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "444fa81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.537413</td>\n",
       "      <td>0.770312</td>\n",
       "      <td>0.726088</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.387218</td>\n",
       "      <td>1.062577</td>\n",
       "      <td>0.732909</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.352577</td>\n",
       "      <td>1.069417</td>\n",
       "      <td>0.718311</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.318735</td>\n",
       "      <td>1.123301</td>\n",
       "      <td>0.733351</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.308855</td>\n",
       "      <td>0.992836</td>\n",
       "      <td>0.732296</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the data. We are not fine-tuning here. We are learning from our training data alone.\n",
    "learn_tab.fit_one_cycle(5,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8399b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_test_df = testing_df[X_cols].copy()\n",
    "tab_test_dl = learn_tab.dls.test_dl(tab_test_df,ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "c81ec6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds, _, decoded = learn_tab.get_preds(dl=tab_test_dl, with_decoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3f4c608e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2,  ..., 2, 2, 2])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ab5ebd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9a052bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded[decoded == 0] = -1\n",
    "decoded[decoded == 1] = 0\n",
    "decoded[decoded == 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2aa2c284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1,  0,  1])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b3757ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6423349568038629"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(testing_df[y_col],decoded,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738d3c7",
   "metadata": {},
   "source": [
    "### Large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce15053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
